<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Madhukar's Blog</title>
    <description>Thoughts on technology, life and everything else.</description>
    <link>http://blog.madhukaraphatak.com/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Introduction to Flink Streaming - Part 9 : Event time in Flink</title>
        <description>&lt;p&gt;In the last post in our flink series , we discussed about different abstractions of time supported in flink. In this we are going to discuss about how to work with these different abstractions. I will be discussing about event time abstraction specifically as it will be pretty new to users who are coming from other stream processing systems.  You can find all the other posts in the series &lt;a href=&quot;/categories/flink-streaming&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;event-time-in-flink&quot;&gt;Event time in Flink&lt;/h2&gt;

&lt;p&gt;Event time, as name suggests, is the time when event is generated. Normally the data which we collect from sources like sensors, logs have a time embedded in them. This time signifies when a given event is generated at the source. Flink allows us to work with this time, with event time support in the framework level.&lt;/p&gt;

&lt;p&gt;Whenever we talk about time, normally we need to address two different components of it. They are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When a given event is generated?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How long ago a given event is generated?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first question is about where a given event fits in event time line. The second question deals about tracking  of passing of time. Answering these two questions helps us to define how event time works in Flink.&lt;/p&gt;

&lt;h2 id=&quot;event-time-characteristic&quot;&gt;Event time characteristic&lt;/h2&gt;

&lt;p&gt;Before we start answering above questions, we need to understand how to tell to flink that we want to use event time abstraction. Flink uses processing time as default time abstraction. We can change it using below code.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setStreamTimeCharacteristic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TimeCharacteristic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;EventTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above line, we are setting in environment that we are going to use event time as our time characteristic.&lt;/p&gt;

&lt;h2 id=&quot;when-an-event-is-generated&quot;&gt;When an event is generated?&lt;/h2&gt;

&lt;p&gt;Once we have, set the time as event, we need to now say when a given event is occurred. There are few ways to do in flink. One of the easiest way is extract the time stamp embedded in the event itself.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Stock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  
  &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parsedStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;nc&quot;&gt;Stock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toLong&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;})&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above code, we declared a model which tracks stock price in a given point of time. The first field is timestamp of stock, then second is the symbol and third one is the value of the stock at that point of given time. Normally stock price analysis depends on event time rather than processing time as they want to correlate the change in stock prices when they happened in the market rather than they ended up in our processing engine.&lt;/p&gt;

&lt;p&gt;So once we define the model, we convert of string network stream into model which we want to use. So the time in the model, signifies when this stock reading is done.&lt;/p&gt;

&lt;h2 id=&quot;passing-of-time&quot;&gt;Passing of time&lt;/h2&gt;

&lt;p&gt;Whenever we say, we want to calculate max of a stock in last 5 seconds, how flink knows all the records for that 5 seconds are reached? It’s the way of saying how flink knows the passage of time in the source? We cannot use system clocks because there will be delay between these two systems.&lt;/p&gt;

&lt;p&gt;As we discussed in previous post, watermarks are the solution to this problem. Watermark signify the passage of time in source which will help to flink to understand flow in time. One of the easiest way of water marking is called ascending time. The below code show how to use ascending watermarks.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timedValue&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parsedStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assignAscendingTimestamps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The meaning of ascending timestamps is, whenever an event of t is occurred, it signifies that all the events happened before t are arrived to the system. This is very simplistic view of event time, as it doesn’t have a robust support for the late events. Still it’s good enough to understand overall event time abstraction. This will become much clearer when we run the example.&lt;/p&gt;

&lt;p&gt;Once we have the both questions answered, we can now work with event time to group relevant stocks.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyedStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timedValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyedStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;timedwindow&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;print sink&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code, calculates max of a given stock price in last 10 seconds.&lt;/p&gt;

&lt;p&gt;You can access complete code on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/EventTimeExample.scala&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;running-the-example&quot;&gt;Running the example&lt;/h2&gt;

&lt;p&gt;Behavior of event time is best understood using an example. If you are still confused about ascending time stamps, this example should be able help you understand details. Make sure you run this example in local mode, rather from an IDE. For more information how to run flink examples in local mode, refer to this &lt;a href=&quot;/introduction-to-flink-streaming-part-3/&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Enter the below are the records in socket console. These are records for AAPL with time stamps. The first records is for time Wed, 27 Apr 2016 11:34:22 GMT.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1461756862000,&amp;quot;aapl&amp;quot;,500.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we send the next record, which is after 5 seconds. This signifies to flink that, 5 seconds have passed in source. Nothing will evaluate yet, as we are looking for 10 seconds window. This event is for time Wed, 27 Apr 2016 11:34:27 GMT&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1461756867000,&amp;quot;aapl&amp;quot;,600.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we send another event, which is after 6 seconds from this time. Now flink understands 11 seconds have been passed and will evaluate the window. This event is for Wed, 27 Apr 2016 11:34:32 GMT&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1461756872000,&amp;quot;aapl&amp;quot;,400.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now flink prints, maximum value as below in 10 seconds.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1461756872000,&amp;quot;aapl&amp;quot;,600.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This shows how flink is keeping track of time using ascending watermarks to keep track of time in event time.&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Apr 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-9</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-9</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 8 : Understanding Time in Flink Streaming</title>
        <description>&lt;p&gt;Time plays an important role in streaming applications. Using time, we can group, correlate different events happening in the stream. Some of the constructs like window, heavily use the time component. Most of the streaming frameworks supports a single meaning of time, which is mostly tied to the processing time.&lt;/p&gt;

&lt;p&gt;Processing time is a clock which keep tracks of the time passed from the beginning of the process. Having support for processing time is not good enough in modern streaming applications. Most of them need flexibility in the time abstractions and flink supports that flexibility compared to other stream processing frameworks.&lt;/p&gt;

&lt;p&gt;In this eight post in the series, we will be talking about different time abstractions supported in flink. You can find all the other posts in the series &lt;a href=&quot;/categories/flink-streaming&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;concept-of-time-in-streaming-application&quot;&gt;Concept of time in streaming application&lt;/h2&gt;

&lt;p&gt;A streaming application is an always running application. So in order to understand the behavior of the application over time, we need to take snapshots of the stream in various points. Normally these various points are defined using a time component.&lt;/p&gt;

&lt;p&gt;Time in streaming application is way to correlate different events in the stream to extract some meaningful insights. For example, when we say count of words in a word count example for last 10 seconds, we normally mean to collect all the records arrived in that point of time and run a word count on it.&lt;/p&gt;

&lt;p&gt;Normally most of the streaming frameworks like storm, spark-streaming only support one concept of time. But flink does support multiple different ones.&lt;/p&gt;

&lt;h2 id=&quot;time-in-flink&quot;&gt;Time in Flink&lt;/h2&gt;

&lt;p&gt;When we say, last 10 seconds what it means? Flink say it depends. It can be one of three following&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;processing-time&quot;&gt;Processing Time&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This concept of time is very familiar to most of the users. In this, time is tracked using a clock run by the processing engine. So in this time, last 10 seconds means the records arrived in last 10 seconds for the processing. Here we only use the semantics of when the records came for processing.&lt;/p&gt;

&lt;p&gt;Though processing time is good time measure to have,it’s not always enough. For example, if we want to calculate state of sensors at given point of time, we want to collect events that happened in that time range. But if the events arrive lately to processing system due to various reasons, we may miss some of the events as processing clock does not care about the actual time of events. To address this, flink support another kind of time called event time.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;event-time&quot;&gt;Event Time&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Event time is the time embed in the data that is coming into the system. So here 10 seconds means, all the records generated in those 10 seconds at the source. These may come out of order to processing. This time is independent of the clock that is kept by the processing engine.Event time is extremely useful for handling the late arrival events.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;ingestion-time&quot;&gt;Ingestion Time&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ingestion time is the time when events ingested into the system. This time is in between of the event time and processing time. Normally in processing time, each machine in cluster is used to assign the time stamp to track events. This may result in little inconsistent view of the data, as there may be delays in time across the cluster. But ingestion time, timestamp is assigned in ingestion so that all the machines in the cluster have exact same view. These are useful to calculate results on data that arrive in order at the level of ingestion.&lt;/p&gt;

&lt;h2 id=&quot;watermarks&quot;&gt;WaterMarks&lt;/h2&gt;

&lt;p&gt;As flink supports multiple concept of time, how flink keep tracks of time?. Because in the normal processing time, a system clock can be used. But you cannot use the system clock in case of the event time and ingestion time. So there has to be a generic mechanism to handle this.&lt;/p&gt;

&lt;p&gt;Watermarks is the mechanism used by the flink in order to signify the passing of time in stream. Watermarks are the special control events which are part of the stream itself.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/fig/stream_watermark_in_order.svg&quot; alt=&quot;WaterMarks in Flink&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above diagram shows, the water mark events as part of the stream. In above example, w(20) signifies a watermark event that 20 units of time have passed.These water marks can be generated using system clock in case of processing time, source in case of event time. This flexibility of keeping track of time allows flink to support different abstractions of time.&lt;/p&gt;

&lt;p&gt;Now we understand different time abstractions supported by Flink. In next blog, we will look at how to use work with event time.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark-streaming-api&quot;&gt;Compared to Spark Streaming API&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;Spark streaming only supports the processing time as of now. The time is tracked using the internal clock in the spark driver.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Event time in Flink Docs - &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/event_time.html&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.0/apis/streaming/event_time.html&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Wed, 27 Apr 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-8</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-8</guid>
      </item>
    
      <item>
        <title>Apache Beam : Next Step in Big Data Unification</title>
        <description>&lt;p&gt;Unification been the theme of big data processing for quite some time. More and more frameworks in big data want to be platforms which can do multiple things, rather than yet another framework to solve a specific problem. This idea of unification has taken big data to new levels. These days we have come to expect single platform to do batch, streaming and interactive processing with plethora of libraries on top of it.&lt;/p&gt;

&lt;p&gt;As the processing unification is settling down, there is a new trend in big data to unify the API across multiple platforms like Spark, Flink etc. Apache Beam, is one of the first steps towards that direction.&lt;/p&gt;

&lt;p&gt;In this post, I will be talking about the different level of unification happened over the years and how Apache Beam will be next step in that direction.&lt;/p&gt;

&lt;h2 id=&quot;unification-of-cluster-infrastructure&quot;&gt;Unification of cluster/ infrastructure&lt;/h2&gt;

&lt;p&gt;When Hadoop came along, it was only capable of running Map/Reduce. To run any other system, like Storm, you were needed to have a separate cluster. This meant that you need to maintain separate cluster for each framework which is big overhead in terms of maintenance and cost.&lt;/p&gt;

&lt;p&gt;Over time, frameworks like Mesos and YARN solved those issues. YARN became an unified platform to run the multiple processing frameworks on same cluster. In this way, we need to have only one cluster and we can easily share the resources between the frameworks.&lt;/p&gt;

&lt;h2 id=&quot;unification-of-processing-frameworks&quot;&gt;Unification of processing frameworks&lt;/h2&gt;

&lt;p&gt;As infrastructure unified, we wanted unification in processing. In early days of big data, we used separate frameworks for batch, streaming and structure data analysis namely map/reduce, storm and hive. Learning, developing and maintaining these multiple framework code is became trickier. Also sharing data between these systems became tricky as they don’t share any common abstraction.&lt;/p&gt;

&lt;p&gt;Apache Spark came along to solve this problem. It proposed an single abstraction to base all different kind of processing. This made sharing data trivial and developers can improve applications in much rapid way as they only have to learn single framework than multiple ones. Apache spark dawned the platform era in big data. All other systems, like Flink, came after spark are following that way.&lt;/p&gt;

&lt;h2 id=&quot;unification-of-developers&quot;&gt;Unification of developers&lt;/h2&gt;

&lt;p&gt;Any big data analysis comprise data scientists, the one who focus on the data modeling and data engineers, the one who focus on data pipelines. Earlier each of these teams used to use different toolset. This made difficult to share knowledge and code across the teams.&lt;/p&gt;

&lt;p&gt;Apache Spark with it’s Dataframe API is trying to unify these developers where both can use same platform for modeling and pipeline building.&lt;/p&gt;

&lt;p&gt;So from the above points, it’s clear that more and more efforts are spent by various frameworks to unify the different aspects of big data analysis. But now there is a new level of unification is needed. That is API unification.&lt;/p&gt;

&lt;h2 id=&quot;need-of-api-unification&quot;&gt;Need of API unification&lt;/h2&gt;

&lt;p&gt;Whenever there is a new platform, developers are forced to rewrite their application to get the benefits. Most of the time these platforms share a very similar API for data processing, only differing in the runtime implementation. This need of the rewrite of applications, holding back the adoption of new platforms. Also this makes difficult to use best tool for a given use case.&lt;/p&gt;

&lt;p&gt;For example, Apache Flink has good support for streaming compare to Spark. Spark has good support for batch compared Flink. I want to use both Flink and Spark, but doing same computations for streaming and batch. In the current scenario, I need to write the program twice one in flink and one in spark. Though the API’s look similar, they do differ in various details. So as of now whenever we want to use best of multiple frameworks, we need to write program multiple times.&lt;/p&gt;

&lt;p&gt;So if you want to write program only once, then you need to stick with single platform. This restricts the developers where they cannot use the best frameworks for the work. Isn’t be nice where we can write the program once and run on different platforms?&lt;/p&gt;

&lt;h2 id=&quot;apache-beam--unifying-the-platforms&quot;&gt;Apache Beam : Unifying the platforms&lt;/h2&gt;

&lt;p&gt;Apache Beam, is the new unified batch and streaming framework which is trying to solve the exact problem which we discussed above. Apache Beam allows to write a program in a DSL which can run on both flink and spark. Apache Beam is bringing unification to platforms as YARN brought to infrastructure.&lt;/p&gt;

&lt;p&gt;From Apache Bean website, Apache Beam is&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;An open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So from the definition, it’s clear that apache beam is trying to have a single API which can express both batch and streaming application which can run on multiple runtimes. So now I can write a single program which not only can do both streaming and batch but allows me to choose the platform on which I want to run these programs on without any change to the code.&lt;/p&gt;

&lt;p&gt;This kind of unification is taking big data to next level where developers are not need to write code multiple times whenever there is a new framework is around. As of now Beam is still in proposal stage, it will take still time to mature. But this new way of looking at big data will going to immensely benefit the organization to adopt new platforms much faster way.&lt;/p&gt;

</description>
        <pubDate>Sun, 17 Apr 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/apache-beam-next-step-in-big-data-unification</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/apache-beam-next-step-in-big-data-unification</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 7 : Implementing Session Windows using Custom Trigger</title>
        <description>&lt;p&gt;In our last blog we discussed about the internals of window API in flink. As we discussed in the blog, understanding internals allows us to implement custom windows in flink API. This flexibility to define our own window logic helps us to implement business rules seamlessly on the stream data.&lt;/p&gt;

&lt;p&gt;In this seventh post of the series, we are going to implement a custom window using trigger API. We will be implementing a window which allow us to understand user behavior across a specific session. This will be useful for applications where we want to analyze the data coming from an online portal where user logs in and perform some actions. You can find all other posts from the series &lt;a href=&quot;/categories/flink-streaming&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;user-session&quot;&gt;User session&lt;/h2&gt;
&lt;p&gt;A session is often period of time that capture different interactions with an application from user. A session is set up or established at a certain point in time, and then torn down at some later point.&lt;/p&gt;

&lt;p&gt;As an example, in an online portal session normally starts when user logs into the application. All the purchases made in this point of time are captured in this session. Session will be torn down when user logged out or it expires when there is no activity for some time.&lt;/p&gt;

&lt;h2 id=&quot;session-window&quot;&gt;Session window&lt;/h2&gt;

&lt;p&gt;A session window, is a window which allows us to group different records from the stream for a specific session. This window will start when the session starts and evaluated when session is ended. This window also will support tracking multiple sessions at a same time.&lt;/p&gt;

&lt;p&gt;Session windows are often used to analyze user behavior across multiple interactions bounded by session.&lt;/p&gt;

&lt;p&gt;In flink, we only have built in windows for time and count based evaluation. But our session window doesn’t depend upon any of these. So we need to create a custom window which can satisfy our requirement.&lt;/p&gt;

&lt;h2 id=&quot;modeling-user-session&quot;&gt;Modeling user session&lt;/h2&gt;
&lt;p&gt;Once we understood about the session and session window, we need to model the session in our code. I have a simple representation of session for explaining the example. Most of the real world session information will much more complicated than it.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sessionId&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endSignal&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above case class is model of our session. It has following three components&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;sessionid&quot;&gt;sessionId&lt;/h3&gt;
    &lt;p&gt;An identifier which uniquely identifies a session. There can be multiple sessions active at same time. Each of these sessions will have unique session id.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;value&quot;&gt;value&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s a value associated each interaction in the session. This signifies the information we want to track with in the session. In real world scenarios, it will be user interactions with the system. As an example, in a online store it may be the product user added to the cart. In our example, it’s a simple double value.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;endsignal&quot;&gt;endSignal&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s an optional value in record. This signifies end of the session from the application side. It may be risen because user has logged out or may the session expired. It depends on the application to generate these signals.&lt;/p&gt;

&lt;p&gt;Once we modeled the our session, we can now build a trigger which works on this model.&lt;/p&gt;

&lt;h2 id=&quot;session-trigger&quot;&gt;Session Trigger&lt;/h2&gt;

&lt;p&gt;As we discussed in earlier blog, trigger is a function which decides when a given window evaluates. In our example, we want the window to evaluate when endSignal for given session is sent. As we don’t have any built in trigger for it, we are going to define our own custom trigger.&lt;/p&gt;

&lt;p&gt;The below code is for the session trigger&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SessionTrigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;&amp;lt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Window&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Trigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Session&lt;/span&gt;,&lt;span class=&quot;kt&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;override&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;onElement&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;TriggerContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;TriggerResult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endSignal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isDefined&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TriggerResult&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;FIRE&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TriggerResult&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;CONTINUE&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;override&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;onProcessingTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;TriggerContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;TriggerResult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;TriggerResult&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;CONTINUE&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;override&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;onEventTime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;TriggerContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;TriggerResult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;TriggerResult&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;CONTINUE&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above code, we are extending Trigger. When we are extending, we are specifying the that data will be represented using &lt;em&gt;Session&lt;/em&gt; model class.&lt;/p&gt;

&lt;p&gt;Once we extend the trigger, we need to override the 3 methods. They are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;onelement&quot;&gt;onElement&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a callback method, which will be called whenever each record is added to the window. In our code, we check is the endSingal is present. If it’s present we return &lt;em&gt;TriggerResult.FIRE&lt;/em&gt; which signifies we need to fire the trigger. Otherwise we will return &lt;em&gt;TriggerResult.CONTINUE&lt;/em&gt; which signifies we need to continue adding elements to window.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;oneventtime-and-onprocessingtime&quot;&gt;onEventTime and onProcessingTime&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above two methods are used when we use a window which depends on time. As our session logic doesn’t depend on time, we don’t need to worry about them.&lt;/p&gt;

&lt;p&gt;You can access complete code &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/sessionwindow/SessionTrigger.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now we have our custom trigger ready. Now we can create a window which evaluates when a given session completes.&lt;/p&gt;

&lt;h2 id=&quot;putting-all-together&quot;&gt;Putting all together&lt;/h2&gt;

&lt;p&gt;In this section, we glue different things we did earlier. The below are the steps.&lt;/p&gt;

&lt;h3 id=&quot;step-1--read-data-from-socket-and-convert-to-session&quot;&gt;Step 1 : Read data from socket and convert to session&lt;/h3&gt;

&lt;p&gt;As the first step, we need to read from the source and model it as session object.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;StreamExecutionEnvironment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getExecutionEnvironment&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endSignal&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Try&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getOrElse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endSignal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;})&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-2--create-keyed-stream-based-on-sessionid&quot;&gt;Step 2 : Create keyed stream based on sessionId&lt;/h3&gt;

&lt;p&gt;As we discussed earlier, we want to evaluate multiple sessions at a time. So we need to created keyedstream stream based on session id.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sessionId&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-3--create-session-window&quot;&gt;Step 3 : Create session window&lt;/h3&gt;
&lt;p&gt;Once we have keyvalue stream, now we can define a window using our custom trigger. As we did with count window, we are going to use &lt;em&gt;GlobalWindow&lt;/em&gt; as our window assigner and rather than using &lt;em&gt;CountTrigger&lt;/em&gt; we are going to add our custom trigger. We use purging trigger for purging session once it’s evaluated.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sessionWindowStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;GlobalWindows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()).&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;trigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;PurgingTrigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SessionTrigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;GlobalWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]()))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code created &lt;em&gt;sessionWindowStream&lt;/em&gt; which tracks sessions.&lt;/p&gt;

&lt;h3 id=&quot;step-4--aggregate-and-print&quot;&gt;Step 4 : Aggregate and Print&lt;/h3&gt;

&lt;p&gt;Once we have the window, we need to define an aggregate function over window. In this example, we are going to sum the value over session and print to console.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;sessionWindowStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can access complete code &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/sessionwindow/SessionWindowExample.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;running-the-example&quot;&gt;Running the example&lt;/h2&gt;
&lt;p&gt;As we ran our earlier example, we will be entering data in the stdin of the socket. In our example, socket will be listening on port 9000.&lt;/p&gt;

&lt;p&gt;Enter below two lines in the stdin&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;session1,100
session2,200&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above, we have started two sessions &lt;em&gt;session1&lt;/em&gt; and &lt;em&gt;session2&lt;/em&gt;. This will create two windows. As of now, no window will evaluate as session is not yet ended.&lt;/p&gt;

&lt;p&gt;Let’s end &lt;em&gt;session1&lt;/em&gt; using below line&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;session1,200,end&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you will observe the below result in flink&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Session(session1,300.0,None)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Window for session1 is evaluated now, as it is ended.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/flink-user/201510.mbox/%3CC195B624-FB46-4D90-AE0F-B8782EB81951@apache.org%3E&quot;&gt;http://mail-archives.apache.org/mod_mbox/flink-user/201510.mbox/%3CC195B624-FB46-4D90-AE0F-B8782EB81951@apache.org%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/aljoscha/a7c6f22548e7d24bc4ac&quot;&gt;https://gist.github.com/aljoscha/a7c6f22548e7d24bc4ac&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Apr 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-7</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-7</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 6 : Anatomy of Window API</title>
        <description>&lt;p&gt;In last few post in flink series, we have discussed about window API in flink. Window API is a powerful construct which allows us to express different stream processing concepts in elegant manner. In Flink, we can build windows using different recipes. Understanding these recipes helps us use them in efficient manner and also allows us to extend them to suit our custom requirements.&lt;/p&gt;

&lt;p&gt;In this sixth blog of series, we will take a deep dive into window internals. You can access all the posts in the series &lt;a href=&quot;/categories/flink-streaming&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;anatomy-of-window-api&quot;&gt;Anatomy of Window API&lt;/h2&gt;

&lt;p&gt;From earlier blogs, window API in flink may look like a single monolithic API. But under the covers it’s a modular API. A window in flink is constructed using few different functions. This modular nature of defining window gives user flexibility to define the windows for their use case.&lt;/p&gt;

&lt;p&gt;The different functions needed to define a window in flink are,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;window-assigner&quot;&gt;Window Assigner&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A function which is responsible for assigning given element to window. Depending upon the definition of window, one element can belong to one or more window at a time.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;trigger&quot;&gt;Trigger&lt;/h3&gt;
    &lt;p&gt;Function which defines the condition for triggering window evaluation. This function control when a given window created by window assigner is evaluated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;evictor&quot;&gt;Evictor&lt;/h3&gt;
    &lt;p&gt;An optional function which defines the preprocessing before firing window operations.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above 3 different functions are recipes for putting together the window.&lt;/p&gt;

&lt;h2 id=&quot;understanding-count-window&quot;&gt;Understanding Count window&lt;/h2&gt;

&lt;p&gt;Once we understood the different components of window API, let’s take one of existing window and see how these functions are defined. We will be using count window as example.&lt;/p&gt;

&lt;h3 id=&quot;window-assigner-1&quot;&gt;Window Assigner&lt;/h3&gt;
&lt;p&gt;When you are building an count based window, there is no start or end to the window. So for those kind of non time based windows, we have window assigner called GlobalWindow. For a given key, all values are filled into the same window.&lt;/p&gt;

&lt;p&gt;The below code shows how to assign the window assigner.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;GlobalWindows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;window&lt;/em&gt; API allows us to add the window assigner to the window. Every window assigner has a default trigger. The default trigger for a global window is &lt;em&gt;NeverTrigger&lt;/em&gt; which never triggers. So this window assigner has to be used with a custom trigger.&lt;/p&gt;

&lt;h3 id=&quot;count-trigger&quot;&gt;Count trigger&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;trigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;CountTrigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once we have the window assigner, now we have to define when the window needs to be trigger. In the above code, we add a trigger which evaluates the window for every two records.&lt;/p&gt;

&lt;p&gt;For this example, we are not using any evictor.&lt;/p&gt;

&lt;h2 id=&quot;aggregation&quot;&gt;Aggregation&lt;/h2&gt;

&lt;p&gt;Once we create window, we need to run an aggregation on window to make it non lazy.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;countWindowWithoutPurge&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can access full code &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/WindowAnatomy.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;running-example&quot;&gt;Running example&lt;/h2&gt;

&lt;p&gt;Let’s run above count window with following data. Enter this data in socket stdin similar to our last &lt;a href=&quot;/introduction-to-flink-streaming-part-5/&quot;&gt;example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Input&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;hello how are you
hello who are you&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Result&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(you,2)
(are,2)
(hello,2)
(how,2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above result shows, flink created 4 windows for 4 keys. Now if you enter the below lines again&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;hello how are you
hello who are you&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;you will be observe the below result.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(you,4)
(are,4)
(hello,4)
(how,4)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Flink still has 4 windows, but now count is 4 rather than 2. It’s because
rather than evaluating just new window, flink has retained the records from the earlier window also. This is the default behavior of a trigger. But most of the cases we want to clear the records once window evaluates, rather than keeping forever. In those situations we need to use purge trigger with count trigger.&lt;/p&gt;

&lt;h2 id=&quot;purging-trigger&quot;&gt;Purging Trigger&lt;/h2&gt;

&lt;p&gt;Purging trigger is a trigger which normally wraps the other triggers. Purging trigger is responsible for purging all the values which are passed to the trigger from the window once window evaluates.&lt;/p&gt;

&lt;p&gt;The below code shows how to use purging trigger with count trigger.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countWindowWithPurge&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;GlobalWindows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()).&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;trigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;PurgingTrigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;CountTrigger&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;GlobalWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now &lt;em&gt;PurgingTrigger&lt;/em&gt; wraps the &lt;em&gt;CountTrigger&lt;/em&gt;. So purging trigger is always used in combination with other triggers.&lt;/p&gt;

&lt;p&gt;If you run the code with same data, you will observe that result will be only calculated for latest window not across evaluations.&lt;/p&gt;

&lt;p&gt;Now we understand anatomy of a window in Flink streaming API.&lt;/p&gt;

</description>
        <pubDate>Tue, 05 Apr 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-6</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-6</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 5 : Window API in Flink</title>
        <description>&lt;p&gt;In recent years, there is been a lot of discussion on stateful stream processing. When initial open source stream processor like storm came along, stream processing was viewed as the faster batch processing. The API’s were more geared towards the stateless ETL pipelines. But as realtime/ stream processing became more and more important having stateful processing became necessary. So all modern stream processing frameworks have varied degree of support to do stateful operations.&lt;/p&gt;

&lt;p&gt;Window is one of the way to define continuous state across the stream. So in the fifth blog of the series, I will be discussing about window support in flink API. You can access all the posts in the series &lt;a href=&quot;/categories/flink-streaming&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;window-in-streaming&quot;&gt;Window in Streaming&lt;/h2&gt;

&lt;p&gt;Window is a mechanism to take a snapshot of the stream. This snapshot can be based on time or other variables. For example, if we create a window for 5 seconds then it will be all the records which arrived in the that time frame. You can define the window based on no of records or other stream specific variables also.&lt;/p&gt;

&lt;h2 id=&quot;types-of-window-in-flink&quot;&gt;Types of window in Flink&lt;/h2&gt;

&lt;p&gt;Flink support wide variety of window operations. The different windows supported in flink are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Time Windows
    &lt;ul&gt;
      &lt;li&gt;Tumbling Windows&lt;/li&gt;
      &lt;li&gt;Sliding Windows&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Count Windows&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;creating-keyeddatastream&quot;&gt;Creating KeyedDataStream&lt;/h2&gt;

&lt;p&gt;Before we discuss about each of above windows, we need to be aware of one fact. Most of the window operations are encouraged to be used on KeyedDataStream. A KeyedDataStream is a datastream which is partitioned by the key. This partitioning by key allows window to be distributed across machines resulting in good performance. The following code shows how to create a KeyedDataStream from data coming from the socket.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;\\s+&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above example, we are reading data and creating a stream named &lt;em&gt;source&lt;/em&gt;. Once we receive the data we are extracting the words using &lt;em&gt;flatMap&lt;/em&gt; and &lt;em&gt;map&lt;/em&gt; operators. Once we have a tuple, we are creating a KeyedDataStream using keyBy operation. Once we have a KeyedDataStream, we can start defining the window.&lt;/p&gt;

&lt;p&gt;You can also define windows on non keyed stream. But they often result in poor performance. So I will be not discussing them here.&lt;/p&gt;

&lt;h2 id=&quot;tumbling-window&quot;&gt;Tumbling Window&lt;/h2&gt;

&lt;p&gt;We have already seen this window on our earlier &lt;a href=&quot;/introduction-to-flink-streaming-part-2&quot;&gt;post&lt;/a&gt;. In this section we will be discussing little more.&lt;/p&gt;

&lt;p&gt;A tumbling window is a time based window which tumbles once the window is evaluated. In essence, all the state and records of the window will be purged once the window evaluates. This kind of window is very useful for dividing stream in to multiple discrete batches.&lt;/p&gt;

&lt;p&gt;The below code shows how to create a tumbling window&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tumblingWindow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To create a tumbling window, we use timeWindow API. In above example, stream will be evaluated for every 15 seconds. Here we are calculating the word counts for every 15 seconds.&lt;/p&gt;

&lt;h2 id=&quot;sliding-window&quot;&gt;Sliding window&lt;/h2&gt;

&lt;p&gt;Sliding window is one of most known windowing in streaming. They usually used for keeping running count for a distant past. They allow us to answer questions like “what is word count for last 5 seconds”?.&lt;/p&gt;

&lt;p&gt;Sliding window is also a time based window. The only difference with tumbling window is, an element can belong to multiple windows in sliding window compared to only one window in case of tumbling window. So sliding window normally creates overlapping windows compared to discrete windows in tumbling window.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slidingWindow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above example, we are calculating wordcount for last 15 seconds, in each 5 second interval.&lt;/p&gt;

&lt;h2 id=&quot;count-based-windows&quot;&gt;Count based windows&lt;/h2&gt;

&lt;p&gt;The above two windows were based on time. But in flink we can define windows on other properties also. One of them is count windows. As name suggests, count window are evaluated when no of records received hits the threshold.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countWindow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code defines a count window which fires for after every 5 records. Please note that as the stream is keyed, for each key it tracks no records not across the multiple keys.&lt;/p&gt;

&lt;p&gt;You can access complete code for all the three window &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/WindowExample.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;running-with-examples&quot;&gt;Running with examples&lt;/h2&gt;

&lt;p&gt;Window operations are hard to wrap mind around without examples. So in the next few sections we are going to discuss how to run these examples with sample data and understand their behavior. You can run this example from IDE or in local mode. But before running you need to make sure comment out the non necessary outputs. For example, when you are running tumbling window make sure you comment out the below lines in the code.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;slidingWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;slidingwindow&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;countWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;count window&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We comment out these lines just to make sure they don’t interfere with our output. Follow the same for other two also.&lt;/p&gt;

&lt;p&gt;All the input is entered in stadin of nc command. You can start socket for the program using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;nc -lk localhost 9000&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;running-tumbling-window&quot;&gt;Running tumbling window&lt;/h2&gt;

&lt;p&gt;We are going to run tumbling window in this section. Enter the below lines one by one with in 15 seconds on nc stdin.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;hello how are you
hello who are you&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You will observe the below result once 15 seconds are done.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(hello,2)
(you,2)
(are,2)
(who,1)
(how,1)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you wait for sometime and enter same lines you will observe that the count is reset and you get same result as above. So this shows how tumbling window discretized the stream.&lt;/p&gt;

&lt;h2 id=&quot;running-sliding-window&quot;&gt;Running sliding window&lt;/h2&gt;

&lt;p&gt;In this section we are going to run sliding window.&lt;/p&gt;

&lt;p&gt;If you send the same lines as above in the beginning you will see the result is printed thrice. The reason being, we created a window for 15 seconds which is three times of the 5 seconds. So when window evaluates every 5 seconds, it recalculates the same result. You can add more rows in between to observe how count changes.&lt;/p&gt;

&lt;h2 id=&quot;running-count-window&quot;&gt;Running count window&lt;/h2&gt;
&lt;p&gt;In this section, we are going to run count based windows.&lt;/p&gt;

&lt;p&gt;If you send those two lines, nothing will be printed. The reason is no key has a count of 5.&lt;/p&gt;

&lt;p&gt;Enter the same lines again. Nothing will be printed. Again we have not yet hit the threshold.&lt;/p&gt;

&lt;p&gt;Enter the first line again. Now you will see the below result.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(are,5)
(hello,5)
(you,5)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So as soon as the the count hits 5 window will be triggered. So from the example it’s apparent the count is kept for a key not across keys.&lt;/p&gt;

&lt;p&gt;So window API in flink is very powerful compared to other frameworks. These constructs should allow you to express your application logic elegantly and efficiently.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark-streaming-api&quot;&gt;Compared to Spark Streaming API&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;You can simulate the tumbling window using sliding window operation available in spark. If both window duration and sliding duration is same, you get tumbling window.&lt;/p&gt;

&lt;p&gt;Sliding windows are supported as first class citizens.&lt;/p&gt;

&lt;p&gt;Count based window is not supported in spark streaming. As windowing system of spark is tightly coupled with time, no builtin support for other types of window are there as of now.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Introducing Stream Windows in Apache Flink - &lt;a href=&quot;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&quot;&gt;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Mon, 14 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-5</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-5</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 4 : Understanding Flink's Advanced Stream Processing using Google Cloud Dataflow</title>
        <description>&lt;p&gt;Flink streaming is a new generation of streaming framework which bring a lot of new concepts to the table. It’s is not just a low latency batch processing like spark streaming or it’s not primitive event processor like storm. It has best of both the worlds and much more.&lt;/p&gt;

&lt;p&gt;As with any new generation framework, it’s hard for people coming from the other frameworks to appreciate full power of the new system. We normally end up trying to replicate same old application on new framework. This happened when we moved from Map/Reduce to Spark. So understanding the strengths of these new frameworks is critical to build new generation streaming applications rather than replicating the already existing one.&lt;/p&gt;

&lt;p&gt;When I was researching on flink streaming , I came across this excellent video which demonstrated power of these new generation streaming frameworks. This video is from flink forward conference which talks about google cloud dataflow. Google cloud dataflow is a unified batch and streaming API for big data from Google. Lot of ideas for flink streaming API are inspired from google dataflow. So understanding motivations and programming model of google dataflow may help to understand the power of flink streaming. Also recently google made it open source under the name &lt;a href=&quot;https://wiki.apache.org/incubator/BeamProposal&quot;&gt;apache beam&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As the fourth blog in the series, I am sharing the google dataflow talk from flink forward below. You can access the slides on &lt;a href=&quot;http://www.slideshare.net/FlinkForward/william-vambenepe-google-cloud-dataflow-and-flink-stream-processing-by-default&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can access other blog in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/y7f6wksGM6c&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

</description>
        <pubDate>Sat, 12 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-4</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-4</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 3 : Running Streaming Applications in Flink Local Mode</title>
        <description>&lt;p&gt;In the last two blogs of our &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;series&lt;/a&gt;, we discussed about how to use flink streaming API’s to do word count. In those blogs we ran our examples from IDE. IDE is a good way to start learning any API. But if we want to understand how code executes in distributed setting it will be good to run it outside of IDE.&lt;/p&gt;

&lt;p&gt;In this third blog, I will be discussing about how to run flink streaming examples in local mode, which is a good starting point to understand distributed nature of flink. Access other blog in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;modes-of-execution-in-flink&quot;&gt;Modes of execution in Flink&lt;/h2&gt;
&lt;p&gt;As with any modern big data frameworks, flink allows user to run the code on different cluster management systems. No change in code is required to run on these different systems. This makes it very easy to change the cluster management system depending on the use case.&lt;/p&gt;

&lt;p&gt;The different modes of execution supported in flink are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;local-mode&quot;&gt;Local Mode&lt;/h3&gt;
    &lt;p&gt;Local mode is a pseudo distributed mode which runs all the daemons in the single jvm. It uses AKKA framework for parallel processing which underneath uses multiple threads.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;standalone-cluster-mode&quot;&gt;Standalone Cluster Mode&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this setup, different daemons runs on different jvms on a single machine or multiple machines. This mode often used when we want to run only Flink in our infrastructure.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;yarn&quot;&gt;YARN&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This mode makes flink run on YARN cluster management. This mode often used when we want to run flink on our existing hadoop clusters.&lt;/p&gt;

&lt;p&gt;Though standalone/yarn is suitable for production, local mode is often good start point to understand the distributed model of flink streaming. So in this blog we will be setting up flink in local mode and run our word count example.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-local-mode&quot;&gt;Setting up local mode&lt;/h2&gt;

&lt;p&gt;The following are the steps for setting up flink in local mode.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;step-1--download-flink-10-distribution&quot;&gt;Step 1 : Download Flink 1.0 distribution&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can download binary distribution of flink &lt;a href=&quot;http://flink.apache.org/downloads.html&quot;&gt;here&lt;/a&gt;. As of now latest version is 1.0. As our examples are compiled against scala 2.10, choose one with that scala version. I chose hadoop 2.6 version for my examples. You can choose the one which matches your hadoop version if you have already any hadoop setup on your machine. If you don’t have any set up choose 2.6.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;step-2--extract-downloaded-file&quot;&gt;Step 2 : Extract downloaded file&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;tar -zxvf flink-1.0.0-bin-hadoop26-scala_2.10.tgz&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;step-3--start-flink-in-local-mode&quot;&gt;Step 3 : Start Flink in local mode&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The below command starts the flink in localmode.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;bin/start-local.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once it started successfully, you can access web UI at &lt;a href=&quot;http://localhost:8081&quot;&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;packaging-code&quot;&gt;Packaging code&lt;/h2&gt;

&lt;p&gt;Once we have the local flink running, we need to package our code as the jar in order to run. Use the below sbt command to create jar.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;sbt clean package&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once the build is successful, you will get &lt;em&gt;flink-examples_2.10-1.0.jar&lt;/em&gt; under directory &lt;em&gt;target/scala-2.10&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;running-wordcount-in-local-mode&quot;&gt;Running wordcount in local mode&lt;/h2&gt;

&lt;p&gt;Once we have packaged jar, now we can run it outside IDE. Run the below command from flink directory. Replace the path to jar with absolute path to the jar generated in last step.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;bin/flink run -c com.madhukaraphatak.flink.streaming.examples.StreamingWordCount &amp;lt;path-to-flink-examples_2.10-1.0.jar&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command uses &lt;em&gt;flink&lt;/em&gt; command to run the example. &lt;em&gt;flink&lt;/em&gt;  is a command used for interact with jobs. The &lt;em&gt;run&lt;/em&gt; sub command is used for submit jobs. &lt;em&gt;-c&lt;/em&gt; option indicates the jar to be added to classpath. Next two parameters are main class and the jar path.&lt;/p&gt;

&lt;p&gt;Once you run the above command, wordcount start running in the local mode. You can see this running job &lt;a href=&quot;http://localhost:8081/#/running-jobs&quot;&gt;here&lt;/a&gt;. As you enter the lines in socket you can observe output &lt;a href=&quot;http://localhost:8081/#/jobmanager/stdout&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now we have successfully ran our example outside IDE in flink local mode.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark&quot;&gt;Compared to Spark&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;The local mode of flink, is similar to spark local mode. &lt;em&gt;flink&lt;/em&gt; command is similar to &lt;em&gt;spark-submit&lt;/em&gt;. One of the difference is, in spark you don’t need to start web ui daemon as spark itself starts it when you create spark context.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Flink Local Setup - &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/setup/local_setup.html&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-master/setup/local_setup.html&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 10 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-3</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-3</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 2 : Discretization of Stream using Window API</title>
        <description>&lt;p&gt;In the last &lt;a href=&quot;/introduction-to-flink-streaming-part-1&quot;&gt;blog&lt;/a&gt;, we discussed about how to do continuous stream processing using flink streaming API. Our wordcount example keeps on updating the counts as and when we received new data. This is good for some of the use cases. But in some use cases we want to aggregate some set of records in a given time interval, in order to keep track of variance over time. In those cases, we need to divide the stream into small batches. This discretization allow us to capture the change in aggregated value overtime. This discretized batches is also known as micro batches.&lt;/p&gt;

&lt;p&gt;In this second blog, I will be discussing about how to discretized the stream using flink’s window operation.We will be using same word count example in this post also. You can access all the blogs in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;window-in-streaming&quot;&gt;Window in streaming&lt;/h2&gt;

&lt;p&gt;Window is a mechanism to take a snapshot of the stream. This snapshot can be based on time or other variables. For example, if we create a window for 5 seconds then it will be all the records which arrived in the that time frame. You can define the window based on no of records or other stream specific variables also.&lt;/p&gt;

&lt;p&gt;Window allows us to understand change in stream data by taking snapshots in regular intervals. Flink API has wide variety of window support. In this post we are only going to focus on one kind of window known as Tumbling Windows.&lt;/p&gt;

&lt;h2 id=&quot;tumbling-window-in-flink-streaming&quot;&gt;Tumbling Window in Flink Streaming&lt;/h2&gt;

&lt;p&gt;Tumbling window is one kind of windowing operation which will discretize the stream into non overlapping windows. This means every record in the stream only belongs to one window. This kind of discretization allows observing the change in the stream over fixed intervals. There are other kind of windows supported in the flink which we will discuss in the future posts.&lt;/p&gt;

&lt;h2 id=&quot;windowed-wordcount-example&quot;&gt;Windowed Wordcount example&lt;/h2&gt;

&lt;p&gt;In last blog, we saw how to calculate wordcount using Flink API. In this post, we will be calculating wordcount for every 15 seconds. So in this example, we will be dividing the stream for every 15 seconds. Once those 15 seconds passes, the count will be started from zero.&lt;/p&gt;

&lt;p&gt;This example shows to how to snapshot wordcount for each 15 seconds to analyze the trend over time to say how wordcount have changed. This is not possible when we have continuous updation of the count as in earlier example.&lt;/p&gt;

&lt;p&gt;Most of the code to setup and run is same as earlier example. So I am only going to focus on how to add tumbling window to our stream&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordsStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code creates a window using timeWindow API. timeWindow API internally uses tumbling window API to do the windowing operation. In this case, the wordcount will be counted for each 15 seconds and then forgotten.&lt;/p&gt;

&lt;p&gt;You can access complete code &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/WindowedStreamingWordCount.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark-streaming-api&quot;&gt;Compared to Spark Streaming API&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;The tumbling windows in Flink are similar to microbatches in Spark. As in spark microbatch, tumbling windows are used for discretizing stream into independent batches.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Introducing Stream Windows in Apache Flink - &lt;a href=&quot;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&quot;&gt;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-2</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 1 : WordCount</title>
        <description>&lt;p&gt;Apache Flink is one of the new generation distributed systems which unifies batch and streaming processing. Earlier in my blog, I have &lt;a href=&quot;/introduction-to-flink-for-spark-developers-flink-vs-spark&quot;&gt;discussed&lt;/a&gt; about how it’s different than Apache Spark and also given a introductory &lt;a href=&quot;/introduction-to-flink-talk&quot;&gt;talk&lt;/a&gt; about it’s batch API. In batch world, Flink looks very similar to Spark API as it uses similar concepts from Map/Reduce. But in the case of streaming, flink is much different than the Spark or any other stream processing systems out there.&lt;/p&gt;

&lt;p&gt;So in these series of blogs, I will be discussing about how to get started with flink streaming API and using it’s different unique features. Flink streaming API has undergone significant changes from 0.10 to 1.0 version. So I will be discussing latest 1.0 API. You can access all the blogs in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this first blog, I will be discussing about how to run word count example in flink streaming. If you are new to flink, I encourage you to watch my &lt;a href=&quot;/introduction-to-flink-talk&quot;&gt;introductory talk&lt;/a&gt; before continuing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;flink-streaming-api&quot;&gt;Flink Streaming API&lt;/h2&gt;

&lt;p&gt;Flink provides a streaming API called as Flink DataStream API to process continuous unbounded streams of data in realtime. This API build on top of the pipelined streaming execution engine of flink.&lt;/p&gt;

&lt;p&gt;Datastream API has undergone a significant change from 0.10 to 1.0. So many examples you see in the other blogs including flink blog have become obsolete. I will be discussing about Flink 1.0 API which is released in maven central and yet to be released in binary releases.&lt;/p&gt;

&lt;h2 id=&quot;adding-dependency&quot;&gt;Adding dependency&lt;/h2&gt;

&lt;p&gt;To start using Datastream API, you should add the following dependency to project. I am using sbt for build management. You can also use other build tools like maven.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;org.apache.flink&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;flink-scala&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;1.0.0&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can access complete build.sbt &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/build.sbt&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;hello-world-example&quot;&gt;Hello World Example&lt;/h2&gt;

&lt;p&gt;Whenever we learn any new API in big data, it has become custom to do word count. In this example, we are reading some lines from a socket and doing word count on them.&lt;/p&gt;

&lt;p&gt;The below are the steps to write an streaming example in datastream API.&lt;/p&gt;

&lt;h3 id=&quot;step-1-get-streaming-environment&quot;&gt;Step 1. Get Streaming Environment&lt;/h3&gt;

&lt;p&gt;In both batch and streaming example, first step is to create a pointer to environment on which this program runs. Flink can run same program in local or cluster mode. You can read more about modes &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/apis/common/index.html#anatomy-of-a-flink-program&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;StreamExecutionEnvironment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getExecutionEnvironment&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you are familiar with Spark, StreamExecutionEnvironment is similar to spark context.&lt;/p&gt;

&lt;p&gt;One of the things to remember when using scala API of Flink is to import the implicts. If you don’t import them you will run into strange error messages.&lt;/p&gt;

&lt;p&gt;You can import the implicts for streaming as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.flink.streaming.api.scala._&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-2-create-datastream-from-socket&quot;&gt;Step 2. Create DataStream from socket&lt;/h3&gt;

&lt;p&gt;Once we have the pointer to execution environment, next step is to create a stream from socket.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;socketStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;socketStream&lt;/em&gt; will be of the type DataStream. DataStream is basic abstraction of flink’s streaming API.&lt;/p&gt;

&lt;h3 id=&quot;step-3-implement-wordcount-logic&quot;&gt;Step 3. Implement wordcount logic&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordsStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;socketStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;\\s+&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordsStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countPair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above is very standard code to do word count in map/reduce style. Notable differences are we are using keyBy rather than groupBy and sum for reduce operations. The value 0 and 1 in keyBy and sum calls signifies the index of columns in tuple to be used as key and values.&lt;/p&gt;

&lt;h3 id=&quot;step-4-print-the-word-counts&quot;&gt;Step 4. Print the word counts&lt;/h3&gt;

&lt;p&gt;Once we have wordcount stream, we want to call print, to print the values into standard output&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;countPair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-5-trigger-program-execution&quot;&gt;Step 5. Trigger program execution&lt;/h3&gt;

&lt;p&gt;All the above steps only defines the processing, but do not trigger execution. This needs to be done explicitly using execute.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have complete code for the word count example. You can access full code &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/StreamingWordCount.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;executing-code&quot;&gt;Executing code&lt;/h2&gt;

&lt;p&gt;To run this example, we need to start the socket at 9000 at following command to&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;nc -lk 9000&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once you do that, you can run the program from the IDE and &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/apis/cli.html&quot;&gt;command line interface&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can keep on entering the lines in nc command line and press enter. As you pass the lines you can observe the word counts printed on the stdout.&lt;/p&gt;

&lt;p&gt;Now we have successfully executed the our first flink streaming example.&lt;/p&gt;

&lt;h2 id=&quot;unbounded-state&quot;&gt;Unbounded state&lt;/h2&gt;

&lt;p&gt;If you observe the result, as an when you pass more rows the count keeps increasing. This indicates that flink keeps updating the count state indefinitely. This may be desired in some examples, but most of the use cases we want to limit the state to some certain time. We will see how to achieve it using window functionality in the next blog in the series.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark-streaming-api&quot;&gt;Compared to Spark Streaming API&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;The above code looks a lot similar to Spark streaming’s DStream API. Though syntax looks same there are few key differences. Some of them are&lt;/p&gt;

&lt;h3 id=&quot;no-need-of-batch-size-in-flink&quot;&gt;1. No need of Batch Size in Flink&lt;/h3&gt;

&lt;p&gt;Spark streaming needs batch size to be defined before any stream processing. It’s because spark streaming follows micro batches for stream processing which is also known as near realtime . But flink follows one message at a time way where each message is processed as and when it arrives. So flink doesnot need any batch size to be specified.&lt;/p&gt;

&lt;h3 id=&quot;state-management&quot;&gt;2. State management&lt;/h3&gt;

&lt;p&gt;In spark, after each batch, the state has to be updated explicitly if you want to keep track of wordcount across batches. But in flink the state is up-to-dated as and when new records arrive implicitly.&lt;/p&gt;

&lt;p&gt;We discuss more differences in future posts.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Apache Flink 1.0 Streaming Guide - &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-master/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Introducing Flink Streaming - &lt;a href=&quot;https://flink.apache.org/news/2015/02/09/streaming-example.html&quot;&gt;https://flink.apache.org/news/2015/02/09/streaming-example.html&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-1</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-1</guid>
      </item>
    
  </channel>
</rss>
