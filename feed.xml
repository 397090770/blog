<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Madhukar&#39;s Blog</title>
    <description>Thoughts on technology, life and everything else.</description>
    <link>http://blog.madhukaraphatak.com/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Introduction to Flink Streaming - Part 2 : Discretization of Stream using Window API</title>
        <description>&lt;p&gt;In the last &lt;a href=&quot;/introduction-to-flink-streaming-part-1&quot;&gt;blog&lt;/a&gt;, we discussed about how to do continuous stream processing using flink streaming API. Our wordcount example keeps on updating the counts as and when we received new data. This is good for some of the use cases. But in some use cases we want to aggregate some set of records in a given time interval, in order to keep track of variance over time. In those cases, we need to divide the stream into small batches. This discretization allow us to capture the change in aggregated value overtime. This discretized batches is also known as micro batches.&lt;/p&gt;

&lt;p&gt;In this second blog, I will be discussing about how to discretized the stream using flink’s window operation.We will be using same word count example in this post also. You can access all the blogs in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;window-in-streaming&quot;&gt;Window in streaming&lt;/h2&gt;

&lt;p&gt;Window is a mechanism to take a snapshot of the stream. This snapshot can be based on time or other variables. For example, if we create a window for 5 seconds then it will be all the records which arrived in the that time frame. You can define the window based on no of records or other stream specific variables also.&lt;/p&gt;

&lt;p&gt;Window allows us to understand change in stream data by taking snapshots in regular intervals. Flink API has wide variety of window support. In this post we are only going to focus on one kind of window known as Tumbling Windows.&lt;/p&gt;

&lt;h2 id=&quot;tumbling-window-in-flink-streaming&quot;&gt;Tumbling Window in Flink Streaming&lt;/h2&gt;

&lt;p&gt;Tumbling window is one kind of windowing operation which will discretize the stream into non overlapping windows. This means every record in the stream only belongs to one window. This kind of discretization allows observing the change in the stream over fixed intervals. There are other kind of windows supported in the flink which we will discuss in the future posts.&lt;/p&gt;

&lt;h2 id=&quot;windowed-wordcount-example&quot;&gt;Windowed Wordcount example&lt;/h2&gt;

&lt;p&gt;In last blog, we saw how to calculate wordcount using Flink API. In this post, we will be calculating wordcount for every 15 seconds. So in this example, we will be dividing the stream for every 15 seconds. Once those 15 seconds passes, the count will be started from zero.&lt;/p&gt;

&lt;p&gt;This example shows to how to snapshot wordcount for each 15 seconds to analyze the trend over time to say how wordcount have changed. This is not possible when we have continuous updation of the count as in earlier example.&lt;/p&gt;

&lt;p&gt;Most of the code to setup and run is same as earlier example. So I am only going to focus on how to add tumbling window to our stream&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordsStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code creates a window using timeWindow API. timeWindow API internally uses tumbling window API to do the windowing operation. In this case, the wordcount will be counted for each 15 seconds and then forgotten.&lt;/p&gt;

&lt;p&gt;You can access complete code &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/WindowedStreamingWordCount.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark-streaming-api&quot;&gt;Compared to Spark Streaming API&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;The tumbling windows in Flink are similar to microbatches in Spark. As in spark microbatch, tumbling windows are used for discretizing stream into independent batches.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Introducing Stream Windows in Apache Flink - &lt;a href=&quot;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&quot;&gt;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-2</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 1 : WordCount</title>
        <description>&lt;p&gt;Apache Flink is one of the new generation distributed systems which unifies batch and streaming processing. Earlier in my blog, I have &lt;a href=&quot;/introduction-to-flink-for-spark-developers-flink-vs-spark&quot;&gt;discussed&lt;/a&gt; about how it’s different than Apache Spark and also given a introductory &lt;a href=&quot;/introduction-to-flink-talk&quot;&gt;talk&lt;/a&gt; about it’s batch API. In batch world, Flink looks very similar to Spark API as it uses similar concepts from Map/Reduce. But in the case of streaming, flink is much different than the Spark or any other stream processing systems out there.&lt;/p&gt;

&lt;p&gt;So in these series of blogs, I will be discussing about how to get started with flink streaming API and using it’s different unique features. Flink streaming API has undergone significant changes from 0.10 to 1.0 version. So I will be discussing latest 1.0 API. You can access all the blogs in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this first blog, I will be discussing about how to run word count example in flink streaming. If you are new to flink, I encourage you to watch my &lt;a href=&quot;/introduction-to-flink-talk&quot;&gt;introductory talk&lt;/a&gt; before continuing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;flink-streaming-api&quot;&gt;Flink Streaming API&lt;/h2&gt;

&lt;p&gt;Flink provides a streaming API called as Flink DataStream API to process continuous unbounded streams of data in realtime. This API build on top of the pipelined streaming execution engine of flink.&lt;/p&gt;

&lt;p&gt;Datastream API has undergone a significant change from 0.10 to 1.0. So many examples you see in the other blogs including flink blog have become obsolete. I will be discussing about Flink 1.0 API which is released in maven central and yet to be released in binary releases.&lt;/p&gt;

&lt;h2 id=&quot;adding-dependency&quot;&gt;Adding dependency&lt;/h2&gt;

&lt;p&gt;To start using Datastream API, you should add the following dependency to project. I am using sbt for build management. You can also use other build tools like maven.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;org.apache.flink&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;flink-scala&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;1.0.0&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can access complete build.sbt &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/build.sbt&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;hello-world-example&quot;&gt;Hello World Example&lt;/h2&gt;

&lt;p&gt;Whenever we learn any new API in big data, it has become custom to do word count. In this example, we are reading some lines from a socket and doing word count on them.&lt;/p&gt;

&lt;p&gt;The below are the steps to write an streaming example in datastream API.&lt;/p&gt;

&lt;h3 id=&quot;step-1-get-streaming-environment&quot;&gt;Step 1. Get Streaming Environment&lt;/h3&gt;

&lt;p&gt;In both batch and streaming example, first step is to create a pointer to environment on which this program runs. Flink can run same program in local or cluster mode. You can read more about modes &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/apis/common/index.html#anatomy-of-a-flink-program&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;StreamExecutionEnvironment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getExecutionEnvironment&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you are familiar with Spark, StreamExecutionEnvironment is similar to spark context.&lt;/p&gt;

&lt;p&gt;One of the things to remember when using scala API of Flink is to import the implicts. If you don’t import them you will run into strange error messages.&lt;/p&gt;

&lt;p&gt;You can import the implicts for streaming as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.flink.streaming.api.scala._&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-2-create-datastream-from-socket&quot;&gt;Step 2. Create DataStream from socket&lt;/h3&gt;

&lt;p&gt;Once we have the pointer to execution environment, next step is to create a stream from socket.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;socketStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;socketStream&lt;/em&gt; will be of the type DataStream. DataStream is basic abstraction of flink’s streaming API.&lt;/p&gt;

&lt;h3 id=&quot;step-3-implement-wordcount-logic&quot;&gt;Step 3. Implement wordcount logic&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordsStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;socketStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;\\s+&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordsStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countPair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above is very standard code to do word count in map/reduce style. Notable differences are we are using keyBy rather than groupBy and sum for reduce operations. The value 0 and 1 in keyBy and sum calls signifies the index of columns in tuple to be used as key and values.&lt;/p&gt;

&lt;h3 id=&quot;step-4-print-the-word-counts&quot;&gt;Step 4. Print the word counts&lt;/h3&gt;

&lt;p&gt;Once we have wordcount stream, we want to call print, to print the values into standard output&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;countPair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-5-trigger-program-execution&quot;&gt;Step 5. Trigger program execution&lt;/h3&gt;

&lt;p&gt;All the above steps only defines the processing, but do not trigger execution. This needs to be done explicitly using execute.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have complete code for the word count example. You can access full code &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/StreamingWordCount.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;executing-code&quot;&gt;Executing code&lt;/h2&gt;

&lt;p&gt;To run this example, we need to start the socket at 9000 at following command to&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;nc -lk 9000&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once you do that, you can run the program from the IDE and &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/apis/cli.html&quot;&gt;command line interface&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can keep on entering the lines in nc command line and press enter. As you pass the lines you can observe the word counts printed on the stdout.&lt;/p&gt;

&lt;p&gt;Now we have successfully executed the our first flink streaming example.&lt;/p&gt;

&lt;h2 id=&quot;unbounded-state&quot;&gt;Unbounded state&lt;/h2&gt;

&lt;p&gt;If you observe the result, as an when you pass more rows the count keeps increasing. This indicates that flink keeps updating the count state indefinitely. This may be desired in some examples, but most of the use cases we want to limit the state to some certain time. We will see how to achieve it using window functionality in the next blog in the series.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark-streaming-api&quot;&gt;Compared to Spark Streaming API&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;The above code looks a lot similar to Spark streaming’s DStream API. Though syntax looks same there are few key differences. Some of them are&lt;/p&gt;

&lt;h3 id=&quot;no-need-of-batch-size-in-flink&quot;&gt;1. No need of Batch Size in Flink&lt;/h3&gt;

&lt;p&gt;Spark streaming needs batch size to be defined before any stream processing. It’s because spark streaming follows micro batches for stream processing which is also known as near realtime . But flink follows one message at a time way where each message is processed as and when it arrives. So flink doesnot need any batch size to be specified.&lt;/p&gt;

&lt;h3 id=&quot;state-management&quot;&gt;2. State management&lt;/h3&gt;

&lt;p&gt;In spark, after each batch, the state has to be updated explicitly if you want to keep track of wordcount across batches. But in flink the state is up-to-dated as and when new records arrive implicitly.&lt;/p&gt;

&lt;p&gt;We discuss more differences in future posts.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Apache Flink 1.0 Streaming Guide - &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-master/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Introducing Flink Streaming - &lt;a href=&quot;https://flink.apache.org/news/2015/02/09/streaming-example.html&quot;&gt;https://flink.apache.org/news/2015/02/09/streaming-example.html&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-1</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-1</guid>
      </item>
    
      <item>
        <title>Interactive Scheduling using Azkaban - Part 2 : Challenges in scheduling interactive workloads</title>
        <description>&lt;p&gt;Every big data application needs some kind of scheduling to run daily jobs. So over the years having a good stable scheduling systems for hadoop, spark jobs has become more and more important. The different workloads in big data have different requirements from  the scheduler. So in this blog post I will be discussing about different scheduling requirements for batch, streaming and interactive usecases and challenges associated with interactive workload.&lt;/p&gt;

&lt;p&gt;This is second post in series of blogs where I will be discussing about using Azkaban scheduler to do interactive scheduling. You can access all other posts from the series &lt;a href=&quot;/categories/azkaban&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;scheduling-needs-of-different-big-data-workloads&quot;&gt;Scheduling needs of different big data workloads&lt;/h2&gt;

&lt;p&gt;The below are the different requirements of big data workloads from scheduler system.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;batch&quot;&gt;Batch&lt;/h3&gt;
    &lt;p&gt;Set of jobs which needs to be executed on timely manner. In this scenario, a scheduler system needs to allow user to define the script with all the dependencies of a flow and allow it to be scheduled. To add/modify the jobs user will normally changes the script and runs the updated ones. The examples for these kind of scheduler systems are Ozzie, airflow etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;streaming&quot;&gt;Streaming&lt;/h3&gt;
    &lt;p&gt;Continuous stream of data is processed to produce results. Normally streaming only needs scheduler to initiate stream processing system and from there streaming framework will take over.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above two scenarios are one of most supported and common place in big data world from quite sometime. So all the scheduling system, including azkaban, supports them well. But there is a new workload emerging these days which needs special attention.&lt;/p&gt;

&lt;h2 id=&quot;interactive-big-data-workload&quot;&gt;Interactive big data workload&lt;/h2&gt;

&lt;p&gt;As spark became popular, it has made interactive programming as one of the important part of big data workloads. In interactive settings, a user will be analyzing the data adhocly and once he/she is happy with the steps then they want to schedule them to run in timely manner.&lt;/p&gt;

&lt;p&gt;The notebook systems like Zeppelin,Jupiter have made interactive programming highly popular. Initially used for the data science use cases, they are also used for data engineering use cases these days.&lt;/p&gt;

&lt;p&gt;So as interactive workloads becoming common place, supporting ability to scheduling jobs interactively becoming more and more important. But doing this with existing systems is not easy.&lt;/p&gt;

&lt;h2 id=&quot;challenges-of-scheduling-interactive-workloads&quot;&gt;Challenges of scheduling Interactive workloads&lt;/h2&gt;

&lt;p&gt;Unlike batch workloads, interactive workloads are not static. They evolve as user adds/removes the code. Normally user may want to add / remove scheduling on the fly rather than modifying the script. So the non azkaban frameworks cannot be used in scenario because of following reasons.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;nolimited-rest-api-support&quot;&gt;No/Limited REST API support&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of the scheduling systems like oozie have very limited support for programmatic access. Often they rely upon the traditional scripting world, where you need to configure jobs using script and submit them. It works great for batch, but cannot be used for interactive applications as they need an good programmatic API to schedule jobs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;lack-of-good-user-interface-for-monitoring&quot;&gt;Lack of good user interface for monitoring&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of the scheduling system have very limited user interfaces. Most of them limit themselves to show work flow graphs. Also many of them doesn’t allow users to extend the user interfaces which results in building the custom ones themselves.&lt;/p&gt;

&lt;p&gt;In batch, normally user interface is not that important. But in interactive it plays a huge role. Ability to monitor the jobs in a small time frame is important as it results in a good user feed back.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;support-for-different-executors&quot;&gt;Support for different executors&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many scheduling systems limit themselves for Hadoop or Spark. But in interactive application one often likes to run different kind of processing on same system. So ability to run different workloads becomes extremely important.&lt;/p&gt;

&lt;p&gt;So from above points it’s clear that the most of the existing scheduler systems are geared towards the batch processing scenarios. So using them in a interactive application is hard.&lt;/p&gt;

&lt;h2 id=&quot;note-on-azkaban-for-batch-workload&quot;&gt;Note on Azkaban for batch workload&lt;/h2&gt;

&lt;p&gt;Though my blog posts are focusing on using azkaban for interactive workloads, azkaban fares well in the batch also. Even most of it’s documentation is dedicated to do batch scheduling using it’s web UI rather than for interactive workloads. But with it’s hidden gem of REST API, it’s well suited for the interactive applications too.&lt;/p&gt;

&lt;p&gt;So in this blogpost, we discussed about challenges in scheduling interactive workloads. In the next blogpost, we are going to discuss how azkaban solves these issues and is good candidate scheduler framework for interactive workloads.&lt;/p&gt;

</description>
        <pubDate>Mon, 07 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/interactive-scheduling-using-azkaban-challenges-in-scheduling-interactive-workloads</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/interactive-scheduling-using-azkaban-challenges-in-scheduling-interactive-workloads</guid>
      </item>
    
      <item>
        <title>What&#39;s New in Spark : Tales from Spark Summit East - Framework Improvements</title>
        <description>&lt;p&gt;Recently Databricks, company behind the Apache Spark, held this year’s first &lt;a href=&quot;https://spark-summit.org/east-2016/&quot;&gt;spark summit&lt;/a&gt;, spark developer conference, in new york city. Lots of new exciting improvements in spark and it’s ecosystem got discussed in various talks. I was watching the videos of the conference on youtube and wanted to share the ones I found interesting.&lt;/p&gt;

&lt;p&gt;These are the series of blog posts focusing on various talks categorized into different aspects of Spark. You can access all the posts in the series &lt;a href=&quot;/categories/spark-summit-east-2016&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is the first post in the series where I will be sharing talks which focused on improvements to the core of the spark itself.&lt;/p&gt;

&lt;h3 id=&quot;matei-zaharia-keynote-on-spark-20&quot;&gt;1. Matei Zaharia keynote on Spark 2.0&lt;/h3&gt;

&lt;p&gt;Matei Zaharia, Spark’s creator, laid out plans for next version of Spark in his keynote. His talks mainly revolved around performance and new abstraction like Dataset. Spark 2.0 is one of the major steps in spark’s evolution.You can read more about my thoughts on Spark 2.0 &lt;a href=&quot;/introduction-to-spark-2.0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The below is the video of the talk. You can find slides on &lt;a href=&quot;http://www.slideshare.net/databricks/2016-spark-summit-east-keynote-matei-zaharia&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/ZFBgY0PwUeY&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

&lt;h3 id=&quot;structuring-spark-dataframes-datasets-and-spark-streaming&quot;&gt;2. Structuring Spark: Dataframes, Datasets and Spark Streaming&lt;/h3&gt;

&lt;p&gt;Structured data analysis has become very important part of spark in last few releases. More and more work is done on Dataframe API compared to RDD API. In this talk speaker talks about how these API’s share common core and how they are planning to bring the same API’s for stream analysis also.&lt;/p&gt;

&lt;p&gt;The below is the video of the talk. You can find slides on &lt;a href=&quot;http://www.slideshare.net/SparkSummit/structuring-spark-dataframes-datasets-and-streaming-by-michael-armbrust&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/i7l3JQRx7Qw&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

&lt;h3 id=&quot;the-future-of-real-time-spark---a-revamped-spark-streaming&quot;&gt;3. The Future of Real Time Spark - A Revamped Spark Streaming&lt;/h3&gt;

&lt;p&gt;Building streaming application is hard. Combining batch processing and stream processing in a single application needs a lot of design and detailed implementation. Compared to other components of Spark, there was not much going on in spark streaming for a while. But its more the case. Spark 2.0 going to bring a completely new revamped API for spark streaming.&lt;/p&gt;

&lt;p&gt;The below is the video of the talk. You can find slides on &lt;a href=&quot;http://www.slideshare.net/rxin/the-future-of-realtime-in-spark&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/oXkxXDG0gNk&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

&lt;h3 id=&quot;spark-performance-whats-next---10x-performance-improvement-in-spark-20&quot;&gt;4. Spark performance: What’s Next - 10x performance improvement in Spark 2.0&lt;/h3&gt;

&lt;p&gt;With introduction of tungsten and codegen in 1.4, spark performance is significantly improved in last few releases. Spark 2.0 bring whole new set of techniques which going to take the spark performance to next level. In this talk, speaker talks about different techniques getting developed to improve spark performance. Most of these already in master branch which you can start using to test it yourself.&lt;/p&gt;

&lt;p&gt;The below is the video of the talk. You can find slides on &lt;a href=&quot;http://www.slideshare.net/databricks/spark-performance-whats-next&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/JX0CdOTWYX4&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

&lt;p&gt;In next blog in the series, I will be sharing my thoughts on the talks which focused on the ecosystem around spark.&lt;/p&gt;
</description>
        <pubDate>Sat, 05 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/whats-new-in-spark-framework-improvements</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/whats-new-in-spark-framework-improvements</guid>
      </item>
    
      <item>
        <title>Introduction to Spark 2.0 : A Sneak Peek At Next Generation Spark</title>
        <description>&lt;p&gt;Spark 2.0 is the next stable of release of spark, which is expected to be released in April/May 2016. As the major version bump suggests, its is going to be bring some drastic changes to framework. In recent &lt;a href=&quot;https://spark-summit.org/east-2016/&quot;&gt;spark summit&lt;/a&gt;, spark contributors discussed some of those in various talks.&lt;/p&gt;

&lt;p&gt;In this post, I am going to talk about some of the important changes made to the framework and as a spark developer how we can prepare for it.&lt;/p&gt;

&lt;h2 id=&quot;embrace-dataset-dataframe-api-over-rdd-api&quot;&gt;1. Embrace Dataset/ Dataframe API over RDD API&lt;/h2&gt;

&lt;p&gt;Normally, whoever starts learning spark first learns about RDD abstraction. RDD was one of the novel idea of Spark which gave us a single abstraction over different big data workloads like batch, streaming, ML etc. So naturally RDD API became the way people build spark applications.&lt;/p&gt;

&lt;p&gt;But overtime, people have realized RDD as a user facing API held back spark runtime from advanced optimizations. So from Spark 1.3, Dataframe API was introduced to solve some of the optimization issues. Dataframe brought custom memory management and runtime code generation which greatly improved performance. So in last year most of the improvements went into Dataframe API whereas RDD API stood still.&lt;/p&gt;

&lt;p&gt;Though dataframe API solved many issues, it was not a good enough replacement for RDD API. One of the major issues with dataframe API was no compile time safety and not able to work with domain objects. So this held back people using dataframe API everywhere. But with introduction of Dataset API in 1.6, we were able to fill the gap.&lt;/p&gt;

&lt;p&gt;So in Spark 2.0, Dataset API will be become a stable API. So Dataset API combined with Dataframe API should able to cover most of the use cases where RDD was used earlier. So as a spark developer it is advised to start embracing these two API’s over RDD API from Spark 2.0.&lt;/p&gt;

&lt;p&gt;Does that mean RDD API will be removed? Not really. Spark as a project is very serious about backward compatibility. So they don’t want to remove any stable API’s. So RDD API will remain as low level API mostly used by runtime. As developer you will be using Dataset or Dataframe API from Spark 2.0.&lt;/p&gt;

&lt;p&gt;Everything above sounds great for batch processing, but what about Spark streaming? Spark streaming has lot of API’s around RDD. What will happen to them?&lt;/p&gt;

&lt;h2 id=&quot;structured-stream-processing&quot;&gt;2. Structured Stream processing&lt;/h2&gt;

&lt;p&gt;Spark 2.0 will introduce structured stream processing, which is a higher level API to do stream processing. It essentially going to leverage dataframe and dataset API for stream processing which will get rid of using RDD as an abstraction. Not only this bring dataframes to stream processing, it brings bunch of other benefits like datasource API support for spark streaming. You can watch &lt;a href=&quot;https://www.youtube.com/watch?v=i7l3JQRx7Qw&quot;&gt;this video&lt;/a&gt; to know more.&lt;/p&gt;

&lt;p&gt;So from Spark 2.0, you will be interacting with spark streaming using same DF abstraction that you were using in batch layer.&lt;/p&gt;

&lt;h2 id=&quot;dataset-is-the-new-single-abstraction&quot;&gt;3. Dataset is the new single abstraction&lt;/h2&gt;

&lt;p&gt;Spark always loved to have a single abstraction which it made it to improve in a rapid phase. Also it meant that different libraries can exchange data between each other. So doesn’t having Dataframe and Dataset as two API’s beats that single abstraction idea?&lt;/p&gt;

&lt;p&gt;Currently in Spark 1.6, these are two independent abstractions. But from 2.0 Dataframe will be a special case of Dataset API. So this make dataset as a single abstraction on which all the API’s are build. So Dataset will be new single abstraction which will take the place of RDD in the user API world.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;4. Performance improvements&lt;/h2&gt;

&lt;p&gt;Spark 2.0 going to bring many performance improvements thanks to tungsten and code generation. It’s been one of the constant theme in last few releases to going 1-2x performance gains. But in 2.0, data bricks is promising 6-10x performance gains. It’s particularly do with more and more intelligent code generation and moving libraries on better abstraction layers like dataset.&lt;/p&gt;

&lt;p&gt;So the above are the most important updates landing in Spark 2.0. I will be discussing more about this as and when we get to see these changes getting implemented.&lt;/p&gt;

&lt;p&gt;Source : You can access all the talks and slides of spark summit from &lt;a href=&quot;https://spark-summit.org/east-2016/schedule/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Fri, 04 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-spark-2.0</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-spark-2.0</guid>
      </item>
    
      <item>
        <title>Interactive Scheduling using Azkaban - Part 1 : Setting up Solo Server</title>
        <description>&lt;p&gt;Azkaban is a scheduler for big data workloads like Hadoop, Spark. One of the differentiator of azkaban compared to other schedulers like oozie, airflow is it has good support for REST API to interact with scheduler problematically. This programmatic access is important for interactive applications.&lt;/p&gt;

&lt;p&gt;In these series of blogs I will be discussing about setting up azkaban and using azkaban AJAX(REST) API.&lt;/p&gt;

&lt;p&gt;This is the first post in series, where we discuss about setting up azkaban. In this post, we will be setting up azkaban 3.0.&lt;/p&gt;

&lt;h2 id=&quot;building-azkaban&quot;&gt;Building Azkaban&lt;/h2&gt;

&lt;p&gt;Though azkaban provides binary &lt;a href=&quot;http://azkaban.github.io/downloads.html&quot;&gt;downloads&lt;/a&gt; it is not up to date. So we will be getting latest code from the github in order to build azkaban 3.0.&lt;/p&gt;

&lt;p&gt;The following are the steps to get code and build.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;clone-code&quot;&gt;Clone code&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;git clone https://github.com/azkaban/azkaban.git&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;build&quot;&gt;Build&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;./gradlew distZip&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;copy-from-build&quot;&gt;Copy from build&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;cp build/distributions/azkaban-solo-server-3.0.0.zip ~&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;installing-solo-server&quot;&gt;Installing solo server&lt;/h2&gt;

&lt;p&gt;Azkaban supports different mode of executions like solo server, two server mode and multiple executor mode. Solo server is used for initial developments where as other ones are geared towards production scenarios. In this blog, we discuss about setting up solo server, for other modes refer &lt;a href=&quot;http://azkaban.github.io/azkaban/docs/latest/#getting-started&quot;&gt;azkaban documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The below are the steps for installing.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;unzip&quot;&gt;Unzip&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;unzip ~/azkaban-solo-server-3.0.0.zip
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/azkaban-solo-server-3.0.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;starting-solo-server&quot;&gt;Starting solo server&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;bin/azkaban-solo-start.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;accessing-log&quot;&gt;Accessing log&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;tail -f logs/azkaban-execserver.log&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;accessing-web-ui&quot;&gt;Accessing web UI&lt;/h2&gt;

&lt;p&gt;Once azkaban solo server started, you can access at &lt;a href=&quot;http://localhost:8081/&quot;&gt;http://localhost:8081&lt;/a&gt;. By default username is &lt;em&gt;azkaban&lt;/em&gt; and password is &lt;em&gt;azkaban&lt;/em&gt;. You can change it in &lt;em&gt;conf/azkaban-users.xml&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now you have successfully installed azkaban server. In the next set of posts, we will explore how to use this installation to do scheduling.&lt;/p&gt;
</description>
        <pubDate>Thu, 03 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/interactive-scheduling-using-azkaban-setting-up-solo-server</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/interactive-scheduling-using-azkaban-setting-up-solo-server</guid>
      </item>
    
      <item>
        <title>Building Distributed Systems from Scratch - Part 2 : Handling third party libraries</title>
        <description>&lt;p&gt;The below video is a recording of my talk on &lt;em&gt;Building Distributed Systems from Scratch - Part 2&lt;/em&gt; in recent spark meet up. It’s second talk in series of talks about how to build a distributed processing system from scratch which looks similar to Apache Spark. This talk focuses on how to implement a system to distribute third party libraries on mesos.&lt;/p&gt;

&lt;p&gt;Find the slides on &lt;a href=&quot;http://www.slideshare.net/datamantra/building-distributed-processing-system-from-scratch-part-2&quot;&gt;slideshare&lt;/a&gt; and code on &lt;a href=&quot;https://github.com/phatak-dev/distributedsystems&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/Oj8IO8OICAc&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;
</description>
        <pubDate>Wed, 17 Feb 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/building-distributed-systems-from-scratch-part2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/building-distributed-systems-from-scratch-part2</guid>
      </item>
    
      <item>
        <title>Introduction to Hadoop (HDFS &amp; Map/Reduce) for Spark developers</title>
        <description>&lt;p&gt;Whenever I talk about Spark in meetup or training, people ask one question &lt;strong&gt;“Do I need to know hadoop for learning Spark?”&lt;/strong&gt;. My answer to this question was “not in the beginning”. In my view, to learn spark you don’t need to know about hadoop. But if you want to be proficient in spark, then knowing hadoop concepts is a must.&lt;/p&gt;

&lt;p&gt;So in below video I have captured neccessary HDFS and Map/Reduce concepts which are needed for improving understanding of Spark. This video is for all the ones who has some understanding of Spark and want to know how ideas from hadoop and spark connect.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/strJwh0hLT8&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;
</description>
        <pubDate>Sun, 07 Feb 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-hadoop-for-spark-developers</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-hadoop-for-spark-developers</guid>
      </item>
    
      <item>
        <title>Introduction to Apache Flink - Meetup talk</title>
        <description>&lt;p&gt;The below video is recording of my talk on &lt;em&gt;Introduction to Apache Flink&lt;/em&gt; in recent spark meetup. In this talk, we will discuss how apache flink is evolving
as new generation platform for big data processing. We also discuss how flink
compares to apache spark.&lt;/p&gt;

&lt;p&gt;Find the slides on &lt;a href=&quot;http://www.slideshare.net/datamantra/introduction-to-apache-flink-56892153&quot;&gt;slideshare&lt;/a&gt; and code on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/jErEhxP8LYQ&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-talk</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-talk</guid>
      </item>
    
      <item>
        <title>Introduction to Apache Flink for Spark Developers : Flink vs Spark</title>
        <description>&lt;style type=&quot;text/css&quot;&gt;
.post-content blockquote {
    color: #A50707;
    font: bold;
    font-size: 20px;
    border-left: none;
}
&lt;/style&gt;

&lt;p&gt;Does world need yet another big data processing system? That was the question popped up when I first heard of the Apache Flink. In big data space we don’t have dearth of frameworks. But we do have shortcoming of cohesive platform which can solve all our different data processing needs. Apache spark seems to be the best framework in town which is trying to solve that problem. So I was skeptic about need of yet another framework which has similar goals.&lt;/p&gt;

&lt;p&gt;In last few weeks I started spending some time on flink out of curiosity. Initially when I looked at the standard examples they looked very similar to one of the Spark. So I started with the impression that its just another framework which is mimicking the functionality of the spark. But as I spent more and more time, it was apparent that, there are  few novel ideas behind those same look API’s which makes flink stand apart from spark. I got fascinated by those ideas and spent more and more and time understanding and exploring those.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Many of the flink ideas like custom memory management, dataset API are already finding their home in Spark which proves that those ideas are really good. So understanding flink may help us to understand what’s going to be the future of the distributed data processing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this post I am tried put together my first impressions of Apache flink as a spark developer. This rant/review is heavily biased as I spent my last two years in Spark and just 2-3 weeks playing with Apache flink. So take all the things I say here with grain of salt.&lt;/p&gt;

&lt;h2 id=&quot;what-is-apache-flink&quot;&gt;What is Apache Flink?&lt;/h2&gt;

&lt;p&gt;Apache Flink is yet another new generation general big data processing engine which targets to unify different data loads. Does it sounds like Apache Spark? Exactly. Flink is trying to address same issue that Spark trying to solve. Both systems are targeted towards building the single platform where you can run batch, streaming, interactive , graph processing , ML etc. So flink does not differ much from  Spark interms of ideology. But they do differ a lot in the implementation details.&lt;/p&gt;

&lt;p&gt;So in the following section I will be comparing different aspects of the spark and flink. Some of the approaches are same in both frameworks and some differ a lot.&lt;/p&gt;

&lt;h2 id=&quot;apache-spark-vs-apache-flink&quot;&gt;Apache Spark vs Apache Flink&lt;/h2&gt;

&lt;h3 id=&quot;abstraction&quot;&gt;1. Abstraction&lt;/h3&gt;

&lt;p&gt;In Spark, for batch we have &lt;strong&gt;RDD&lt;/strong&gt; abstraction and &lt;strong&gt;DStream&lt;/strong&gt; for streaming which is internally RDD itself. So all the data we represent in Spark underneath represented using RDD abstraction.&lt;/p&gt;

&lt;p&gt;In flink, we have &lt;strong&gt;Dataset&lt;/strong&gt; abstraction for batch and &lt;strong&gt;DataStreams&lt;/strong&gt; for the streaming application. They sound very similar to RDD and DStreams but they are not. The differences are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dataset are represented as plans in runtime&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In spark RDD are represented as java objects in the runtime. With introduction of Tungsten, it is changed little bit. But in Apache flink Dataset is represented as a logical plan. Does it sound familiar? Yes they are like dataframes in Spark. So in flink you get Dataframe like api as first class citizen which are optimized using an optimizer. But in Spark RDD don’t do any optimization in between.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dataset of flink  are like Dataframe API of spark which are optimized before executed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In spark 1.6, dataset API is getting added to spark, which may eventually replace RDD abstraction.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dataset and DataStream are independent API’s&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In Spark all the different abstractions like DStream, Dataframe  are built on top of RDD abstraction. But in flink, Dataset and DataStream are two independent abstractions built on top common engine. Though they mimic the similar API, you cannot combine those together as you can do in case of DStream and RDD. Though there are &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-2320&quot;&gt;some efforts&lt;/a&gt; in this direction, there is not enough clarity what will be the end result.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We cannot combine DataSet and DataStreams like RDD and DStreams.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So though both flink and spark have similar abstractions, their implementation differs.&lt;/p&gt;

&lt;h2 id=&quot;memory-management&quot;&gt;Memory management&lt;/h2&gt;

&lt;p&gt;Till spark 1.5, Spark used Java heap for caching data. Though it was easier for project to start with, it resulted in OOM issues and gc pauses. So from 1.5, spark moved into custom memory management which is called as project tungsten.&lt;/p&gt;

&lt;p&gt;Flink did custom memory management from day one. Actually it was one of the inspiration for Spark to move in that direction. Not only flink stores data in it’s custom binary layout, it does operate on binary data directly. In spark all dataframe operations are operated directly on tungsten binary data from 1.5.&lt;/p&gt;

&lt;p&gt;Doing custom memory management on JVM result in better performance and better resource utilization.&lt;/p&gt;

&lt;h2 id=&quot;language-of-implementation&quot;&gt;Language of implementation.&lt;/h2&gt;

&lt;p&gt;Spark is implemented in Scala. It provides API’s in other languages like Java,Python and R.&lt;/p&gt;

&lt;p&gt;Flink is implemented in Java. It does provide Scala API too.&lt;/p&gt;

&lt;p&gt;So language of choice is better in Spark compared to flink. Also in some of the scala API’s of flink, the java abstractions does API’s. I think this will improve as they get more users for scala API. I am not much aware of Java API’s both in Spark and Flink as I moved to Scala long back.&lt;/p&gt;

&lt;h2 id=&quot;api&quot;&gt;API&lt;/h2&gt;

&lt;p&gt;Both Spark and Flink mimic scala collection API’s. So from surface both API’s look very similar. Following is the scala word count using RDD and Dataset API.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// Spark wordcount&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;WordCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;local&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;wordCount&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;hi&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;how are you&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;hi&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataSet&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallelize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataSet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;\\s+&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mappedWords&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mappedWords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduceByKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt;

  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// Flink wordcount&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;WordCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ExecutionEnvironment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getExecutionEnvironment&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;hi&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;how are you&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;hi&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataSet&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fromCollection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataSet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;\\s+&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mappedWords&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grouped&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mappedWords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grouped&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Though I am not sure, is this coincidence or deliberate, having very similar API
s does help to switch between these frameworks very easily. It seems that the collection API going to be the standard API to do data pipeline in near future. Even Martin Odersky, creator of Scala, &lt;a href=&quot;https://www.youtube.com/watch?v=NW5h8d_ZyOs&quot;&gt;acknowledges&lt;/a&gt; this fact.&lt;/p&gt;

&lt;h2 id=&quot;streaming&quot;&gt;Streaming&lt;/h2&gt;

&lt;p&gt;Apache Spark looks at streaming as fast batch processing. Where as Apache flink looks at batch processing as the special case of stream processing. Both of these approaches have fascinating implications. The some of the differences or implications of the two different approaches are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Realtime vs Near Realtime&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apache flink provides event level processing which is also known as real time streaming. It’s very similar to the Storm model.&lt;/p&gt;

&lt;p&gt;In case of Spark, you get mini batches which doesn’t provide event level granularity. This approach is known as near real-time.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Spark streaming is faster batch processing and Flink batch processing is bounded streaming processing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Though most of the applications are ok with near realtime, there are few applications who need event level realtime processing. These applications normally storm rather than Spark streaming. For them flink going to be very interesting alternative.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Ability to combine the historical data with stream&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the advantage of running streaming processing as faster batch is, then we can use same abstractions in the both cases. Spark has excellent support for combining batch and stream data because both underneath are using rdd abstraction.&lt;/p&gt;

&lt;p&gt;In case of flink, batch and streaming don’t share same api abstractions. So though there are ways to combine historical file based data with stream it is not that clean as Spark.&lt;/p&gt;

&lt;p&gt;In many application this ability is very important. In these applications Spark shines in place of Flink streaming.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Flexible windowing&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Due to nature of mini batches, support for windowing is very limited in Spark as of now. Only you can window the batches based on the process time.&lt;/p&gt;

&lt;p&gt;Flink provides very flexible windowing system compared to any other system out there. Window is one of the major focus of the flink streaming API’s. It allows window based on process time, data time, no of records etc etc. This flexibility makes flink streaming API very powerful compared to spark ones.&lt;/p&gt;

&lt;p&gt;I am not sure how easy to bring those API’s to Spark, so till that time flink has superior window API compared to the Spark streaming.&lt;/p&gt;

&lt;h2 id=&quot;sql-interface&quot;&gt;SQL interface&lt;/h2&gt;

&lt;p&gt;One of the most active Spark library as of now is spark-sql. Spark provided both Hive like query language and Dataframe like DSL for querying structured data. It is matured API and getting used extensively both in batch and soon to be in streaming world.&lt;/p&gt;

&lt;p&gt;As of now, Flink Table API only supports dataframe like DSL and it’s still in beta. There are plans to add the sql interface but not sure when it will land in framework.&lt;/p&gt;

&lt;p&gt;So as of now Spark has good sql story compared to flink. I think flink will catch up as it was late into the game compared to Spark.&lt;/p&gt;

&lt;h2 id=&quot;data-source-integration&quot;&gt;Data source Integration&lt;/h2&gt;

&lt;p&gt;Spark data source API is one the best API’s in the framework. The data source API made all the smart sources like NoSQL databases, parquet , ORC as the first class citizens on spark. Also this API provides the ability to do advanced operations like predicate push down in the source level.&lt;/p&gt;

&lt;p&gt;Flink still relies heavily upon the map/reduce InputFormat to do the data source integration. Though it
s good enough API to pull the data it’s can’t make use of source abilities smartly. So flink lags behind the data source integration as of now.&lt;/p&gt;

&lt;h2 id=&quot;iterative-processing&quot;&gt;Iterative processing&lt;/h2&gt;

&lt;p&gt;One of the most talked feature of Spark is ability to do machine learning effectively. With in memory caching and other implementation details its really powerful platform to implement ML algorithms.&lt;/p&gt;

&lt;p&gt;Though ML algorithm is a cyclic data flow it’s represented as direct acyclic graph inside the spark. Normally no distributed processing systems encourage having cyclic data flow as they become tricky to reason about.&lt;/p&gt;

&lt;p&gt;But flink takes little bit different approach to others. They support controlled cyclic dependency graph in runtime. This makes them to represent the ML algorithms in a very efficient way compared to DAG representation. So the Flink supports the iterations in native platform which results in superior scalability and performance compared to DAG approach.&lt;/p&gt;

&lt;p&gt;I hope spark also start supporting this in framework which will benefit the ML community immensely.&lt;/p&gt;

&lt;h2 id=&quot;stream-as-platform-vs-batch-as-platform&quot;&gt;Stream as platform vs Batch as Platform&lt;/h2&gt;

&lt;p&gt;Apache Spark comes from the era of Map/Reduce which represents whole computation as the movement of the data as collections of the files. These files may be sitting in memory as arrays or physical files on the disk. This has very nice properties like fault tolerance etc.&lt;/p&gt;

&lt;p&gt;But Flink is new kind of systems which represents the whole computation as the stream processing where data is moved contentiously without any barriers. This idea is very similar to new reactive streams systems like akka-streams.&lt;/p&gt;

&lt;p&gt;Though with my limited research it’s not very apparent that which one is the future of big data systems, doing everything as stream seems to picking up these days. So in that sense flink breathes a fresh air into way we think about big data systems.&lt;/p&gt;

&lt;h2 id=&quot;maturity&quot;&gt;Maturity&lt;/h2&gt;

&lt;p&gt;After knowing all the differences, one question you may ask is Flink production ready like Spark? I argue it’s not fully ready. There are parts like batch which already in production, but other pieces like streaming , table API are still getting evolved. It’s not saying that people are not using flink streaming in production. There are some brave hearts out there who are doing that. But as mass market tool its need to be matured and stabilized over course of time.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At this point of time Spark is much mature and complete framework compared to Flink. But flink does bring very interesting ideas like custom memory management, data set API etc to the table. Spark community is recognizing it and adopting these ideas into spark. So in that sense flink is taking big data processing to next level altogether. So knowing flink API and internals will help you to understand this new stream paradigm shift much before it lands in Spark.&lt;/p&gt;

</description>
        <pubDate>Sun, 06 Dec 2015 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-for-spark-developers-flink-vs-spark</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-for-spark-developers-flink-vs-spark</guid>
      </item>
    
  </channel>
</rss>
