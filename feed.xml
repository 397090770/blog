<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Madhukar&#39;s Blog</title>
    <description>Thoughts on technology, life and everything else.</description>
    <link>http://blog.madhukaraphatak.com/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 6 : Building Spark 2.0 Two Node Cluster</title>
        <description>&lt;p&gt;In last post, we have built spark 2.0 docker image. As a next step we will be building two node spark standalone cluster using that image. In the context of of kubernetes,  node analogues to a container. So in the sixth blog of the series, we will be building two node cluster containing single master and single worker.You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR you can access all the source code on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;spark-master-deployment&quot;&gt;Spark Master Deployment&lt;/h3&gt;

&lt;p&gt;To start with we define our master using kubernetes deployment abstraction. As you can recall from &lt;a href=&quot;/scaling-spark-with-kubernetes-part-3&quot;&gt;earlier&lt;/a&gt; post, deployment abstraction is used for defining one or morepods. Even though we need single master in our cluster, we will use deployment abstraction over pod as it gives us more flexiblity.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-2.1.0-bin-hadoop2.6&lt;/span&gt; 
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;IfNotPresent&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7077&lt;/span&gt;
          &lt;span class=&quot;l-Scalar-Plain&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;TCP&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/bin/bash&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;-c&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;--&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;./start-master.sh&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;infinity&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above yaml configuration shows the configuration for the master. The noteworthy pieces are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;image - We are using the image we built in our last post. This is availble in local docker images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;imagePullPolicy - By default kubernetes tries to pull the image from remote servers like dockerhub. But as our image is only available locally, we need to tell to kubernetes not to pull from remote. &lt;em&gt;imagePullPolicy&lt;/em&gt; property of configuration allows to us to control that. In our example, we say &lt;em&gt;IfNotPresent&lt;/em&gt; , which means pull only if there is no local copy. As we already have built the image, it will be avaialble and kubernetes will not try to pull from remote.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ports - We are exposing port &lt;em&gt;7077&lt;/em&gt; on which spark master will listen.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;command - Command is the configuration which tells what command to run when container bootstraps. Here we are specifying it to run &lt;em&gt;start-master&lt;/em&gt; script&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/spark-master.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-spark-master&quot;&gt;Starting Spark Master&lt;/h3&gt;

&lt;p&gt;Once we have our configuration ready, we can start the spark master pod using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-master.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;spark-master-service&quot;&gt;Spark Master Service&lt;/h3&gt;

&lt;p&gt;Once we have defined and ran the spark master, next step is to define the service for spark master. This service exposes the spark master on network and other workers can connect to it.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# the port that this service should serve on&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;webui&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;8080&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;8080&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7077&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7077&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above yaml configuration for spark master service. We are naming the our service also &lt;em&gt;spark-master&lt;/em&gt; which helps in resolving proper hosts on cluster.&lt;/p&gt;

&lt;p&gt;We are also exposing the additional port 8080 for accessing spark web ui.&lt;/p&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/spark-master-service.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-spark-master-service&quot;&gt;Starting Spark Master Service&lt;/h3&gt;

&lt;p&gt;Once we have defined the master service, we can now start the service using below command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-master-service.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;spark-worker-configuration&quot;&gt;Spark Worker Configuration&lt;/h3&gt;

&lt;p&gt;Once we have our spark master and it’s service started, we can define the worker configuration.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-2.1.0-bin-hadoop2.6&lt;/span&gt; 
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;imagePullPolicy&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;IfNotPresent&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7078&lt;/span&gt;
          &lt;span class=&quot;l-Scalar-Plain&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;TCP&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/bin/bash&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;-c&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;--&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;./start-worker.sh&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;infinity&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we are building two node cluster, we will be running only single worker as of now. Most of the configuration are same as master other than command which starts the worker.&lt;/p&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/spark-worker.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-worker&quot;&gt;Starting Worker&lt;/h3&gt;

&lt;p&gt;You can start worker deployment using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-worker.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have all services are ready&lt;/p&gt;

&lt;h3 id=&quot;verifying-the-setup&quot;&gt;Verifying the Setup&lt;/h3&gt;

&lt;p&gt;Run below command to verify that both spark master and spark worker deployments are started.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command should two pods running as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;NAME                            READY     STATUS    RESTARTS   AGE
spark-master-498980536-6ljcw    1/1       Running   0          15h
spark-worker-1887160080-nmpq5   1/1       Running   0          14h&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Please note that exact name of the pod will differ from machine to machine.&lt;/p&gt;

&lt;p&gt;Once we verified the pods, verify the service using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl describe svc spark-master&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command should show result as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Name:                   spark-master
Namespace:              default
Labels:                 name=spark-master
Selector:               name=spark-master
Type:                   ClusterIP
IP:                     10.0.0.147
Port:                   webui   8080/TCP
Endpoints:              172.17.0.3:8080
Port:                   spark   7077/TCP
Endpoints:              172.17.0.3:7077
Session Affinity:       None&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If both of the commands ran successfully, then we have spark cluster running successfully.&lt;/p&gt;

&lt;h3 id=&quot;testing-our-spark-cluster&quot;&gt;Testing our spark cluster&lt;/h3&gt;

&lt;p&gt;We can test our spark deployment using observing web ui and running some commands from spark shell.&lt;/p&gt;

&lt;h4 id=&quot;accessing-web-ui&quot;&gt;Accessing Web UI&lt;/h4&gt;

&lt;p&gt;In our configuration of spark master, we have exposed the UI port 8080. Normally it will be only available within spark cluster. But using the port forwarding, we can access the port on our local machine.&lt;/p&gt;

&lt;p&gt;First let’s see the pods running on cluster using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It should show the below result&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;NAME                           READY     STATUS    RESTARTS   AGE
spark-master-498980536-kfgg8   1/1       Running   0          14m
spark-worker-91608803-l22pw    1/1       Running   0          56s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We should port forward from master pod. Run below command. The exact name of the pod will differ from machine to machine.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl port-forward spark-master-498980536-kfgg8 8080:8080&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Port-forward takes two parameters. One is the pod name and then port pair. In port pair the first port is container port and next one is local.&lt;/p&gt;

&lt;p&gt;Once port is forwarded, go to this link &lt;a href=&quot;http://localhost:8080&quot;&gt;http://localhost:8080&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You should see the below image&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/spark-ui-kube.png&quot; alt=&quot;spark-ui-kube&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;spark-shell&quot;&gt;Spark Shell&lt;/h4&gt;

&lt;p&gt;Once we have spark ui, we can test the spark from shell. Let’s run the spark shell from master container.&lt;/p&gt;

&lt;p&gt;First we need to login to our master pod. Run below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -it spark-master-498980536-kfgg8 bash&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Start the spark shell using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;/opt/spark/bin/spark-shell --master spark://spark-master:7077&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Run below command to run some spark code&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makeRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If the code runs successfully, then our cluster setup is working.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this blog, we have succesfully built two node spark cluster using kubernetes absttractions.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h3&gt;

&lt;p&gt;Now we have defined our barebone cluster. In next blog, we will how to scale the cluster using kubernetes tools. Also we will discuss how to do resource management in the cluster.&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-6</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-6</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 5 : Building Spark 2.0 Docker Image</title>
        <description>&lt;p&gt;In last few posts of our kubernetes series, we discussed about the various abstractions available in the framework. In next set of posts, we will be
building a spark cluster using those abstractions. As part of the cluster setup, we will discuss how to use various different configuration available
in kubernetes to achieve some of the import features of clustering. This is the fifth blog of the series, where we will discuss about building a spark
2.0 docker image for running spark stand alone cluster. You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR you can access all the source code on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;need-for-custom-spark-image&quot;&gt;Need for Custom Spark Image&lt;/h3&gt;

&lt;p&gt;Kubernetes already has documented creating a spark cluster on &lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/examples/spark&quot;&gt;github&lt;/a&gt;. But currently it uses old version of the spark. Also it has some configurations which are specific to google cloud. These configurations are not often needed in most of the use cases. So in this blog, we will developing a simple spark image which is based on kubernetes one.&lt;/p&gt;

&lt;p&gt;This spark image is built for standalone spark clusters. From my personal experience, spark standalone mode is more suited for containerization
compared to yarn or mesos.&lt;/p&gt;

&lt;h3 id=&quot;docker-file&quot;&gt;Docker File&lt;/h3&gt;

&lt;p&gt;First step of creating a docker image is to write a docker file. In this section, we will discuss how to write a docker file needed
for spark.&lt;/p&gt;

&lt;p&gt;The below are the different steps of docker file.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Base Image&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;FROM java:openjdk-8-jdk&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above statement in the docker file defines the base image. We are using
a base image which gives us a debian kernel with java installed. We need 
java for all spark services.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Define Spark Version&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ENV spark_ver 2.1.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above line defines the version of spark. Using ENV, we can defines a variable and use it in different places in the script. Here we are building the spark with version 2.1.0. If you want other version, change this configuration.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download and Install Spark Binary&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;RUN mkdir -p /opt &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    curl http://www.us.apache.org/dist/spark/spark-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/spark-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;-bin-hadoop2.6.tgz &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        tar -zx &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    ln -s spark-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;-bin-hadoop2.6 spark &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Spark &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; installed in /opt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above curl command and downloads the spark binary. It will be symlinked into /opt/spark.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add start scripts to image&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ADD start-common.sh start-worker.sh start-master.sh /
RUN chmod +x /start-common.sh /start-master.sh /start-worker.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above lines add some start scripts. We discuss more about these scripts
in next section.&lt;/p&gt;

&lt;p&gt;Now we have our docker file ready. Save it as &lt;em&gt;Dockerfile&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;You can access the complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/Dockerfile&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;scripts&quot;&gt;Scripts&lt;/h3&gt;

&lt;p&gt;In above, we have added some scripts for starting master and worker. Let’s see what’s inside them.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start-common.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a script which runs before starting master and worker.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;unset &lt;/span&gt;SPARK_MASTER_PORT&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above script unsets a variable set by kubernetes. This is needed as this configuration interferes with the
spark clustering. We will discuss more about service variable in next post.&lt;/p&gt;

&lt;p&gt;You can access complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/start-common.sh&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start-master.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a script for starting master.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;

. /start-common.sh

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;$(hostname -i) spark-master&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; /etc/hosts

/opt/spark/sbin/start-master.sh --ip spark-master --port 7077&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the first step, we run the common script. We will be using &lt;em&gt;spark-master&lt;/em&gt; as the host name for our master container. So we are adding that into &lt;em&gt;/etc/hosts&lt;/em&gt; file.&lt;/p&gt;

&lt;p&gt;Then we start the master using &lt;em&gt;start-master.sh&lt;/em&gt; command. We will be listening on 7077 port for the master.&lt;/p&gt;

&lt;p&gt;You can access complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/start-master.sh&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start-worker.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is the script for starting worker containers.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;

. /start-common.sh

/opt/spark/sbin/start-slave.sh spark://spark-master:7077&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It is similar to master script. The only difference is we are using &lt;em&gt;start-slave.sh&lt;/em&gt; for starting our worker nodes.&lt;/p&gt;

&lt;p&gt;You can access complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/start-worker.sh&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now we have our docker script ready. To build an image from the script, we need docker.&lt;/p&gt;

&lt;h3 id=&quot;installing-docker&quot;&gt;Installing Docker&lt;/h3&gt;

&lt;p&gt;You can install the docker on you machine using the steps &lt;a href=&quot;https://docs.docker.com/engine/installation/&quot;&gt;here&lt;/a&gt;. I am using docker version &lt;em&gt;1.10.0&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;using-kubernetes-docker-environment&quot;&gt;Using Kubernetes Docker Environment&lt;/h3&gt;

&lt;p&gt;Whenever we want to use docker, it normally runs a daemon on our machine. This daemon is used for building and pulling docker images. Even though we can build our docker image in our machine, it will be not that useful as our kubernetes runs in a vm. In this case, we need to push our docker image to vm and then only we can use the image in kubernetes.&lt;/p&gt;

&lt;p&gt;Alternative to that, another approach is to use minikube docker daemon. In this way we can build the docker images directly on our virtual machine.&lt;/p&gt;

&lt;p&gt;To access minikube docker daemon, run the below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;minikube docker-env&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you can run&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;docker ps&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you can see all the kubernetes containers as docker containers. Now you have successfully connected to minikube docker environment.&lt;/p&gt;

&lt;h3 id=&quot;building-image&quot;&gt;Building image&lt;/h3&gt;

&lt;p&gt;Clone code from github as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;git clone https://github.com/phatak-dev/kubernetes-spark.git&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;cd to &lt;em&gt;docker&lt;/em&gt; folder then run the below docker command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;docker

docker build -t spark-2.1.0-bin-hadoop2.6 .&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above command, we are tagging (naming) the image as &lt;em&gt;spark-2.1.0-bin-hadoop-2.6&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now our image is ready to deploy, spark 2.1.0 on kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this post, we discussed how to build a spark 2.0 docker image from scratch. Having our own image gives more flexibility than using
off the shelf ones.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h3&gt;

&lt;p&gt;Now we have our spark image ready. In our next blog, we will discuss how to use this image to create a two node cluster in kubernetes.&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-5</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-5</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 4 : Service Abstractions</title>
        <description>&lt;p&gt;In last blog, we discussed about the compute abstraction of kubernetes. In that blog, we discussed about creating a pod with nginx container. At the end of the blog, we needed ability to expose nginx pod for consuming it services. To do that, we need to understand how networking works in kubernetes.So in this fourth blog of the series, we are going to discuss various network related abstractions provided kubernetes. You can access all the blog in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;network-abstractions&quot;&gt;Network Abstractions&lt;/h2&gt;

&lt;p&gt;Network abstractions in the kubernetes are the one which facilitate the communication between the pods or the communication of the pods from external world. Commonly these are known as service abstractions.&lt;/p&gt;

&lt;p&gt;In the following sections, we are going to explore different service abstractions.&lt;/p&gt;

&lt;h3 id=&quot;container-port&quot;&gt;Container Port&lt;/h3&gt;

&lt;p&gt;As part of the pod definition, we can  define which ports to be exposed from the container using &lt;em&gt;containerPort&lt;/em&gt; property. This will expose that specific port in
the container on it’s ip address.&lt;/p&gt;

&lt;p&gt;Let’s define port at 80 in our nginx deployment.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;80&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can access complete file &lt;a href=&quot;https://github.com/phatak-dev/blog/blob/master/code/KubernetesExamples/nginxdeployment.yaml&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;service&quot;&gt;Service&lt;/h3&gt;

&lt;p&gt;Once we defined the container port, next step is to define service.&lt;/p&gt;

&lt;p&gt;Service abstraction defines a set of logical pods. This is a network abstraction which defines a policy to expose micro service using these pods to other parts of the application.&lt;/p&gt;

&lt;p&gt;This separation of container and it’s service layer allows us to upgrade the different parts of the applications independent of each other. This is the strength of the microservice.&lt;/p&gt;

&lt;p&gt;Let’s define a service for our nginx deployment.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-service&lt;/span&gt;
   &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; 
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-service&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;selector&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; 
     &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;port&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;80&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above configuration defines the service. The import sections to focus are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;kind - As we specified with pod and deployment abstractions, we specify the service using this parameter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;selector - Connecting pods with service. This is the way kubernetes knows which pod to forward the requests to the service. In this , we
are specifying the selector on label called &lt;em&gt;name&lt;/em&gt; and it’s value &lt;em&gt;nginx&lt;/em&gt;. This should be same labels that we have specified in the 
nginxdeployment.yaml. The below was the our deployment definition&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;80&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above configuration, we have specified the labels in our template. This shows how label abstraction is used to connect service and pod abstractions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ports - This specifies the ports which service should connect on the container. By default the service port on which it listens is same as container
port. You can change it if you want by specifying the &lt;em&gt;targetPort&lt;/em&gt; parameter.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/blog/blob/master/code/KubernetesExamples/nginxservice.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-service&quot;&gt;Starting Service&lt;/h3&gt;

&lt;p&gt;Once we have defined the configuration, we can start the service using below command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubctl create -f nginxservice.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can list all the services, as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get svc&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It should show the service running below.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;nginx-service   10.0.0.197   &amp;lt;none&amp;gt;        80/TCP    23h&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have successfully started the service.&lt;/p&gt;

&lt;h3 id=&quot;service-endpoint&quot;&gt;Service EndPoint&lt;/h3&gt;

&lt;p&gt;Service we have created above is only accessible within the kubernetes cluster. There is a way to expose the service to external world, but we will be not discussing that in this post.&lt;/p&gt;

&lt;p&gt;To connect to the service, we need to know the machine it runs. As we are running kubernetes in local mode, it will be virtual machine running minikube.&lt;/p&gt;

&lt;p&gt;Run below command to get the end point details&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl describe svc&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It should show the output as below.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Name:                   nginx-service
Namespace:              default
Labels:                 name=nginx-service
Selector:               name=nginx
Type:                   ClusterIP
IP:                     10.0.0.197
Port:                   &amp;lt;unset&amp;gt; 80/TCP
Endpoints:              172.17.0.4:80
Session Affinity:       None&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above command, we are describing the complete information about service. We are interested in the &lt;em&gt;EndPoints&lt;/em&gt; parameter. This gives the IP and port of the machine to which we can connect. Note that the actual values of these parameter will be different on your machine.&lt;/p&gt;

&lt;h3 id=&quot;testing-with-busy-box&quot;&gt;Testing with busy box&lt;/h3&gt;

&lt;p&gt;Now we have end point to call. But we need another pod in cluster to connect to this machine. So let’s run another pod&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl run -i --tty busybox --image&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;busybox --restart&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Never -- sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command shows another way creating and running the pods. The different pieces of the command are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;run - Specifies create and run pod&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;-i - Specifies run the pod interactively. This allows us to send commands using pod&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;–tty - Gives access to the terminal of the pod&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;busybox - Name of the pod.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;–image - image to run inside the container. We are a running an image called busybox, which gives minimal linux shell utilities&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;– restart-never - Since it’s a temporary pod, we don’t need HA&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sh - Specifies run shell command to access&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once you run the above command, you should drop into a familiar linux shell.&lt;/p&gt;

&lt;p&gt;From the shell, run below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;wget -O - http://172.17.0.4&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Replace the IP address with the one you got from end point. This should print the welcome page of nginx as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have successfully connected our service and used our pod.&lt;/p&gt;

&lt;p&gt;Service layer of the kubernetes may look little complicated. It is. It’s built for varieties of use cases. So it has multiple layer of redirection. We will explore more about this abstraction in upcoming posts.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this blog, we have discussed how to define and consume services. Services are one of the important features of the kubernetes which makes it powerful platform to deploy clustered applications.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h3&gt;

&lt;p&gt;Now we know pod, deployment and service abstractions. These are minimal abstractions we need, to build our spark cluster on kubernetes. In next post, we will be discus how to build and scale spark cluster on kubernetes.&lt;/p&gt;

</description>
        <pubDate>Thu, 23 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-4</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-4</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 3 : Kubernetes Abstractions</title>
        <description>&lt;p&gt;In last blog of the series, we discussed about how to install kubernetes in our local machine.In this third blog, we will discuss what are the different abstractions
provided by the kubernetes. You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-abstractions&quot;&gt;Kubernetes Abstractions&lt;/h2&gt;

&lt;p&gt;Kubernetes is a production grade  container orchestration system. It follows an API driven approach to interact between different components. It has a vast API surface.We are not going to cover each of those API’s/abstractions here. We are only going to focus on few of the one which are used most of the times. For all the abstractions, refer to &lt;a href=&quot;https://kubernetes.io/docs/user-guide/&quot;&gt;user guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Kubernetes abstractions can be divided into following four major categories&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compute Abstractions - All the abstractions related to running a computing unit. Ex : Container, Pod etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Network Abstractions - All the abstractions related to expose the computing units on network ex: Container Port, Service etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Storage Abstractions - All the abstractions related to providing and managing storage for compute ex: Volume, VolumeClaim etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metadata Abstractions - All the abstractions related to discovering compute, network and storage abstractions ex : labels&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the following sections, we will be discussing about important compute abstractions. The other abstractions will be covered in future posts.&lt;/p&gt;

&lt;h2 id=&quot;a-brief-word-about-containers&quot;&gt;A Brief Word about Containers&lt;/h2&gt;

&lt;p&gt;Kubernetes is a container orchestration framework. But what is a container? In simple terms, container is a light weight virtual machine which runs one of the services
of an application. The major difference between VM and Containers is how they share operating system and underneath resources. In VM world, each VM has it’s own full copy of operating system. But in case of containers, all the containers share a common operating system kernel. So containers are much more light weight than the VM’s.&lt;/p&gt;

&lt;p&gt;Even though containers are around more than a decade, docker made containers popular. You can get basics of docker or container in general by going through this &lt;a href=&quot;https://www.youtube.com/watch?v=Q5POuMHxW-0&quot;&gt;video&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;compute-abstractions&quot;&gt;Compute Abstractions&lt;/h2&gt;

&lt;p&gt;Once we know what is a container, we can now start discussing about the different compute abstractions in kubernetes. Most of these abstractions discuss about how to create, manage and destroy the containers on scale.&lt;/p&gt;

&lt;h3 id=&quot;pod-abstraction&quot;&gt;Pod Abstraction&lt;/h3&gt;

&lt;p&gt;Pod is a collection of one or more containers. It’s smallest compute unit you can deploy on the kubernetes.&lt;/p&gt;

&lt;p&gt;One of the important aspects of pods are, they run all the containers in the single node. This gives the locality to the containers which need low latency connection between them. Also since they run on same machine, kubernetes creates a networking scheme which allows each containers to address them each other by “localhost”&lt;/p&gt;

&lt;h3 id=&quot;defining-the-pod&quot;&gt;Defining the Pod&lt;/h3&gt;

&lt;p&gt;Kubernetes uses yaml as it’s configuration language for defining various resources.&lt;/p&gt;

&lt;p&gt;In below configuration, we are defining a pod which runs a single container of nginx. Nginx is a popular web server.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-pod&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above yaml snippet defines the pod. The below are the different pieces.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;apiVersion&lt;/strong&gt; - parameter defines the  kubernetes API we are using. This versioning scheme allows kubernetes to support multiple versions of the API’s at same time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;kind&lt;/strong&gt; - This parameter defines for which abstraction of kubernetes we are defining this configuration. Here we are defining for a pod.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;metadata&lt;/strong&gt; - Metadata of the pod. This allows kubernetes to locate the pod uniquely across the cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;spec&lt;/strong&gt; - This defines the all the containers we want to run&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For each container we define&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;name&lt;/strong&gt; - Name of the container. This will be also used as the host name of the container. So this has to be unique within the pod&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;image&lt;/strong&gt; - Docker image that needs to be used to create the container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can read more about pod abstraction &lt;a href=&quot;https://kubernetes.io/docs/user-guide/pods/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can find the complete yaml file on &lt;a href=&quot;https://github.com/phatak-dev/blog/blob/master/code/KubernetesExamples/nginxpod.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;creating-pod-from-configuration&quot;&gt;Creating Pod from Configuration&lt;/h3&gt;

&lt;p&gt;Once we define the pod, then we can use &lt;em&gt;kubectl create&lt;/em&gt; command to create a pod&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f nginx.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will download the latest nginx image from dockerhub and starts the container inside the pod.&lt;/p&gt;

&lt;p&gt;If you run the below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should see the results as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;nginx-pod                          1/1       Running   0          37s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you have successfully ran a pod on your kubernetes instance.&lt;/p&gt;

&lt;h3 id=&quot;deployment-abstraction&quot;&gt;Deployment Abstraction&lt;/h3&gt;

&lt;p&gt;In earlier section, we discussed about pod abstraction. Pod abstraction works well when we need to create single copy of the container. But in clustered use cases like spark, we may need multiple instance of same containers. For example, we need multiple instances of spark workers. Expressing them individually is tedious and doesn’t scale well.So in those cases using pod abstraction is not good enough.Also pod abstraction doesn’t allow us to update the code inside the pod without changing the configuration file. This will be challenging in cluster environment where we may want to dynamically update configs/ version of software.&lt;/p&gt;

&lt;p&gt;So to overcome these challenges, kubernetes gives us another abstraction called deployments. As name suggest, this abstraction allows end to end deployment of a pod. This allows us to create, update and destroy pods with much cleaner abstractions than the bare bone pod abstraction. So kubernetes documentation prefers the deployment abstraction over simple pod abstraction.&lt;/p&gt;

&lt;p&gt;So let’s rewrite our nginx pod example using deployment abstraction. The below is the yaml configuration for deployment&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The below are the major differences are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;replicas&lt;/strong&gt; - We can create multiple instances of the pod using this. As we need only instance here we are specifying as the 1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;template&lt;/strong&gt; - This holds the template for the pod. This information is same whatever we specified in the pod definition.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can access the complete file on &lt;a href=&quot;https://github.com/phatak-dev/blog/blob/master/code/KubernetesExamples/nginxdeployment.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;running-the-deployment&quot;&gt;Running the deployment&lt;/h3&gt;

&lt;p&gt;Use the below command to run the deployment&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f nginxdeployment.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can see all running deployments using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get deployments&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you have successfully ran the deployment. You can run multiple copies of the container just by increasing the replicas count.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Now we know the major compute abstractions of the kubernetes. Use deployment abstraction even when you need single pod. It makes things much cleaner.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h3&gt;

&lt;p&gt;Even though we have run the pod, we have not accessed  anything from it. So you may be asking how to access the front-page of nginx. To understand that, we need to understand the network/service abstractions provided by the kubernetes. We will be discussing about them in the next blog.&lt;/p&gt;

</description>
        <pubDate>Fri, 17 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-3</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-3</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 2 : Installing Kubernetes Locally using Minikube</title>
        <description>&lt;p&gt;In last blog, we discussed about what is kubernetes and what are it’s advantages. In this second post of the series, we are going to discuss
how to install the kubernetes locally on your machine.You can find all the posts in the series &lt;a href=&quot;/categories/kubernetes-series/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;installing-kubernetes-on-local-machine&quot;&gt;Installing Kubernetes on Local Machine&lt;/h2&gt;

&lt;p&gt;One of the cool features of kubernetes that it can be installed and tried out in local. It behaves exactly as it will be on a cluster. To try out 
kubernetes on local we need to install minikube and kubectl.&lt;/p&gt;

&lt;p&gt;The below are the steps&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;step-1-pre-requisites&quot;&gt;Step 1 :Pre-Requisites&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To install the kubernetes on local machine, we install minikube. But minikube normally uses some kind of virtualization layer to install the
needed software. So for our example, we will use virtualbox as our virtualization layer. For more pre-requisites refer &lt;a href=&quot;https://kubernetes.io/docs/getting-started-guides/minikube/#requirements&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Download and Install virtualbox from &lt;a href=&quot;http://www.virtualbox.org&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;step-2--install-minikube&quot;&gt;Step 2 : Install MiniKube&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Run the below commands to install minikube on linux. For other operating system, refer &lt;a href=&quot;https://github.com/kubernetes/minikube/releases&quot;&gt;here&lt;/a&gt;.
Latest version as of now is 0.16.0&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.16.0/minikube-linux-amd64 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; chmod +x minikube &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo mv minikube /usr/local/bin/&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;step-3--install-kubectl&quot;&gt;Step 3 : Install KubeCtl&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubectl is a command line utility which communicates to kubernetes over it’s REST API. We can install it using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;curl -LO https://storage.googleapis.com/kubernetes-release/release/&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;interacting-with-minikube&quot;&gt;Interacting With Minikube&lt;/h2&gt;

&lt;p&gt;Once we installed the minikube and kubectl , we can start playing with kubernetes.&lt;/p&gt;

&lt;p&gt;We can start minikube using below command. It downloads minikube iso and start a virtual machine in virtualbox.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;minikube start&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can open the kubernetes dashboard using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;minikube dashboard&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can check is anything running or not, using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po --all-namespaces&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This command should show some kubernetes container running.&lt;/p&gt;

&lt;p&gt;Now we have successfully installed and configured the kubernetes on our machine.&lt;/p&gt;

&lt;p&gt;In our next post, we will discuss the different abstractions of kubernetes and how to use them in our applications.&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-2</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 1 : Introduction to Kubernetes</title>
        <description>&lt;p&gt;As our workloads become more and more micro service oriented, building an infrastructure to deploy them easily 
becomes important. Most of the big data applications need multiple services likes HDFS, YARN, Spark  and their clusters.
Creating, deploying and monitoring them manually is tedious and error prone.&lt;/p&gt;

&lt;p&gt;So most of the users move to cloud to simplify it. Solutions like EMR, Databricks etc help in this regard. But then users will be locked into
those specific services. Also sometimes we want same deployment strategy to work on premise also. Most of the cloud providers don’t have that option today.&lt;/p&gt;

&lt;p&gt;So we need a framework which helps us to create and monitor complex big data clusters. Also it should helps us move between on premise and
other cloud providers seamlessly. Kubernetes is one those frameworks that can help us in that regard.&lt;/p&gt;

&lt;p&gt;In this set of posts, we are going to discuss how kubernetes, an open source container orchestration framework from Google, helps us
to achieve a deployment strategy for spark and other big data tools which works across the on premise and cloud. As part of the series, we will 
discuss how to install, configure and scale kubernetes on local and cloud. Also we are going to discuss how to build our own customised images for the services and applications.&lt;/p&gt;

&lt;p&gt;This is the first blog in the series where we discuss about what is kubernetes and it’s advantages. You can access
all other blogs in the series &lt;a href=&quot;/categories/kubernetes-series/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-is-kubernetes&quot;&gt;What is Kubernetes?&lt;/h2&gt;

&lt;p&gt;Kubernetes is an open source container orchestration framework. In simple words, it’s a framework which allows us
to create and manage multiple containers. These containers will be docker containers which will be running some services. 
These can be your typical webapp, database or even big data tools like spark, hbase etc.&lt;/p&gt;

&lt;h2 id=&quot;why-kubernetes&quot;&gt;Why Kubernetes?&lt;/h2&gt;

&lt;p&gt;Most of the readers may have tried docker before. It’s a framework which allows developers containerise their application. It has become a
popular way to develop, test and deploy applications on scale. When we already have docker, what is kubernetes bring into picture? Can’t we 
just build our clusters using normal docker itself?&lt;/p&gt;

&lt;p&gt;The below are the some of the advantages of using kubernetes over plain docker tools.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;orchestration&quot;&gt;Orchestration&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the import feature that sets kubernetes apart from docker is it’s not a container framework. But it’s more of a orchestration layer for multiple containers
that normally make an application. Docker itself has compose feature but it’s very limited. So as our application become complex, we will have
multiple containers which needs to be orchestrated. Doing them manually becomes tricky. So kubernetes helps in that regard.&lt;/p&gt;

&lt;p&gt;Also kubernetes has support for multiple container frameworks. Currently it supports docker and rkt. This makes users
to choose their own container frameworks rather than sticking with only docker.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;cloud-independent&quot;&gt;Cloud Independent&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the import design goal of kubernetes, is ability to run everywhere. We can run kubernetes in local machine, on-premise clusters or on cloud.
Kubernetes has support for AWS,GCE and Azure out of the box. Not only it normalises the deployment across the cloud, it will use best tool for given
problem given by specific cloud. So it tries to optimise for each cloud.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;support-for-easy-clustering&quot;&gt;Support for Easy Clustering&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the hard part of installing big data tools like spark on cloud is to build the cluster and maintain it. Creating clusters often need tinkering with networking to make sure all services are started in right places. Also once cluster is up and running, making sure each node has sufficient resources also is tricky.&lt;/p&gt;

&lt;p&gt;Often scaling cluster, adding node or removing it, is tricky. Kubernetes makes all this much easier compared to current solutions. It has excellent support to
virtual networking and ability to easily scale clusters on will.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;support-for-service-upgradation-and-rollback&quot;&gt;Support for Service Upgradation and Rollback&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the hard part of clustered applications, is to update the software. Sometime it may be you want to update the application code or want to update version of
spark itself. Having a well defined strategy to upgrade the clusters with check and balances is super critical. Also when things go south, ability to rollback 
in reasonably time frame is also important.&lt;/p&gt;

&lt;p&gt;Kubernetes provides well defined image ( container image) based upgradation policies which can unify the upgrading different services across cluster. This makes
life easier for all the ops people out there.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;effective-resource-isolation-and-management&quot;&gt;Effective Resource Isolation and Management&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the question, we often ponder should we run services like kafka next to spark or not? Most of the time people advise to have separate machines
so that each service gets sufficient resources. But defining machine size and segregating services based on machines becomes tricky as we want to scale our
services.&lt;/p&gt;

&lt;p&gt;Kubernetes frees you from the machine. Kubernetes asks you to define how much resources you want to dedicate for service. Once you do that, it will take care
of figuring out which machine to run those. It will make sure that it will effectively using all resources across machines and also give guarantees about resource
allocation. You no more need to worry about is one service is taking over all resources and depriving others or your machines are under utilized.&lt;/p&gt;

&lt;p&gt;Not only kubernetes allows you to define resources In terms of GB of RAM or number of cpu’s, it allows it to be defined in terms of percentage of machine resource or
in terms of no of requests. These options are there to dedicate the resources more granularly.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;well-defined-storage-management&quot;&gt;Well Defined Storage Management&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the challenges of micro service oriented architectures is to store the state across the restart/ upgradation of containers. It’s critical for applications like Databases not loose data when something goes wrong with container or machine.&lt;/p&gt;

&lt;p&gt;Kubernetes gives a clear abstraction of storage who’s life cycle is independent of the container itself. This makes users ability to use different storages like host based, network attached drives to make sure that there will be no data loss. These abstractions ties well with persistence options provided by cloud like EBS from aws. Kubernetes makes long running persistent services like databases a breeze.&lt;/p&gt;

&lt;p&gt;Now we know what kubernetes brings to the table. In our next post, we will be discussing how to install kubernetes on local machine.&lt;/p&gt;

</description>
        <pubDate>Mon, 13 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-1</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-1</guid>
      </item>
    
      <item>
        <title>Statistical Data Exploration using Spark 2.0 - Part 3 : Outlier Detection using Quantiles</title>
        <description>&lt;p&gt;In our &lt;a href=&quot;/statistical-data-exploration-spark-part-1&quot;&gt;first blog&lt;/a&gt; of the series, we discussed about generating summary data using spark.This summary data included mean, standard deviation and quantiles. Quantiles gives pretty good idea about spread of data and are one of the robust measurements compared to mean.&lt;/p&gt;

&lt;p&gt;In this third blog of the series, we will be discussing about how to use quantiles to identify the outliers in our data. You can find all other blogs in the series &lt;a href=&quot;/categories/statistical-data-exploration&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR All code examples available on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;outlier&quot;&gt;Outlier&lt;/h2&gt;
&lt;p&gt;For a given variable in data, outlier is a value distant from other values. Normally outlier is  introduced in data due to issue with measurements or some error. Outlier effects our inference of the data as they may skew the results.&lt;/p&gt;

&lt;p&gt;So in statistics its important to identify the outliers in the data, before we use it for analysis.&lt;/p&gt;

&lt;h2 id=&quot;outlier-detection-using-box-and-whisker-plot&quot;&gt;Outlier detection using Box-and-Whisker Plot&lt;/h2&gt;

&lt;p&gt;There are many methods to identify outlier in statistics. In this blog, we are going to discuss about one of the method which uses quantiles. The logic of the algorithm as follows&lt;/p&gt;

&lt;p&gt;Let’s say we have Q1 as first quantile(25%) and Q3 as third quantile(75%) , the inter quantile range or IQR will be given as&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;IQR = Q3 - Q1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;IQR gives the width of distribution of data between 25% and 75% of data. Using IQR we can identify the outliers. This method is known as Box and Whisker method.&lt;/p&gt;

&lt;p&gt;In this method, any value smaller than Q1- 1.5 * IQR or any value greater than Q3+1.5 * IQR will be categorised as the outlier.&lt;/p&gt;

&lt;p&gt;You can find more information on this method &lt;a href=&quot;http://www.purplemath.com/modules/boxwhisk3.htm&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;outlier-detection-in-spark&quot;&gt;Outlier detection in Spark&lt;/h2&gt;

&lt;p&gt;Once we understand the method, we can implement it in spark. The following are the steps for implementing the same.&lt;/p&gt;

&lt;h3 id=&quot;create-sample-data&quot;&gt;Create Sample Data&lt;/h3&gt;

&lt;p&gt;First we create a sample dataset to work with and then convert into a spark dataframe.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampleData&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;14.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;mf&quot;&gt;14.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;14.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;15.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;15.9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;16.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rowRDD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkSession&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparkContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makeRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampleData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)))&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;StructType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DoubleType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)))&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkSession&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rowRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above example, we have taken a list of values as sample data. If you observe the data, most of the values are around 14.1-14.7. From that we can assume mostly values 10.2, 16.4 are outliers. There is chance that 15.1 and 15.9 are also outliers but we are not fully sure.&lt;/p&gt;

&lt;h3 id=&quot;calculate-quantiles-and-iqr&quot;&gt;Calculate Quantiles and IQR&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;approxQuantile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q3&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;IQR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we did in earlier posts, we are using &lt;em&gt;approxQuantile&lt;/em&gt;  method to compute the quantiles needed. Once we have quantiles, we can calculate IQR.&lt;/p&gt;

&lt;h3 id=&quot;filter-outliers&quot;&gt;Filter Outliers&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lowerRange&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;IQR&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upperRange&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;IQR&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outliers&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;value &amp;lt; $lowerRange or value &amp;gt; $upperRange&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;outliers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above code, we first calculate the ranges. Then we filter the data using data frame filters.&lt;/p&gt;

&lt;p&gt;When we run this example, we get 10.2 and 16.4 as the outliers.&lt;/p&gt;

&lt;p&gt;You can access complete example on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0/blob/master/src/main/scala/com/madhukaraphatak/spark/dataexploration/OutliersWithIQR.scala&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog, we learned how to use quantiles to detect the outliers in data.&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Nov 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-3</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-3</guid>
      </item>
    
      <item>
        <title>Statistical Data Exploration using Spark 2.0 - Part 2 : Shape of Data with Histograms</title>
        <description>&lt;p&gt;In our last blog, we discussed about generating summary data using spark. The summary works great for understanding the range of data quantitatively. But sometimes, we want to understand how the data is distributed between different range of the values. Also rather than just know the numbers, it will help a lot if we are able visualize the same. This way of exploring data is known as understanding shape of the data.&lt;/p&gt;

&lt;p&gt;In this second blog of the series, we will be discussing how to understand the shape of the data using the histogram. You can find all other blogs in the series &lt;a href=&quot;/categories/statistical-data-exploration&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR All code examples available on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;histogram&quot;&gt;Histogram&lt;/h2&gt;

&lt;p&gt;Histograms are visual representation of the shape/distribution of the data. This visual representation is heavily used in statistical data exploration.&lt;/p&gt;

&lt;h2 id=&quot;histogram-in-r&quot;&gt;Histogram in R&lt;/h2&gt;

&lt;p&gt;In R, histogram is part of package named &lt;strong&gt;ggplot2&lt;/strong&gt;. Once you installed the package you can generate the histogram as below.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;hist&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;LifeExp&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We use hist method provided by the library to draw the histogram. The below picture shows the histogram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/histogram_in_r.png&quot; alt=&quot;Histogram in R&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;histogram-in-spark&quot;&gt;Histogram in Spark&lt;/h2&gt;

&lt;p&gt;In order to generate the histogram, we need two different things&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generate the values for histogram&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Display the visual representation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Calculating the histogram in spark is relatively easy. But unlike R, spark doesn’t come with built in visualization package. So I will be using &lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;Apache Zeppelin&lt;/a&gt; for generating charts.&lt;/p&gt;

&lt;h2 id=&quot;calculating-the-histogram&quot;&gt;Calculating the histogram&lt;/h2&gt;

&lt;p&gt;We will be using same dataset, life expectancy, dataset for generating our histograms. Refer to &lt;a href=&quot;/statistical-data-exploration-spark-part-1/&quot;&gt;last blog&lt;/a&gt; for loading data into spark dataframe.&lt;/p&gt;

&lt;p&gt;Dataframe API doesn’t have builtin function for histogram. But RDD API has. So using RDD API we can calculate histogram values as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;startValues&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lifeExpectancyDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;LifeExp&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;RDD histogram API takes number of bins.&lt;/p&gt;

&lt;p&gt;The result of the histogram are two arrays.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First array contains the starting values of each bin&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Second array contains the count for each bin&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The result of the above code on our data will be as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;startValues: Array[Double] = Array(47.794, 54.914, 62.034, 69.154, 76.274, 83.394)
counts: Array[Long] = Array(24, 18, 32, 69, 54)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So the values signify that there are 24 countries between life expectancy from 47.794 to 54.914. Most countries are between 76-83.&lt;/p&gt;

&lt;p&gt;If you don’t like using RDD API, we can add histogram function directly on Dataframe using implicits. Refer to the code on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0/blob/master/src/main/scala/com/madhukaraphatak/spark/dataexploration/CustomStatFunctions.scala&quot;&gt;github&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&quot;visualizing-the-histogram&quot;&gt;Visualizing the histogram&lt;/h2&gt;

&lt;p&gt;Once we have calculated values for histogram, we want to visualize same. As we discussed earlier, we will be using zeppelin notebook for same.&lt;/p&gt;

&lt;p&gt;In zeppelin, in order to generate a graph easily we need dataframe. But in our case, we got data as arrays. So the below code will convert those arrays to dataframe which can be consumed by the zeppelin.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zippedValues&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;startValues&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HistRow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;startPoint&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rowRDD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zippedValues&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HistRow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histDf&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkSession&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rowRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;histDf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createOrReplaceTempView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;histogramTable&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above code, first we combining both arrays using zip method. It will give us a array of tuples. Then we convert that array into a dataframe using the case class.&lt;/p&gt;

&lt;p&gt;Once we have, dataframe ready we can run sql command and generate nice graphs as below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/histogram_lifexp.png&quot; alt=&quot;Histogram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can download the complete zeppelin notebook from &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0/blob/master/src/main/zeppelin/Shape%20of%20Data%20Histogram.json&quot;&gt;github&lt;/a&gt; and import into yours to test by yourself. Please make sure you are using Zeppelin 0.6.2 stable release.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Combining computing power of spark with visualization capabilities of zeppelin allows us to explore data in a way R or python does but for big data. This combination of tools make statistical data exploration on big data much easier and powerful.&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Oct 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-2</guid>
      </item>
    
      <item>
        <title>Statistical Data Exploration using Spark 2.0 - Part 1 : Five Number Summary</title>
        <description>&lt;p&gt;Data exploration is an important part of data analysis to understand nature of data. Data scientists use various mathematical and statistics techniques to understand the distribution and shape of the data which comes handy to draw conclusions.&lt;/p&gt;

&lt;p&gt;Recently I started going through the coursera course “Making Sense of Data” videos to understand the data exploration techniques. Its an excellent course which explains all the basics of theory and practical aspects of data exploration. Currently the course is not available at the coursera. But you can find recording of earlier course on &lt;a href=&quot;https://www.youtube.com/watch?v=rXZD3yVFN9w&amp;amp;list=PL7wD1yDs0UYYmTGN3ZJnZuwloaoEuHvmE&quot;&gt;youtube&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Most of the examples of course are explained using R programming language. As Spark 2.0 and R share dataframe as common abstraction, I thought it will be interesting to explore possibility of using Spark dataframe/datasets abstractions to do explore the data.&lt;/p&gt;

&lt;p&gt;This series of blog posts are focused  on the data exploration using spark. I will be comparing the R dataframe capabilities with spark ones. I will be using Spark 2.0 version with Scala API and Zeppelin notebooks for visualizations.This is the first blog in series where we will be discussing how to derive summary statistics of a dataset. You can find all other blogs in the series &lt;a href=&quot;/categories/statistical-data-exploration&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR All code examples available on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;loading-dataset&quot;&gt;Loading Dataset&lt;/h2&gt;
&lt;p&gt;For our example, we will be using life expectancy dataset. This is a dataset which has average life expectancy of all countries across the world. Its space separated file with following three fields.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Country&lt;/li&gt;
  &lt;li&gt;Life Expectancy&lt;/li&gt;
  &lt;li&gt;Region&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The below code shows to how to read the data in R&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; read.table&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;LifeExpentancy.txt&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kp&quot;&gt;colnames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;LifeExp&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Region&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can load the same data in spark as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rawDF&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkSession&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;delimiter&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;inferSchema&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;src/main/resources/LifeExpentancy.txt&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;//only extract the values we need&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;StructType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
           &lt;span class=&quot;nc&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Country&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt;
           &lt;span class=&quot;nc&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;LifeExp&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DoubleType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt;
           &lt;span class=&quot;nc&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Region&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;StringType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;selectedDF&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rawDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;_c0&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;_c2&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;_c4&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lifeExpectancyDF&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkSession&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selectedDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;lifeExpectancyDF&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We have to do little bit more work in spark as spark-csv package doesn’t handle double spaces properly.&lt;/p&gt;

&lt;p&gt;Now we have data ready to explore.&lt;/p&gt;

&lt;h2 id=&quot;five-number-summary&quot;&gt;Five Number Summary&lt;/h2&gt;

&lt;p&gt;Five number summary is one of the basic data exploration technique where we will find how values of dataset columns are distributed. In our example, we are interested to know the summary of “LifeExp” column.&lt;/p&gt;

&lt;p&gt;Five Number Summary Contains following information&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Min - Minimum value of the column&lt;/li&gt;
  &lt;li&gt;First Quantile - The 25% th data&lt;/li&gt;
  &lt;li&gt;Median - Middle Value&lt;/li&gt;
  &lt;li&gt;Third Quartile - 75% of the value&lt;/li&gt;
  &lt;li&gt;Max - maximum value&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above values gives a fair idea about how the values are distributed for the column. For categorical columns it will be different. We will discuss about that in future blogs.&lt;/p&gt;

&lt;p&gt;The below code in R allows to compute above values&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;kn&quot;&gt;attach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;df&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kp&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;LifeExp&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We get below result&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  47.79   64.67   73.24   69.86   76.65   83.39&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;R shows mean also as part of the summary.&lt;/p&gt;

&lt;h2 id=&quot;five-number-summary-in-spark&quot;&gt;Five Number Summary in Spark&lt;/h2&gt;

&lt;p&gt;In Spark, we can get same information using &lt;em&gt;describe&lt;/em&gt; method.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;lifeExpectancyDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;LifeExp&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The below is the output&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;+-------+-----------------+
|summary|          LifeExp|
+-------+-----------------+
|  count|              197|
|   mean|69.86281725888323|
| stddev|9.668736205594511|
|    min|           47.794|
|    max|           83.394|
+-------+-----------------+&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you observe the result, rather than giving quantiles values and median, spark gives standard deviation. The reason is, median and quantiles are costly to compute on large data. Both values need data to be in sorted order and result in skewed calculations.&lt;/p&gt;

&lt;h2 id=&quot;calculating-quantiles-in-spark&quot;&gt;Calculating Quantiles in Spark&lt;/h2&gt;

&lt;p&gt;We can calculate quartiles using &lt;em&gt;approxQuantile&lt;/em&gt; method introduced in spark 2.0. This allows us to find 25%, median and 75% values like R. The name of the method suggests that we can get approximate values whenever we specify the error rate. This makes calculations much faster compared to absolute value.&lt;/p&gt;

&lt;p&gt;In our example we will be calculating the exact values so that we can compare to R.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;medianAndQuantiles&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lifeExpectancyDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;approxQuantile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;LifeExp&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above code, the below are the parameter to &lt;em&gt;approxQuantile&lt;/em&gt; method&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Name of the column&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Array of values signifying which quantile we want. In our example we are calculating 25%, 50% and 75%. 50% is same as median&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The last parameter signifies the error rate. 0.0 signifies we want exact value.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The below is the result&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;List(64.986, 73.339, 76.835)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you compare the result, it matches with the R values.&lt;/p&gt;

&lt;p&gt;You can access complete code on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0/blob/master/src/main/scala/com/madhukaraphatak/spark/dataexploration/SummaryExample.scala&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post we discussed how to get started with exploring data using statistics in spark 2.0. In future blogs we will discuss further more techniques and API’s.&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Oct 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-1</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-1</guid>
      </item>
    
      <item>
        <title>Interactive Workflow Management using Azkaban : API Driven Workflow Management for Spark</title>
        <description>&lt;p&gt;The below video is recording of my talk on &lt;em&gt;Interactive Workflow Management using Azkaban&lt;/em&gt; in recent spark meetup. In this talk, we talk about using Azkaban AJAX API to build interactive workflow management for spark applications.&lt;/p&gt;

&lt;p&gt;Find the slides on &lt;a href=&quot;http://www.slideshare.net/datamantra/interactive-workflow-management-using-azkaban&quot;&gt;slideshare&lt;/a&gt; and code on &lt;a href=&quot;https://github.com/phatak-dev/interactive-azkaban&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/6RebQR-5Kh8&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;
</description>
        <pubDate>Sun, 25 Sep 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/interactive-workflow-management-using-azkaban</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/interactive-workflow-management-using-azkaban</guid>
      </item>
    
  </channel>
</rss>
