<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Madhukar&#39;s Blog</title>
    <description>Thoughts on technology, life and everything else.</description>
    <link>http://blog.madhukaraphatak.com/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Introduction to Flink Streaming - Part 5 : Window API in Flink</title>
        <description>&lt;p&gt;In recent years, there is been a lot of discussion on stateful stream processing. When initial open source stream processor like storm came along, stream processing was viewed as the faster batch processing. The API’s were more geared towards the stateless ETL pipelines. But as realtime/ stream processing became more and more important having stateful processing became necessary. So all modern stream processing frameworks have varied degree of support to do stateful operations.&lt;/p&gt;

&lt;p&gt;Window is one of the way to define continuous state across the stream. So in the fifth blog of the series, I will be discussing about window support in flink API. You can access all the posts in the series &lt;a href=&quot;/categories/flink-streaming&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;window-in-streaming&quot;&gt;Window in Streaming&lt;/h2&gt;

&lt;p&gt;Window is a mechanism to take a snapshot of the stream. This snapshot can be based on time or other variables. For example, if we create a window for 5 seconds then it will be all the records which arrived in the that time frame. You can define the window based on no of records or other stream specific variables also.&lt;/p&gt;

&lt;h2 id=&quot;types-of-window-in-flink&quot;&gt;Types of window in Flink&lt;/h2&gt;

&lt;p&gt;Flink support wide variety of window operations. The different windows supported in flink are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Time Windows
    &lt;ul&gt;
      &lt;li&gt;Tumbling Windows&lt;/li&gt;
      &lt;li&gt;Sliding Windows&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Count Windows&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;creating-keyeddatastream&quot;&gt;Creating KeyedDataStream&lt;/h2&gt;

&lt;p&gt;Before we discuss about each of above windows, we need to be aware of one fact. Most of the window operations are encouraged to be used on KeyedDataStream. A KeyedDataStream is a datastream which is partitioned by the key. This partitioning by key allows window to be distributed across machines resulting in good performance. The following code shows how to create a KeyedDataStream from data coming from the socket.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;\\s+&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

 &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above example, we are reading data and creating a stream named &lt;em&gt;source&lt;/em&gt;. Once we receive the data we are extracting the words using &lt;em&gt;flatMap&lt;/em&gt; and &lt;em&gt;map&lt;/em&gt; operators. Once we have a tuple, we are creating a KeyedDataStream using keyBy operation. Once we have a KeyedDataStream, we can start defining the window.&lt;/p&gt;

&lt;p&gt;You can also define windows on non keyed stream. But they often result in poor performance. So I will be not discussing them here.&lt;/p&gt;

&lt;h2 id=&quot;tumbling-window&quot;&gt;Tumbling Window&lt;/h2&gt;

&lt;p&gt;We have already seen this window on our earlier &lt;a href=&quot;/introduction-to-flink-streaming-part-2&quot;&gt;post&lt;/a&gt;. In this section we will be discussing little more.&lt;/p&gt;

&lt;p&gt;A tumbling window is a time based window which tumbles once the window is evaluated. In essence, all the state and records of the window will be purged once the window evaluates. This kind of window is very useful for dividing stream in to multiple discrete batches.&lt;/p&gt;

&lt;p&gt;The below code shows how to create a tumbling window&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tumblingWindow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To create a tumbling window, we use timeWindow API. In above example, stream will be evaluated for every 15 seconds. Here we are calculating the word counts for every 15 seconds.&lt;/p&gt;

&lt;h2 id=&quot;sliding-window&quot;&gt;Sliding window&lt;/h2&gt;

&lt;p&gt;Sliding window is one of most known windowing in streaming. They usually used for keeping running count for a distant past. They allow us to answer questions like “what is word count for last 5 seconds”?.&lt;/p&gt;

&lt;p&gt;Sliding window is also a time based window. The only difference with tumbling window is, an element can belong to multiple windows in sliding window compared to only one window in case of tumbling window. So sliding window normally creates overlapping windows compared to discrete windows in tumbling window.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slidingWindow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above example, we are calculating wordcount for last 15 seconds, in each 5 second interval.&lt;/p&gt;

&lt;h2 id=&quot;count-based-windows&quot;&gt;Count based windows&lt;/h2&gt;

&lt;p&gt;The above two windows were based on time. But in flink we can define windows on other properties also. One of them is count windows. As name suggests, count window are evaluated when no of records received hits the threshold.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countWindow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code defines a count window which fires for after every 5 records. Please note that as the stream is keyed, for each key it tracks no records not across the multiple keys.&lt;/p&gt;

&lt;p&gt;You can access complete code for all the three window &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/WindowExample.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;running-with-examples&quot;&gt;Running with examples&lt;/h2&gt;

&lt;p&gt;Window operations are hard to wrap mind around without examples. So in the next few sections we are going to discuss how to run these examples with sample data and understand their behavior. You can run this example from IDE or in local mode. But before running you need to make sure comment out the non necessary outputs. For example, when you are running tumbling window make sure you comment out the below lines in the code.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;slidingWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;slidingwindow&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;countWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;count window&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We comment out these lines just to make sure they don’t interfere with our output. Follow the same for other two also.&lt;/p&gt;

&lt;p&gt;All the input is entered in stadin of nc command. You can start socket for the program using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;nc -lk localhost 9000&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;running-tumbling-window&quot;&gt;Running tumbling window&lt;/h2&gt;

&lt;p&gt;We are going to run tumbling window in this section. Enter the below lines one by one with in 15 seconds on nc stdin.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;hello how are you
hello who are you&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You will observe the below result once 15 seconds are done.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(hello,2)
(you,2)
(are,2)
(who,1)
(how,1)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you wait for sometime and enter same lines you will observe that the count is reset and you get same result as above. So this shows how tumbling window discretized the stream.&lt;/p&gt;

&lt;h2 id=&quot;running-sliding-window&quot;&gt;Running sliding window&lt;/h2&gt;

&lt;p&gt;In this section we are going to run sliding window.&lt;/p&gt;

&lt;p&gt;If you send the same lines as above in the beginning you will see the result is printed thrice. The reason being, we created a window for 15 seconds which is three times of the 5 seconds. So when window evaluates every 5 seconds, it recalculates the same result. You can add more rows in between to observe how count changes.&lt;/p&gt;

&lt;h2 id=&quot;running-count-window&quot;&gt;Running count window&lt;/h2&gt;
&lt;p&gt;In this section, we are going to run count based windows.&lt;/p&gt;

&lt;p&gt;If you send those two lines, nothing will be printed. The reason is no key has a count of 5.&lt;/p&gt;

&lt;p&gt;Enter the same lines again. Nothing will be printed. Again we have not yet hit the threshold.&lt;/p&gt;

&lt;p&gt;Enter the first line again. Now you will see the below result.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(are,5)
(hello,5)
(you,5)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So as soon as the the count hits 5 window will be triggered. So from the example it’s apparent the count is kept for a key not across keys.&lt;/p&gt;

&lt;p&gt;So window API in flink is very powerful compared to other frameworks. These constructs should allow you to express your application logic elegantly and efficiently.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark-streaming-api&quot;&gt;Compared to Spark Streaming API&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;You can simulate the tumbling window using sliding window operation available in spark. If both window duration and sliding duration is same, you get tumbling window.&lt;/p&gt;

&lt;p&gt;Sliding windows are supported as first class citizens.&lt;/p&gt;

&lt;p&gt;Count based window is not supported in spark streaming. As windowing system of spark is tightly coupled with time, no builtin support for other types of window are there as of now.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Introducing Stream Windows in Apache Flink - &lt;a href=&quot;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&quot;&gt;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Mon, 14 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-5</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-5</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 4 : Understanding Flink&#39;s Advanced Stream Processing using Google Cloud Dataflow</title>
        <description>&lt;p&gt;Flink streaming is a new generation of streaming framework which bring a lot of new concepts to the table. It’s is not just a low latency batch processing like spark streaming or it’s not primitive event processor like storm. It has best of both the worlds and much more.&lt;/p&gt;

&lt;p&gt;As with any new generation framework, it’s hard for people coming from the other frameworks to appreciate full power of the new system. We normally end up trying to replicate same old application on new framework. This happened when we moved from Map/Reduce to Spark. So understanding the strengths of these new frameworks is critical to build new generation streaming applications rather than replicating the already existing one.&lt;/p&gt;

&lt;p&gt;When I was researching on flink streaming , I came across this excellent video which demonstrated power of these new generation streaming frameworks. This video is from flink forward conference which talks about google cloud dataflow. Google cloud dataflow is a unified batch and streaming API for big data from Google. Lot of ideas for flink streaming API are inspired from google dataflow. So understanding motivations and programming model of google dataflow may help to understand the power of flink streaming. Also recently google made it open source under the name &lt;a href=&quot;https://wiki.apache.org/incubator/BeamProposal&quot;&gt;apache beam&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As the fourth blog in the series, I am sharing the google dataflow talk from flink forward below. You can access the slides on &lt;a href=&quot;http://www.slideshare.net/FlinkForward/william-vambenepe-google-cloud-dataflow-and-flink-stream-processing-by-default&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can access other blog in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/y7f6wksGM6c&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

</description>
        <pubDate>Sat, 12 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-4</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-4</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 3 : Running Streaming Applications in Flink Local Mode</title>
        <description>&lt;p&gt;In the last two blogs of our &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;series&lt;/a&gt;, we discussed about how to use flink streaming API’s to do word count. In those blogs we ran our examples from IDE. IDE is a good way to start learning any API. But if we want to understand how code executes in distributed setting it will be good to run it outside of IDE.&lt;/p&gt;

&lt;p&gt;In this third blog, I will be discussing about how to run flink streaming examples in local mode, which is a good starting point to understand distributed nature of flink. Access other blog in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;modes-of-execution-in-flink&quot;&gt;Modes of execution in Flink&lt;/h2&gt;
&lt;p&gt;As with any modern big data frameworks, flink allows user to run the code on different cluster management systems. No change in code is required to run on these different systems. This makes it very easy to change the cluster management system depending on the use case.&lt;/p&gt;

&lt;p&gt;The different modes of execution supported in flink are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;local-mode&quot;&gt;Local Mode&lt;/h3&gt;
    &lt;p&gt;Local mode is a pseudo distributed mode which runs all the daemons in the single jvm. It uses AKKA framework for parallel processing which underneath uses multiple threads.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;standalone-cluster-mode&quot;&gt;Standalone Cluster Mode&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this setup, different daemons runs on different jvms on a single machine or multiple machines. This mode often used when we want to run only Flink in our infrastructure.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;yarn&quot;&gt;YARN&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This mode makes flink run on YARN cluster management. This mode often used when we want to run flink on our existing hadoop clusters.&lt;/p&gt;

&lt;p&gt;Though standalone/yarn is suitable for production, local mode is often good start point to understand the distributed model of flink streaming. So in this blog we will be setting up flink in local mode and run our word count example.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-local-mode&quot;&gt;Setting up local mode&lt;/h2&gt;

&lt;p&gt;The following are the steps for setting up flink in local mode.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;step-1--download-flink-10-distribution&quot;&gt;Step 1 : Download Flink 1.0 distribution&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can download binary distribution of flink &lt;a href=&quot;http://flink.apache.org/downloads.html&quot;&gt;here&lt;/a&gt;. As of now latest version is 1.0. As our examples are compiled against scala 2.10, choose one with that scala version. I chose hadoop 2.6 version for my examples. You can choose the one which matches your hadoop version if you have already any hadoop setup on your machine. If you don’t have any set up choose 2.6.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;step-2--extract-downloaded-file&quot;&gt;Step 2 : Extract downloaded file&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;tar -zxvf flink-1.0.0-bin-hadoop26-scala_2.10.tgz&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;step-3--start-flink-in-local-mode&quot;&gt;Step 3 : Start Flink in local mode&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The below command starts the flink in localmode.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;bin/start-local.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once it started successfully, you can access web UI at &lt;a href=&quot;http://localhost:8081&quot;&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;packaging-code&quot;&gt;Packaging code&lt;/h2&gt;

&lt;p&gt;Once we have the local flink running, we need to package our code as the jar in order to run. Use the below sbt command to create jar.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;sbt clean package&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once the build is successful, you will get &lt;em&gt;flink-examples_2.10-1.0.jar&lt;/em&gt; under directory &lt;em&gt;target/scala-2.10&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;running-wordcount-in-local-mode&quot;&gt;Running wordcount in local mode&lt;/h2&gt;

&lt;p&gt;Once we have packaged jar, now we can run it outside IDE. Run the below command from flink directory. Replace the path to jar with absolute path to the jar generated in last step.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;bin/flink run -c com.madhukaraphatak.flink.streaming.examples.StreamingWordCount &amp;lt;path-to-flink-examples_2.10-1.0.jar&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command uses &lt;em&gt;flink&lt;/em&gt; command to run the example. &lt;em&gt;flink&lt;/em&gt;  is a command used for interact with jobs. The &lt;em&gt;run&lt;/em&gt; sub command is used for submit jobs. &lt;em&gt;-c&lt;/em&gt; option indicates the jar to be added to classpath. Next two parameters are main class and the jar path.&lt;/p&gt;

&lt;p&gt;Once you run the above command, wordcount start running in the local mode. You can see this running job &lt;a href=&quot;http://localhost:8081/#/running-jobs&quot;&gt;here&lt;/a&gt;. As you enter the lines in socket you can observe output &lt;a href=&quot;http://localhost:8081/#/jobmanager/stdout&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now we have successfully ran our example outside IDE in flink local mode.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark&quot;&gt;Compared to Spark&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;The local mode of flink, is similar to spark local mode. &lt;em&gt;flink&lt;/em&gt; command is similar to &lt;em&gt;spark-submit&lt;/em&gt;. One of the difference is, in spark you don’t need to start web ui daemon as spark itself starts it when you create spark context.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Flink Local Setup - &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/setup/local_setup.html&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-master/setup/local_setup.html&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 10 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-3</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-3</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 2 : Discretization of Stream using Window API</title>
        <description>&lt;p&gt;In the last &lt;a href=&quot;/introduction-to-flink-streaming-part-1&quot;&gt;blog&lt;/a&gt;, we discussed about how to do continuous stream processing using flink streaming API. Our wordcount example keeps on updating the counts as and when we received new data. This is good for some of the use cases. But in some use cases we want to aggregate some set of records in a given time interval, in order to keep track of variance over time. In those cases, we need to divide the stream into small batches. This discretization allow us to capture the change in aggregated value overtime. This discretized batches is also known as micro batches.&lt;/p&gt;

&lt;p&gt;In this second blog, I will be discussing about how to discretized the stream using flink’s window operation.We will be using same word count example in this post also. You can access all the blogs in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;window-in-streaming&quot;&gt;Window in streaming&lt;/h2&gt;

&lt;p&gt;Window is a mechanism to take a snapshot of the stream. This snapshot can be based on time or other variables. For example, if we create a window for 5 seconds then it will be all the records which arrived in the that time frame. You can define the window based on no of records or other stream specific variables also.&lt;/p&gt;

&lt;p&gt;Window allows us to understand change in stream data by taking snapshots in regular intervals. Flink API has wide variety of window support. In this post we are only going to focus on one kind of window known as Tumbling Windows.&lt;/p&gt;

&lt;h2 id=&quot;tumbling-window-in-flink-streaming&quot;&gt;Tumbling Window in Flink Streaming&lt;/h2&gt;

&lt;p&gt;Tumbling window is one kind of windowing operation which will discretize the stream into non overlapping windows. This means every record in the stream only belongs to one window. This kind of discretization allows observing the change in the stream over fixed intervals. There are other kind of windows supported in the flink which we will discuss in the future posts.&lt;/p&gt;

&lt;h2 id=&quot;windowed-wordcount-example&quot;&gt;Windowed Wordcount example&lt;/h2&gt;

&lt;p&gt;In last blog, we saw how to calculate wordcount using Flink API. In this post, we will be calculating wordcount for every 15 seconds. So in this example, we will be dividing the stream for every 15 seconds. Once those 15 seconds passes, the count will be started from zero.&lt;/p&gt;

&lt;p&gt;This example shows to how to snapshot wordcount for each 15 seconds to analyze the trend over time to say how wordcount have changed. This is not possible when we have continuous updation of the count as in earlier example.&lt;/p&gt;

&lt;p&gt;Most of the code to setup and run is same as earlier example. So I am only going to focus on how to add tumbling window to our stream&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordsStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeWindow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code creates a window using timeWindow API. timeWindow API internally uses tumbling window API to do the windowing operation. In this case, the wordcount will be counted for each 15 seconds and then forgotten.&lt;/p&gt;

&lt;p&gt;You can access complete code &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/WindowedStreamingWordCount.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark-streaming-api&quot;&gt;Compared to Spark Streaming API&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;The tumbling windows in Flink are similar to microbatches in Spark. As in spark microbatch, tumbling windows are used for discretizing stream into independent batches.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Introducing Stream Windows in Apache Flink - &lt;a href=&quot;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&quot;&gt;https://flink.apache.org/news/2015/12/04/Introducing-windows.html&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-2</guid>
      </item>
    
      <item>
        <title>Introduction to Flink Streaming - Part 1 : WordCount</title>
        <description>&lt;p&gt;Apache Flink is one of the new generation distributed systems which unifies batch and streaming processing. Earlier in my blog, I have &lt;a href=&quot;/introduction-to-flink-for-spark-developers-flink-vs-spark&quot;&gt;discussed&lt;/a&gt; about how it’s different than Apache Spark and also given a introductory &lt;a href=&quot;/introduction-to-flink-talk&quot;&gt;talk&lt;/a&gt; about it’s batch API. In batch world, Flink looks very similar to Spark API as it uses similar concepts from Map/Reduce. But in the case of streaming, flink is much different than the Spark or any other stream processing systems out there.&lt;/p&gt;

&lt;p&gt;So in these series of blogs, I will be discussing about how to get started with flink streaming API and using it’s different unique features. Flink streaming API has undergone significant changes from 0.10 to 1.0 version. So I will be discussing latest 1.0 API. You can access all the blogs in the series &lt;a href=&quot;/categories/flink-streaming/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this first blog, I will be discussing about how to run word count example in flink streaming. If you are new to flink, I encourage you to watch my &lt;a href=&quot;/introduction-to-flink-talk&quot;&gt;introductory talk&lt;/a&gt; before continuing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; All code is written using Flink’s scala API and you can access it on &lt;a href=&quot;https://github.com/phatak-dev/flink-examples&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;flink-streaming-api&quot;&gt;Flink Streaming API&lt;/h2&gt;

&lt;p&gt;Flink provides a streaming API called as Flink DataStream API to process continuous unbounded streams of data in realtime. This API build on top of the pipelined streaming execution engine of flink.&lt;/p&gt;

&lt;p&gt;Datastream API has undergone a significant change from 0.10 to 1.0. So many examples you see in the other blogs including flink blog have become obsolete. I will be discussing about Flink 1.0 API which is released in maven central and yet to be released in binary releases.&lt;/p&gt;

&lt;h2 id=&quot;adding-dependency&quot;&gt;Adding dependency&lt;/h2&gt;

&lt;p&gt;To start using Datastream API, you should add the following dependency to project. I am using sbt for build management. You can also use other build tools like maven.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;org.apache.flink&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;flink-scala&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;1.0.0&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can access complete build.sbt &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/build.sbt&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;hello-world-example&quot;&gt;Hello World Example&lt;/h2&gt;

&lt;p&gt;Whenever we learn any new API in big data, it has become custom to do word count. In this example, we are reading some lines from a socket and doing word count on them.&lt;/p&gt;

&lt;p&gt;The below are the steps to write an streaming example in datastream API.&lt;/p&gt;

&lt;h3 id=&quot;step-1-get-streaming-environment&quot;&gt;Step 1. Get Streaming Environment&lt;/h3&gt;

&lt;p&gt;In both batch and streaming example, first step is to create a pointer to environment on which this program runs. Flink can run same program in local or cluster mode. You can read more about modes &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/apis/common/index.html#anatomy-of-a-flink-program&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;StreamExecutionEnvironment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getExecutionEnvironment&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you are familiar with Spark, StreamExecutionEnvironment is similar to spark context.&lt;/p&gt;

&lt;p&gt;One of the things to remember when using scala API of Flink is to import the implicts. If you don’t import them you will run into strange error messages.&lt;/p&gt;

&lt;p&gt;You can import the implicts for streaming as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;org.apache.flink.streaming.api.scala._&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-2-create-datastream-from-socket&quot;&gt;Step 2. Create DataStream from socket&lt;/h3&gt;

&lt;p&gt;Once we have the pointer to execution environment, next step is to create a stream from socket.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;socketStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socketTextStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;socketStream&lt;/em&gt; will be of the type DataStream. DataStream is basic abstraction of flink’s streaming API.&lt;/p&gt;

&lt;h3 id=&quot;step-3-implement-wordcount-logic&quot;&gt;Step 3. Implement wordcount logic&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordsStream&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;socketStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;\\s+&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordsStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countPair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyValuePair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above is very standard code to do word count in map/reduce style. Notable differences are we are using keyBy rather than groupBy and sum for reduce operations. The value 0 and 1 in keyBy and sum calls signifies the index of columns in tuple to be used as key and values.&lt;/p&gt;

&lt;h3 id=&quot;step-4-print-the-word-counts&quot;&gt;Step 4. Print the word counts&lt;/h3&gt;

&lt;p&gt;Once we have wordcount stream, we want to call print, to print the values into standard output&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;countPair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-5-trigger-program-execution&quot;&gt;Step 5. Trigger program execution&lt;/h3&gt;

&lt;p&gt;All the above steps only defines the processing, but do not trigger execution. This needs to be done explicitly using execute.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have complete code for the word count example. You can access full code &lt;a href=&quot;https://github.com/phatak-dev/flink-examples/blob/master/src/main/scala/com/madhukaraphatak/flink/streaming/examples/StreamingWordCount.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;executing-code&quot;&gt;Executing code&lt;/h2&gt;

&lt;p&gt;To run this example, we need to start the socket at 9000 at following command to&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;nc -lk 9000&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once you do that, you can run the program from the IDE and &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/apis/cli.html&quot;&gt;command line interface&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can keep on entering the lines in nc command line and press enter. As you pass the lines you can observe the word counts printed on the stdout.&lt;/p&gt;

&lt;p&gt;Now we have successfully executed the our first flink streaming example.&lt;/p&gt;

&lt;h2 id=&quot;unbounded-state&quot;&gt;Unbounded state&lt;/h2&gt;

&lt;p&gt;If you observe the result, as an when you pass more rows the count keeps increasing. This indicates that flink keeps updating the count state indefinitely. This may be desired in some examples, but most of the use cases we want to limit the state to some certain time. We will see how to achieve it using window functionality in the next blog in the series.&lt;/p&gt;

&lt;h2 id=&quot;compared-to-spark-streaming-api&quot;&gt;Compared to Spark Streaming API&lt;/h2&gt;

&lt;p&gt;This section is only applicable to you, if you have done spark streaming before. If you are not familiar with Apache Spark feel free to skip it.&lt;/p&gt;

&lt;p&gt;The above code looks a lot similar to Spark streaming’s DStream API. Though syntax looks same there are few key differences. Some of them are&lt;/p&gt;

&lt;h3 id=&quot;no-need-of-batch-size-in-flink&quot;&gt;1. No need of Batch Size in Flink&lt;/h3&gt;

&lt;p&gt;Spark streaming needs batch size to be defined before any stream processing. It’s because spark streaming follows micro batches for stream processing which is also known as near realtime . But flink follows one message at a time way where each message is processed as and when it arrives. So flink doesnot need any batch size to be specified.&lt;/p&gt;

&lt;h3 id=&quot;state-management&quot;&gt;2. State management&lt;/h3&gt;

&lt;p&gt;In spark, after each batch, the state has to be updated explicitly if you want to keep track of wordcount across batches. But in flink the state is up-to-dated as and when new records arrive implicitly.&lt;/p&gt;

&lt;p&gt;We discuss more differences in future posts.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Apache Flink 1.0 Streaming Guide - &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-master/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Introducing Flink Streaming - &lt;a href=&quot;https://flink.apache.org/news/2015/02/09/streaming-example.html&quot;&gt;https://flink.apache.org/news/2015/02/09/streaming-example.html&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-1</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-flink-streaming-part-1</guid>
      </item>
    
      <item>
        <title>Interactive Scheduling using Azkaban - Part 2 : Challenges in scheduling interactive workloads</title>
        <description>&lt;p&gt;Every big data application needs some kind of scheduling to run daily jobs. So over the years having a good stable scheduling systems for hadoop, spark jobs has become more and more important. The different workloads in big data have different requirements from  the scheduler. So in this blog post I will be discussing about different scheduling requirements for batch, streaming and interactive usecases and challenges associated with interactive workload.&lt;/p&gt;

&lt;p&gt;This is second post in series of blogs where I will be discussing about using Azkaban scheduler to do interactive scheduling. You can access all other posts from the series &lt;a href=&quot;/categories/azkaban&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;scheduling-needs-of-different-big-data-workloads&quot;&gt;Scheduling needs of different big data workloads&lt;/h2&gt;

&lt;p&gt;The below are the different requirements of big data workloads from scheduler system.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;batch&quot;&gt;Batch&lt;/h3&gt;
    &lt;p&gt;Set of jobs which needs to be executed on timely manner. In this scenario, a scheduler system needs to allow user to define the script with all the dependencies of a flow and allow it to be scheduled. To add/modify the jobs user will normally changes the script and runs the updated ones. The examples for these kind of scheduler systems are Ozzie, airflow etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;streaming&quot;&gt;Streaming&lt;/h3&gt;
    &lt;p&gt;Continuous stream of data is processed to produce results. Normally streaming only needs scheduler to initiate stream processing system and from there streaming framework will take over.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above two scenarios are one of most supported and common place in big data world from quite sometime. So all the scheduling system, including azkaban, supports them well. But there is a new workload emerging these days which needs special attention.&lt;/p&gt;

&lt;h2 id=&quot;interactive-big-data-workload&quot;&gt;Interactive big data workload&lt;/h2&gt;

&lt;p&gt;As spark became popular, it has made interactive programming as one of the important part of big data workloads. In interactive settings, a user will be analyzing the data adhocly and once he/she is happy with the steps then they want to schedule them to run in timely manner.&lt;/p&gt;

&lt;p&gt;The notebook systems like Zeppelin,Jupiter have made interactive programming highly popular. Initially used for the data science use cases, they are also used for data engineering use cases these days.&lt;/p&gt;

&lt;p&gt;So as interactive workloads becoming common place, supporting ability to scheduling jobs interactively becoming more and more important. But doing this with existing systems is not easy.&lt;/p&gt;

&lt;h2 id=&quot;challenges-of-scheduling-interactive-workloads&quot;&gt;Challenges of scheduling Interactive workloads&lt;/h2&gt;

&lt;p&gt;Unlike batch workloads, interactive workloads are not static. They evolve as user adds/removes the code. Normally user may want to add / remove scheduling on the fly rather than modifying the script. So the non azkaban frameworks cannot be used in scenario because of following reasons.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;nolimited-rest-api-support&quot;&gt;No/Limited REST API support&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of the scheduling systems like oozie have very limited support for programmatic access. Often they rely upon the traditional scripting world, where you need to configure jobs using script and submit them. It works great for batch, but cannot be used for interactive applications as they need an good programmatic API to schedule jobs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;lack-of-good-user-interface-for-monitoring&quot;&gt;Lack of good user interface for monitoring&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of the scheduling system have very limited user interfaces. Most of them limit themselves to show work flow graphs. Also many of them doesn’t allow users to extend the user interfaces which results in building the custom ones themselves.&lt;/p&gt;

&lt;p&gt;In batch, normally user interface is not that important. But in interactive it plays a huge role. Ability to monitor the jobs in a small time frame is important as it results in a good user feed back.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;support-for-different-executors&quot;&gt;Support for different executors&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many scheduling systems limit themselves for Hadoop or Spark. But in interactive application one often likes to run different kind of processing on same system. So ability to run different workloads becomes extremely important.&lt;/p&gt;

&lt;p&gt;So from above points it’s clear that the most of the existing scheduler systems are geared towards the batch processing scenarios. So using them in a interactive application is hard.&lt;/p&gt;

&lt;h2 id=&quot;note-on-azkaban-for-batch-workload&quot;&gt;Note on Azkaban for batch workload&lt;/h2&gt;

&lt;p&gt;Though my blog posts are focusing on using azkaban for interactive workloads, azkaban fares well in the batch also. Even most of it’s documentation is dedicated to do batch scheduling using it’s web UI rather than for interactive workloads. But with it’s hidden gem of REST API, it’s well suited for the interactive applications too.&lt;/p&gt;

&lt;p&gt;So in this blogpost, we discussed about challenges in scheduling interactive workloads. In the next blogpost, we are going to discuss how azkaban solves these issues and is good candidate scheduler framework for interactive workloads.&lt;/p&gt;

</description>
        <pubDate>Mon, 07 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/interactive-scheduling-using-azkaban-challenges-in-scheduling-interactive-workloads</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/interactive-scheduling-using-azkaban-challenges-in-scheduling-interactive-workloads</guid>
      </item>
    
      <item>
        <title>What&#39;s New in Spark : Tales from Spark Summit East - Framework Improvements</title>
        <description>&lt;p&gt;Recently Databricks, company behind the Apache Spark, held this year’s first &lt;a href=&quot;https://spark-summit.org/east-2016/&quot;&gt;spark summit&lt;/a&gt;, spark developer conference, in new york city. Lots of new exciting improvements in spark and it’s ecosystem got discussed in various talks. I was watching the videos of the conference on youtube and wanted to share the ones I found interesting.&lt;/p&gt;

&lt;p&gt;These are the series of blog posts focusing on various talks categorized into different aspects of Spark. You can access all the posts in the series &lt;a href=&quot;/categories/spark-summit-east-2016&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is the first post in the series where I will be sharing talks which focused on improvements to the core of the spark itself.&lt;/p&gt;

&lt;h3 id=&quot;matei-zaharia-keynote-on-spark-20&quot;&gt;1. Matei Zaharia keynote on Spark 2.0&lt;/h3&gt;

&lt;p&gt;Matei Zaharia, Spark’s creator, laid out plans for next version of Spark in his keynote. His talks mainly revolved around performance and new abstraction like Dataset. Spark 2.0 is one of the major steps in spark’s evolution.You can read more about my thoughts on Spark 2.0 &lt;a href=&quot;/introduction-to-spark-2.0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The below is the video of the talk. You can find slides on &lt;a href=&quot;http://www.slideshare.net/databricks/2016-spark-summit-east-keynote-matei-zaharia&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/ZFBgY0PwUeY&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

&lt;h3 id=&quot;structuring-spark-dataframes-datasets-and-spark-streaming&quot;&gt;2. Structuring Spark: Dataframes, Datasets and Spark Streaming&lt;/h3&gt;

&lt;p&gt;Structured data analysis has become very important part of spark in last few releases. More and more work is done on Dataframe API compared to RDD API. In this talk speaker talks about how these API’s share common core and how they are planning to bring the same API’s for stream analysis also.&lt;/p&gt;

&lt;p&gt;The below is the video of the talk. You can find slides on &lt;a href=&quot;http://www.slideshare.net/SparkSummit/structuring-spark-dataframes-datasets-and-streaming-by-michael-armbrust&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/i7l3JQRx7Qw&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

&lt;h3 id=&quot;the-future-of-real-time-spark---a-revamped-spark-streaming&quot;&gt;3. The Future of Real Time Spark - A Revamped Spark Streaming&lt;/h3&gt;

&lt;p&gt;Building streaming application is hard. Combining batch processing and stream processing in a single application needs a lot of design and detailed implementation. Compared to other components of Spark, there was not much going on in spark streaming for a while. But its more the case. Spark 2.0 going to bring a completely new revamped API for spark streaming.&lt;/p&gt;

&lt;p&gt;The below is the video of the talk. You can find slides on &lt;a href=&quot;http://www.slideshare.net/rxin/the-future-of-realtime-in-spark&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/oXkxXDG0gNk&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

&lt;h3 id=&quot;spark-performance-whats-next---10x-performance-improvement-in-spark-20&quot;&gt;4. Spark performance: What’s Next - 10x performance improvement in Spark 2.0&lt;/h3&gt;

&lt;p&gt;With introduction of tungsten and codegen in 1.4, spark performance is significantly improved in last few releases. Spark 2.0 bring whole new set of techniques which going to take the spark performance to next level. In this talk, speaker talks about different techniques getting developed to improve spark performance. Most of these already in master branch which you can start using to test it yourself.&lt;/p&gt;

&lt;p&gt;The below is the video of the talk. You can find slides on &lt;a href=&quot;http://www.slideshare.net/databricks/spark-performance-whats-next&quot;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/JX0CdOTWYX4&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

&lt;p&gt;In next blog in the series, I will be sharing my thoughts on the talks which focused on the ecosystem around spark.&lt;/p&gt;
</description>
        <pubDate>Sat, 05 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/whats-new-in-spark-framework-improvements</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/whats-new-in-spark-framework-improvements</guid>
      </item>
    
      <item>
        <title>Introduction to Spark 2.0 : A Sneak Peek At Next Generation Spark</title>
        <description>&lt;p&gt;Spark 2.0 is the next stable of release of spark, which is expected to be released in April/May 2016. As the major version bump suggests, its is going to be bring some drastic changes to framework. In recent &lt;a href=&quot;https://spark-summit.org/east-2016/&quot;&gt;spark summit&lt;/a&gt;, spark contributors discussed some of those in various talks.&lt;/p&gt;

&lt;p&gt;In this post, I am going to talk about some of the important changes made to the framework and as a spark developer how we can prepare for it.&lt;/p&gt;

&lt;h2 id=&quot;embrace-dataset-dataframe-api-over-rdd-api&quot;&gt;1. Embrace Dataset/ Dataframe API over RDD API&lt;/h2&gt;

&lt;p&gt;Normally, whoever starts learning spark first learns about RDD abstraction. RDD was one of the novel idea of Spark which gave us a single abstraction over different big data workloads like batch, streaming, ML etc. So naturally RDD API became the way people build spark applications.&lt;/p&gt;

&lt;p&gt;But overtime, people have realized RDD as a user facing API held back spark runtime from advanced optimizations. So from Spark 1.3, Dataframe API was introduced to solve some of the optimization issues. Dataframe brought custom memory management and runtime code generation which greatly improved performance. So in last year most of the improvements went into Dataframe API whereas RDD API stood still.&lt;/p&gt;

&lt;p&gt;Though dataframe API solved many issues, it was not a good enough replacement for RDD API. One of the major issues with dataframe API was no compile time safety and not able to work with domain objects. So this held back people using dataframe API everywhere. But with introduction of Dataset API in 1.6, we were able to fill the gap.&lt;/p&gt;

&lt;p&gt;So in Spark 2.0, Dataset API will be become a stable API. So Dataset API combined with Dataframe API should able to cover most of the use cases where RDD was used earlier. So as a spark developer it is advised to start embracing these two API’s over RDD API from Spark 2.0.&lt;/p&gt;

&lt;p&gt;Does that mean RDD API will be removed? Not really. Spark as a project is very serious about backward compatibility. So they don’t want to remove any stable API’s. So RDD API will remain as low level API mostly used by runtime. As developer you will be using Dataset or Dataframe API from Spark 2.0.&lt;/p&gt;

&lt;p&gt;Everything above sounds great for batch processing, but what about Spark streaming? Spark streaming has lot of API’s around RDD. What will happen to them?&lt;/p&gt;

&lt;h2 id=&quot;structured-stream-processing&quot;&gt;2. Structured Stream processing&lt;/h2&gt;

&lt;p&gt;Spark 2.0 will introduce structured stream processing, which is a higher level API to do stream processing. It essentially going to leverage dataframe and dataset API for stream processing which will get rid of using RDD as an abstraction. Not only this bring dataframes to stream processing, it brings bunch of other benefits like datasource API support for spark streaming. You can watch &lt;a href=&quot;https://www.youtube.com/watch?v=i7l3JQRx7Qw&quot;&gt;this video&lt;/a&gt; to know more.&lt;/p&gt;

&lt;p&gt;So from Spark 2.0, you will be interacting with spark streaming using same DF abstraction that you were using in batch layer.&lt;/p&gt;

&lt;h2 id=&quot;dataset-is-the-new-single-abstraction&quot;&gt;3. Dataset is the new single abstraction&lt;/h2&gt;

&lt;p&gt;Spark always loved to have a single abstraction which it made it to improve in a rapid phase. Also it meant that different libraries can exchange data between each other. So doesn’t having Dataframe and Dataset as two API’s beats that single abstraction idea?&lt;/p&gt;

&lt;p&gt;Currently in Spark 1.6, these are two independent abstractions. But from 2.0 Dataframe will be a special case of Dataset API. So this make dataset as a single abstraction on which all the API’s are build. So Dataset will be new single abstraction which will take the place of RDD in the user API world.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;4. Performance improvements&lt;/h2&gt;

&lt;p&gt;Spark 2.0 going to bring many performance improvements thanks to tungsten and code generation. It’s been one of the constant theme in last few releases to going 1-2x performance gains. But in 2.0, data bricks is promising 6-10x performance gains. It’s particularly do with more and more intelligent code generation and moving libraries on better abstraction layers like dataset.&lt;/p&gt;

&lt;p&gt;So the above are the most important updates landing in Spark 2.0. I will be discussing more about this as and when we get to see these changes getting implemented.&lt;/p&gt;

&lt;p&gt;Source : You can access all the talks and slides of spark summit from &lt;a href=&quot;https://spark-summit.org/east-2016/schedule/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Fri, 04 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/introduction-to-spark-2.0</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/introduction-to-spark-2.0</guid>
      </item>
    
      <item>
        <title>Interactive Scheduling using Azkaban - Part 1 : Setting up Solo Server</title>
        <description>&lt;p&gt;Azkaban is a scheduler for big data workloads like Hadoop, Spark. One of the differentiator of azkaban compared to other schedulers like oozie, airflow is it has good support for REST API to interact with scheduler problematically. This programmatic access is important for interactive applications.&lt;/p&gt;

&lt;p&gt;In these series of blogs I will be discussing about setting up azkaban and using azkaban AJAX(REST) API.&lt;/p&gt;

&lt;p&gt;This is the first post in series, where we discuss about setting up azkaban. In this post, we will be setting up azkaban 3.0.&lt;/p&gt;

&lt;h2 id=&quot;building-azkaban&quot;&gt;Building Azkaban&lt;/h2&gt;

&lt;p&gt;Though azkaban provides binary &lt;a href=&quot;http://azkaban.github.io/downloads.html&quot;&gt;downloads&lt;/a&gt; it is not up to date. So we will be getting latest code from the github in order to build azkaban 3.0.&lt;/p&gt;

&lt;p&gt;The following are the steps to get code and build.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;clone-code&quot;&gt;Clone code&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;git clone https://github.com/azkaban/azkaban.git&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;build&quot;&gt;Build&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;./gradlew distZip&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;copy-from-build&quot;&gt;Copy from build&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;cp build/distributions/azkaban-solo-server-3.0.0.zip ~&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;installing-solo-server&quot;&gt;Installing solo server&lt;/h2&gt;

&lt;p&gt;Azkaban supports different mode of executions like solo server, two server mode and multiple executor mode. Solo server is used for initial developments where as other ones are geared towards production scenarios. In this blog, we discuss about setting up solo server, for other modes refer &lt;a href=&quot;http://azkaban.github.io/azkaban/docs/latest/#getting-started&quot;&gt;azkaban documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The below are the steps for installing.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;unzip&quot;&gt;Unzip&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;unzip ~/azkaban-solo-server-3.0.0.zip
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/azkaban-solo-server-3.0.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;starting-solo-server&quot;&gt;Starting solo server&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;bin/azkaban-solo-start.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;accessing-log&quot;&gt;Accessing log&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;tail -f logs/azkaban-execserver.log&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;accessing-web-ui&quot;&gt;Accessing web UI&lt;/h2&gt;

&lt;p&gt;Once azkaban solo server started, you can access at &lt;a href=&quot;http://localhost:8081/&quot;&gt;http://localhost:8081&lt;/a&gt;. By default username is &lt;em&gt;azkaban&lt;/em&gt; and password is &lt;em&gt;azkaban&lt;/em&gt;. You can change it in &lt;em&gt;conf/azkaban-users.xml&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now you have successfully installed azkaban server. In the next set of posts, we will explore how to use this installation to do scheduling.&lt;/p&gt;
</description>
        <pubDate>Thu, 03 Mar 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/interactive-scheduling-using-azkaban-setting-up-solo-server</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/interactive-scheduling-using-azkaban-setting-up-solo-server</guid>
      </item>
    
      <item>
        <title>Building Distributed Systems from Scratch - Part 2 : Handling third party libraries</title>
        <description>&lt;p&gt;The below video is a recording of my talk on &lt;em&gt;Building Distributed Systems from Scratch - Part 2&lt;/em&gt; in recent spark meet up. It’s second talk in series of talks about how to build a distributed processing system from scratch which looks similar to Apache Spark. This talk focuses on how to implement a system to distribute third party libraries on mesos.&lt;/p&gt;

&lt;p&gt;Find the slides on &lt;a href=&quot;http://www.slideshare.net/datamantra/building-distributed-processing-system-from-scratch-part-2&quot;&gt;slideshare&lt;/a&gt; and code on &lt;a href=&quot;https://github.com/phatak-dev/distributedsystems&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/Oj8IO8OICAc&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;
</description>
        <pubDate>Wed, 17 Feb 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/building-distributed-systems-from-scratch-part2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/building-distributed-systems-from-scratch-part2</guid>
      </item>
    
  </channel>
</rss>
