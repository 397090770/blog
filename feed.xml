<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Madhukar's Blog</title>
    <description>Thoughts on technology, life and everything else.</description>
    <link>http://blog.madhukaraphatak.com/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Migrating to Spark 2.0 - Part 1: Scala Version and Dependencies</title>
        <description>&lt;p&gt;Spark 2.0 brings a significant changes to abstractions and API’s of spark platform. With performance boost, this version has made some of non backward compatible changes to the framework. To keep up to date with the latest updates, one need to migrate their spark 1.x code base to 2.x. In last few weeks, I was involved in migrating one of fairly large code base and found it quite involving process. In this series of posts, I will be documenting my experience of migration so it may help all the ones out there who are planning to do the same.&lt;/p&gt;

&lt;p&gt;This is the first post in this series. In this post, we will discuss how to upgrade our dependencies to add right support for spark 2.0. You can access all the posts &lt;a href=&quot;/categories/spark-two-migration-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;choosing-right-scala-version&quot;&gt;Choosing Right Scala Version&lt;/h2&gt;

&lt;p&gt;When you want to upgrade from spark 1.x to spark 2.x, first task is to pick the right scala version. In spark 1.x, spark was built using scala version 2.10.6. But from spark 2.0, the default version is changed to 2.11.8. 2.10 version is still supported even though it’s not default.&lt;/p&gt;

&lt;p&gt;Scala major versions are non binary compatible, i.e you cannot mix and match the libraries built using 2.10 and 2.11. So whenever you change the scala version of the project, you need to upgrade all the libraries of the project including non-spark ones. It’s a significant work as you need to comb through each and every dependency and make sure right version exist.&lt;/p&gt;

&lt;p&gt;Initially I started the upgrade using Scala 2.10 as it was least resistance path. All the other external libraries needed no change and it was smooth. But I soon realised the distribution at spark download page &lt;a href=&quot;https://spark.apache.org/downloads.html&quot;&gt;https://spark.apache.org/downloads.html&lt;/a&gt; is only built using scala 2.11. So to support 2.10 I have to build my own distribution. Also I came across the &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-19810&quot;&gt;jira&lt;/a&gt; which discusses about removing scala 2.10 support altogether in 2.3.0. So this meant investing in 2.10 will be not good as it will be obsolete in next few versions.&lt;/p&gt;

&lt;p&gt;So I chose &lt;em&gt;2.11.8&lt;/em&gt; as my scala version for upgrade.&lt;/p&gt;

&lt;h2 id=&quot;choosing-right-java-version&quot;&gt;Choosing Right Java Version&lt;/h2&gt;

&lt;p&gt;From Spark 2.1.0 version, support for Java 7 has been deprecated. So I started using Java 8 for building and deploying the code.&lt;/p&gt;

&lt;h2 id=&quot;updating-external-dependencies&quot;&gt;Updating External Dependencies&lt;/h2&gt;

&lt;p&gt;One of the major challenges of changing scala version is to update all the project dependencies. My project had a fair bit of them and luckily all of those libraries had scala 2.11 version. So please make sure that all the libraries have 2.11 version before you make decision to change scala version.&lt;/p&gt;

&lt;h2 id=&quot;updating-connectors&quot;&gt;Updating Connectors&lt;/h2&gt;

&lt;p&gt;There are major changes happened to the connector ecosystem in spark 2.0. So when you are upgrading to spark 2.0, you need to make sure that you use the right connectors.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;removal-of-built-in-streaming-connectors&quot;&gt;Removal of Built in Streaming Connectors&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Earlier spark had support for zeromq, twitter as part of spark streaming code base. But in spark 2.x, they have removed &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13843&quot;&gt;it&lt;/a&gt;. No more these connectors are part of spark-streaming. This is done mostly to develop these connectors independent of spark versions. So if you are using these connector code will break.&lt;/p&gt;

&lt;p&gt;To fix this issue, you need to update the dependencies to point to &lt;a href=&quot;https://github.com/apache/bahir&quot;&gt;Apache Bahir&lt;/a&gt;. Apache Bahir is new home to all of this deleted connectors. Follow the README of bahir repository to update the dependencies to bahir ones.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;spark-20-specific-connectors&quot;&gt;Spark 2.0 specific connectors&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many popular connectors now give spark 2.0 specific connectors to build with. These connectors provide both scala 2.10 and 2.11 version. Choose the right one depending upon the scala version you have chosen. As I have chosen 2.11, the below are the some of updated connectors for some sources&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;elastic-search&quot;&gt;Elastic Search&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Elastic search has a dedicated spark connector which was used to be called as elasticsearch-hadoop. You can access latest connector &lt;a href=&quot;https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11/5.3.0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;csv-connector&quot;&gt;Csv Connector&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From Spark 2.0, csv is built in source. Earlier we used to use &lt;a href=&quot;https://github.com/databricks/spark-csv&quot;&gt;spark-csv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are using any other connector, make sure they support 2.0. One thing to note that, if the connector is available in right scala version, it doesn’t need any code changes to support spark 2.x. Spark 2.x data source API is backward compatible with spark 1.x&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So by updating scala version, java version and using right connectors you can update your project build to use spark 2.x.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;In next blog, we will be discuss about major changes in spark csv connector.&lt;/p&gt;

</description>
        <pubDate>Thu, 13 Apr 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/migrating-to-spark-two-part-1</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/migrating-to-spark-two-part-1</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 7 : Dynamic Scaling and Namespaces</title>
        <description>&lt;p&gt;In our last post we created two node spark cluster using kubernetes. Once we have defined and created the cluster
we can easily scale up or scale down using kubernetes. This elastic nature of kubernetes makes easy to scale
the infrastructure as and when the demand increases rather than setting up everything upfront.&lt;/p&gt;

&lt;p&gt;In this seventh blog of the series, we will discuss how to scale the spark cluster on kubernetes.
You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-scaling&quot;&gt;Dynamic Scaling&lt;/h2&gt;

&lt;p&gt;When we discussed deployment abstraction in our previous blog, we talked about &lt;em&gt;replica&lt;/em&gt; factor. In deployment configuration, we can specify the number
of replications we need for a given pod. This number is set to 1 in our current spark worker deployment.&lt;/p&gt;

&lt;p&gt;One of the nice thing about deployment abstraction is, we can change replica size dynamically without changing configuration. This
allows us to scale our spark cluster dynamically.&lt;/p&gt;

&lt;h3 id=&quot;scale-up&quot;&gt;Scale Up&lt;/h3&gt;

&lt;p&gt;Run below command to scale up workers from 1 to 2.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl scale deployment spark-worker --replicas 2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command takes deployment name as parameters and number of replicas. 
You can check results using&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When you run the above command, kubernetes creates more pods using template specified in spark-worker. Whenever these
pods come up they automatically connect to spark-master and scales the cluster.&lt;/p&gt;

&lt;h3 id=&quot;scale-down&quot;&gt;Scale Down&lt;/h3&gt;

&lt;p&gt;We can not only increase the workers, we can also scale down by setting lower replica numbers.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl scale deployment spark-worker --replicas 1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When above command executes, kubernetes will kill one of the worker to reduce the replica count to 1.&lt;/p&gt;

&lt;p&gt;Kubernetes automatically manages all the service related changes. So whenever we scale workers spark will automatically scale.&lt;/p&gt;

&lt;h2 id=&quot;multiple-clusters&quot;&gt;Multiple Clusters&lt;/h2&gt;

&lt;p&gt;Till now, we have run single cluster. But sometime we may want to run multiple clusters on same kubernetes cluster. If we try to run
same configurations twice like below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-master.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You will get below error&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Error from server: error when creating &amp;quot;spark-master.yaml&amp;quot;: deployments.extensions &amp;quot;spark-master&amp;quot; already exists&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Kubernetes is rejecting the request as the spark-master named deployment is already exist. One of the way to solve this issue is to
duplicate the configurations with different name. But it will be tedious and difficult to maintain.&lt;/p&gt;

&lt;p&gt;Better way to solve this issue to use  namespace abstraction of kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;namespace-abstraction&quot;&gt;Namespace Abstraction&lt;/h3&gt;

&lt;p&gt;Kubernetes allows users to create multiple virtual clusters on single physical cluster. These are called as namespaces.&lt;/p&gt;

&lt;p&gt;Namespace abstraction is used for allowing multiple users to share the same physical cluster. This abstraction gives scopes for names. This makes us to have same named services in different namespace.&lt;/p&gt;

&lt;p&gt;By default our cluster is running in a namespace called &lt;em&gt;default&lt;/em&gt;. In next section, we will create another namespace where we can run one more single node cluster.&lt;/p&gt;

&lt;h3 id=&quot;creating-namespace&quot;&gt;Creating Namespace&lt;/h3&gt;

&lt;p&gt;In order to create new cluster, first we need to cluster new namespace. Run below command to create namespace called &lt;em&gt;cluster2&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create namespace cluster2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can list all the namespaces using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get namespaces&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should see the below  result&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;NAME          STATUS    AGE
cluster2      Active    16s
default       Active    81d
kube-system   Active    81d&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;kube-system&lt;/em&gt; is the namespace in which all the kubernetes related pods run.&lt;/p&gt;

&lt;h3 id=&quot;setting-context&quot;&gt;Setting Context&lt;/h3&gt;

&lt;p&gt;By default, kubectl points to default namespace. We should change it to point to other one to create pods in our namespace. We can do it using changing the context variable.&lt;/p&gt;

&lt;p&gt;Run below command to change the context&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CONTEXT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;kubectl config view &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; awk &lt;span class=&quot;s1&quot;&gt;&amp;#39;/current-context/ {print $2}&amp;#39;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;
kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-context &lt;span class=&quot;nv&quot;&gt;$CONTEXT&lt;/span&gt; --namespace&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cluster2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the first step, we get &lt;em&gt;CONTEXT&lt;/em&gt; variable. In the next command, we set namespace to &lt;em&gt;cluster2&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;creating-cluster-in-namespace&quot;&gt;Creating cluster in Namespace&lt;/h3&gt;

&lt;p&gt;Once we set the context, we can use same commands to create cluster. Let’s run below the command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f .&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now our second cluster is started. We can see all the pods across the namespaces using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po --all-namespaces&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should see some result something like below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;NAMESPACE     NAME                           READY     STATUS    RESTARTS   AGE
cluster2      spark-master-498980536-bxda1   1/1       Running   0          1m
cluster2      spark-worker-91608803-p1mfe    1/1       Running   0          1m
default       spark-master-498980536-cfw97   1/1       Running   0          46m
default       spark-worker-91608803-7pwhv    1/1       Running   0          46m
kube-system   kube-addon-manager-minikube    1/1       Running   17         81d
kube-system   kube-dns-v20-s0yyp             3/3       Running   80         81d
kube-system   kubernetes-dashboard-rb46j     1/1       Running   17         81d&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can observe from the result, there are multiple spark-master running in different namespaces.&lt;/p&gt;

&lt;p&gt;So using the namespace abstraction of kubernetes we can create multiple spark clusters on same kubernetes cluster.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog we discussed how to scale our clusters using kubernetes deployment abstraction. Also we discussed how to use 
namespace abstraction to create multiple clusters.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Whenever we run services on kubernetes we may want to restrict their resource usage. This allows better infrastructure planning
and monitoring. In next blog, we will discuss about resource management on kubernetes.&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Mar 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-7</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-7</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 6 : Building Spark 2.0 Two Node Cluster</title>
        <description>&lt;p&gt;In last post, we have built spark 2.0 docker image. As a next step we will be building two node spark standalone cluster using that image. In the context of of kubernetes,  node analogues to a container. So in the sixth blog of the series, we will be building two node cluster containing single master and single worker.You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR you can access all the source code on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;spark-master-deployment&quot;&gt;Spark Master Deployment&lt;/h3&gt;

&lt;p&gt;To start with we define our master using kubernetes deployment abstraction. As you can recall from &lt;a href=&quot;/scaling-spark-with-kubernetes-part-3&quot;&gt;earlier&lt;/a&gt; post, deployment abstraction is used for defining one or morepods. Even though we need single master in our cluster, we will use deployment abstraction over pod as it gives us more flexiblity.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-2.1.0-bin-hadoop2.6&lt;/span&gt; 
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;IfNotPresent&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7077&lt;/span&gt;
          &lt;span class=&quot;l-Scalar-Plain&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;TCP&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/bin/bash&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;-c&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;--&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;./start-master.sh&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;infinity&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above yaml configuration shows the configuration for the master. The noteworthy pieces are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;image - We are using the image we built in our last post. This is availble in local docker images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;imagePullPolicy - By default kubernetes tries to pull the image from remote servers like dockerhub. But as our image is only available locally, we need to tell to kubernetes not to pull from remote. &lt;em&gt;imagePullPolicy&lt;/em&gt; property of configuration allows to us to control that. In our example, we say &lt;em&gt;IfNotPresent&lt;/em&gt; , which means pull only if there is no local copy. As we already have built the image, it will be avaialble and kubernetes will not try to pull from remote.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ports - We are exposing port &lt;em&gt;7077&lt;/em&gt; on which spark master will listen.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;command - Command is the configuration which tells what command to run when container bootstraps. Here we are specifying it to run &lt;em&gt;start-master&lt;/em&gt; script&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/spark-master.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-spark-master&quot;&gt;Starting Spark Master&lt;/h3&gt;

&lt;p&gt;Once we have our configuration ready, we can start the spark master pod using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-master.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;spark-master-service&quot;&gt;Spark Master Service&lt;/h3&gt;

&lt;p&gt;Once we have defined and ran the spark master, next step is to define the service for spark master. This service exposes the spark master on network and other workers can connect to it.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# the port that this service should serve on&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;webui&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;8080&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;8080&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7077&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7077&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above yaml configuration for spark master service. We are naming the our service also &lt;em&gt;spark-master&lt;/em&gt; which helps in resolving proper hosts on cluster.&lt;/p&gt;

&lt;p&gt;We are also exposing the additional port 8080 for accessing spark web ui.&lt;/p&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/spark-master-service.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-spark-master-service&quot;&gt;Starting Spark Master Service&lt;/h3&gt;

&lt;p&gt;Once we have defined the master service, we can now start the service using below command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-master-service.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;spark-worker-configuration&quot;&gt;Spark Worker Configuration&lt;/h3&gt;

&lt;p&gt;Once we have our spark master and it’s service started, we can define the worker configuration.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-2.1.0-bin-hadoop2.6&lt;/span&gt; 
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;imagePullPolicy&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;IfNotPresent&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7078&lt;/span&gt;
          &lt;span class=&quot;l-Scalar-Plain&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;TCP&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/bin/bash&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;-c&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;--&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;./start-worker.sh&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;infinity&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we are building two node cluster, we will be running only single worker as of now. Most of the configuration are same as master other than command which starts the worker.&lt;/p&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/spark-worker.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-worker&quot;&gt;Starting Worker&lt;/h3&gt;

&lt;p&gt;You can start worker deployment using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-worker.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have all services are ready&lt;/p&gt;

&lt;h3 id=&quot;verifying-the-setup&quot;&gt;Verifying the Setup&lt;/h3&gt;

&lt;p&gt;Run below command to verify that both spark master and spark worker deployments are started.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command should two pods running as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;NAME                            READY     STATUS    RESTARTS   AGE
spark-master-498980536-6ljcw    1/1       Running   0          15h
spark-worker-1887160080-nmpq5   1/1       Running   0          14h&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Please note that exact name of the pod will differ from machine to machine.&lt;/p&gt;

&lt;p&gt;Once we verified the pods, verify the service using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl describe svc spark-master&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command should show result as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Name:                   spark-master
Namespace:              default
Labels:                 name=spark-master
Selector:               name=spark-master
Type:                   ClusterIP
IP:                     10.0.0.147
Port:                   webui   8080/TCP
Endpoints:              172.17.0.3:8080
Port:                   spark   7077/TCP
Endpoints:              172.17.0.3:7077
Session Affinity:       None&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If both of the commands ran successfully, then we have spark cluster running successfully.&lt;/p&gt;

&lt;h3 id=&quot;testing-our-spark-cluster&quot;&gt;Testing our spark cluster&lt;/h3&gt;

&lt;p&gt;We can test our spark deployment using observing web ui and running some commands from spark shell.&lt;/p&gt;

&lt;h4 id=&quot;accessing-web-ui&quot;&gt;Accessing Web UI&lt;/h4&gt;

&lt;p&gt;In our configuration of spark master, we have exposed the UI port 8080. Normally it will be only available within spark cluster. But using the port forwarding, we can access the port on our local machine.&lt;/p&gt;

&lt;p&gt;First let’s see the pods running on cluster using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It should show the below result&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;NAME                           READY     STATUS    RESTARTS   AGE
spark-master-498980536-kfgg8   1/1       Running   0          14m
spark-worker-91608803-l22pw    1/1       Running   0          56s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We should port forward from master pod. Run below command. The exact name of the pod will differ from machine to machine.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl port-forward spark-master-498980536-kfgg8 8080:8080&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Port-forward takes two parameters. One is the pod name and then port pair. In port pair the first port is container port and next one is local.&lt;/p&gt;

&lt;p&gt;Once port is forwarded, go to this link &lt;a href=&quot;http://localhost:8080&quot;&gt;http://localhost:8080&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You should see the below image&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/spark-ui-kube.png&quot; alt=&quot;spark-ui-kube&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;spark-shell&quot;&gt;Spark Shell&lt;/h4&gt;

&lt;p&gt;Once we have spark ui, we can test the spark from shell. Let’s run the spark shell from master container.&lt;/p&gt;

&lt;p&gt;First we need to login to our master pod. Run below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -it spark-master-498980536-kfgg8 bash&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Start the spark shell using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;/opt/spark/bin/spark-shell --master spark://spark-master:7077&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Run below command to run some spark code&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makeRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If the code runs successfully, then our cluster setup is working.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this blog, we have succesfully built two node spark cluster using kubernetes absttractions.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h3&gt;

&lt;p&gt;Now we have defined our barebone cluster. In next blog, we will how to scale the cluster using kubernetes tools. Also we will discuss how to do resource management in the cluster.&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-6</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-6</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 5 : Building Spark 2.0 Docker Image</title>
        <description>&lt;p&gt;In last few posts of our kubernetes series, we discussed about the various abstractions available in the framework. In next set of posts, we will be
building a spark cluster using those abstractions. As part of the cluster setup, we will discuss how to use various different configuration available
in kubernetes to achieve some of the import features of clustering. This is the fifth blog of the series, where we will discuss about building a spark
2.0 docker image for running spark stand alone cluster. You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR you can access all the source code on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;need-for-custom-spark-image&quot;&gt;Need for Custom Spark Image&lt;/h3&gt;

&lt;p&gt;Kubernetes already has documented creating a spark cluster on &lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/examples/spark&quot;&gt;github&lt;/a&gt;. But currently it uses old version of the spark. Also it has some configurations which are specific to google cloud. These configurations are not often needed in most of the use cases. So in this blog, we will developing a simple spark image which is based on kubernetes one.&lt;/p&gt;

&lt;p&gt;This spark image is built for standalone spark clusters. From my personal experience, spark standalone mode is more suited for containerization
compared to yarn or mesos.&lt;/p&gt;

&lt;h3 id=&quot;docker-file&quot;&gt;Docker File&lt;/h3&gt;

&lt;p&gt;First step of creating a docker image is to write a docker file. In this section, we will discuss how to write a docker file needed
for spark.&lt;/p&gt;

&lt;p&gt;The below are the different steps of docker file.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Base Image&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;FROM java:openjdk-8-jdk&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above statement in the docker file defines the base image. We are using
a base image which gives us a debian kernel with java installed. We need 
java for all spark services.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Define Spark Version&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ENV spark_ver 2.1.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above line defines the version of spark. Using ENV, we can defines a variable and use it in different places in the script. Here we are building the spark with version 2.1.0. If you want other version, change this configuration.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download and Install Spark Binary&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;RUN mkdir -p /opt &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    curl http://www.us.apache.org/dist/spark/spark-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/spark-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;-bin-hadoop2.6.tgz &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        tar -zx &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    ln -s spark-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;-bin-hadoop2.6 spark &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Spark &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; installed in /opt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above curl command and downloads the spark binary. It will be symlinked into /opt/spark.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add start scripts to image&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ADD start-common.sh start-worker.sh start-master.sh /
RUN chmod +x /start-common.sh /start-master.sh /start-worker.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above lines add some start scripts. We discuss more about these scripts
in next section.&lt;/p&gt;

&lt;p&gt;Now we have our docker file ready. Save it as &lt;em&gt;Dockerfile&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;You can access the complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/Dockerfile&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;scripts&quot;&gt;Scripts&lt;/h3&gt;

&lt;p&gt;In above, we have added some scripts for starting master and worker. Let’s see what’s inside them.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start-common.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a script which runs before starting master and worker.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;unset &lt;/span&gt;SPARK_MASTER_PORT&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above script unsets a variable set by kubernetes. This is needed as this configuration interferes with the
spark clustering. We will discuss more about service variable in next post.&lt;/p&gt;

&lt;p&gt;You can access complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/start-common.sh&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start-master.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a script for starting master.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;

. /start-common.sh

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;$(hostname -i) spark-master&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; /etc/hosts

/opt/spark/sbin/start-master.sh --ip spark-master --port 7077&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the first step, we run the common script. We will be using &lt;em&gt;spark-master&lt;/em&gt; as the host name for our master container. So we are adding that into &lt;em&gt;/etc/hosts&lt;/em&gt; file.&lt;/p&gt;

&lt;p&gt;Then we start the master using &lt;em&gt;start-master.sh&lt;/em&gt; command. We will be listening on 7077 port for the master.&lt;/p&gt;

&lt;p&gt;You can access complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/start-master.sh&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start-worker.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is the script for starting worker containers.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;

. /start-common.sh

/opt/spark/sbin/start-slave.sh spark://spark-master:7077&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It is similar to master script. The only difference is we are using &lt;em&gt;start-slave.sh&lt;/em&gt; for starting our worker nodes.&lt;/p&gt;

&lt;p&gt;You can access complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/start-worker.sh&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now we have our docker script ready. To build an image from the script, we need docker.&lt;/p&gt;

&lt;h3 id=&quot;installing-docker&quot;&gt;Installing Docker&lt;/h3&gt;

&lt;p&gt;You can install the docker on you machine using the steps &lt;a href=&quot;https://docs.docker.com/engine/installation/&quot;&gt;here&lt;/a&gt;. I am using docker version &lt;em&gt;1.10.0&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;using-kubernetes-docker-environment&quot;&gt;Using Kubernetes Docker Environment&lt;/h3&gt;

&lt;p&gt;Whenever we want to use docker, it normally runs a daemon on our machine. This daemon is used for building and pulling docker images. Even though we can build our docker image in our machine, it will be not that useful as our kubernetes runs in a vm. In this case, we need to push our docker image to vm and then only we can use the image in kubernetes.&lt;/p&gt;

&lt;p&gt;Alternative to that, another approach is to use minikube docker daemon. In this way we can build the docker images directly on our virtual machine.&lt;/p&gt;

&lt;p&gt;To access minikube docker daemon, run the below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;minikube docker-env&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you can run&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;docker ps&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you can see all the kubernetes containers as docker containers. Now you have successfully connected to minikube docker environment.&lt;/p&gt;

&lt;h3 id=&quot;building-image&quot;&gt;Building image&lt;/h3&gt;

&lt;p&gt;Clone code from github as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;git clone https://github.com/phatak-dev/kubernetes-spark.git&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;cd to &lt;em&gt;docker&lt;/em&gt; folder then run the below docker command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;docker

docker build -t spark-2.1.0-bin-hadoop2.6 .&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above command, we are tagging (naming) the image as &lt;em&gt;spark-2.1.0-bin-hadoop-2.6&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now our image is ready to deploy, spark 2.1.0 on kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this post, we discussed how to build a spark 2.0 docker image from scratch. Having our own image gives more flexibility than using
off the shelf ones.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h3&gt;

&lt;p&gt;Now we have our spark image ready. In our next blog, we will discuss how to use this image to create a two node cluster in kubernetes.&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-5</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-5</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 4 : Service Abstractions</title>
        <description>&lt;p&gt;In last blog, we discussed about the compute abstraction of kubernetes. In that blog, we discussed about creating a pod with nginx container. At the end of the blog, we needed ability to expose nginx pod for consuming it services. To do that, we need to understand how networking works in kubernetes.So in this fourth blog of the series, we are going to discuss various network related abstractions provided kubernetes. You can access all the blog in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;network-abstractions&quot;&gt;Network Abstractions&lt;/h2&gt;

&lt;p&gt;Network abstractions in the kubernetes are the one which facilitate the communication between the pods or the communication of the pods from external world. Commonly these are known as service abstractions.&lt;/p&gt;

&lt;p&gt;In the following sections, we are going to explore different service abstractions.&lt;/p&gt;

&lt;h3 id=&quot;container-port&quot;&gt;Container Port&lt;/h3&gt;

&lt;p&gt;As part of the pod definition, we can  define which ports to be exposed from the container using &lt;em&gt;containerPort&lt;/em&gt; property. This will expose that specific port in
the container on it’s ip address.&lt;/p&gt;

&lt;p&gt;Let’s define port at 80 in our nginx deployment.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;80&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can access complete file &lt;a href=&quot;https://github.com/phatak-dev/blog/blob/master/code/KubernetesExamples/nginxdeployment.yaml&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;service&quot;&gt;Service&lt;/h3&gt;

&lt;p&gt;Once we defined the container port, next step is to define service.&lt;/p&gt;

&lt;p&gt;Service abstraction defines a set of logical pods. This is a network abstraction which defines a policy to expose micro service using these pods to other parts of the application.&lt;/p&gt;

&lt;p&gt;This separation of container and it’s service layer allows us to upgrade the different parts of the applications independent of each other. This is the strength of the microservice.&lt;/p&gt;

&lt;p&gt;Let’s define a service for our nginx deployment.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-service&lt;/span&gt;
   &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; 
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-service&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;selector&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; 
     &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;port&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;80&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above configuration defines the service. The import sections to focus are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;kind - As we specified with pod and deployment abstractions, we specify the service using this parameter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;selector - Connecting pods with service. This is the way kubernetes knows which pod to forward the requests to the service. In this , we
are specifying the selector on label called &lt;em&gt;name&lt;/em&gt; and it’s value &lt;em&gt;nginx&lt;/em&gt;. This should be same labels that we have specified in the 
nginxdeployment.yaml. The below was the our deployment definition&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;80&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above configuration, we have specified the labels in our template. This shows how label abstraction is used to connect service and pod abstractions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ports - This specifies the ports which service should connect on the container. By default the service port on which it listens is same as container
port. You can change it if you want by specifying the &lt;em&gt;targetPort&lt;/em&gt; parameter.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/blog/blob/master/code/KubernetesExamples/nginxservice.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-service&quot;&gt;Starting Service&lt;/h3&gt;

&lt;p&gt;Once we have defined the configuration, we can start the service using below command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubctl create -f nginxservice.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can list all the services, as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get svc&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It should show the service running below.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;nginx-service   10.0.0.197   &amp;lt;none&amp;gt;        80/TCP    23h&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have successfully started the service.&lt;/p&gt;

&lt;h3 id=&quot;service-endpoint&quot;&gt;Service EndPoint&lt;/h3&gt;

&lt;p&gt;Service we have created above is only accessible within the kubernetes cluster. There is a way to expose the service to external world, but we will be not discussing that in this post.&lt;/p&gt;

&lt;p&gt;To connect to the service, we need to know the machine it runs. As we are running kubernetes in local mode, it will be virtual machine running minikube.&lt;/p&gt;

&lt;p&gt;Run below command to get the end point details&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl describe svc&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It should show the output as below.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Name:                   nginx-service
Namespace:              default
Labels:                 name=nginx-service
Selector:               name=nginx
Type:                   ClusterIP
IP:                     10.0.0.197
Port:                   &amp;lt;unset&amp;gt; 80/TCP
Endpoints:              172.17.0.4:80
Session Affinity:       None&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above command, we are describing the complete information about service. We are interested in the &lt;em&gt;EndPoints&lt;/em&gt; parameter. This gives the IP and port of the machine to which we can connect. Note that the actual values of these parameter will be different on your machine.&lt;/p&gt;

&lt;h3 id=&quot;testing-with-busy-box&quot;&gt;Testing with busy box&lt;/h3&gt;

&lt;p&gt;Now we have end point to call. But we need another pod in cluster to connect to this machine. So let’s run another pod&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl run -i --tty busybox --image&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;busybox --restart&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Never -- sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command shows another way creating and running the pods. The different pieces of the command are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;run - Specifies create and run pod&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;-i - Specifies run the pod interactively. This allows us to send commands using pod&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;–tty - Gives access to the terminal of the pod&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;busybox - Name of the pod.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;–image - image to run inside the container. We are a running an image called busybox, which gives minimal linux shell utilities&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;– restart-never - Since it’s a temporary pod, we don’t need HA&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sh - Specifies run shell command to access&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once you run the above command, you should drop into a familiar linux shell.&lt;/p&gt;

&lt;p&gt;From the shell, run below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;wget -O - http://172.17.0.4&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Replace the IP address with the one you got from end point. This should print the welcome page of nginx as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have successfully connected our service and used our pod.&lt;/p&gt;

&lt;p&gt;Service layer of the kubernetes may look little complicated. It is. It’s built for varieties of use cases. So it has multiple layer of redirection. We will explore more about this abstraction in upcoming posts.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this blog, we have discussed how to define and consume services. Services are one of the important features of the kubernetes which makes it powerful platform to deploy clustered applications.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h3&gt;

&lt;p&gt;Now we know pod, deployment and service abstractions. These are minimal abstractions we need, to build our spark cluster on kubernetes. In next post, we will be discus how to build and scale spark cluster on kubernetes.&lt;/p&gt;

</description>
        <pubDate>Thu, 23 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-4</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-4</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 3 : Kubernetes Abstractions</title>
        <description>&lt;p&gt;In last blog of the series, we discussed about how to install kubernetes in our local machine.In this third blog, we will discuss what are the different abstractions
provided by the kubernetes. You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-abstractions&quot;&gt;Kubernetes Abstractions&lt;/h2&gt;

&lt;p&gt;Kubernetes is a production grade  container orchestration system. It follows an API driven approach to interact between different components. It has a vast API surface.We are not going to cover each of those API’s/abstractions here. We are only going to focus on few of the one which are used most of the times. For all the abstractions, refer to &lt;a href=&quot;https://kubernetes.io/docs/user-guide/&quot;&gt;user guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Kubernetes abstractions can be divided into following four major categories&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compute Abstractions - All the abstractions related to running a computing unit. Ex : Container, Pod etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Network Abstractions - All the abstractions related to expose the computing units on network ex: Container Port, Service etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Storage Abstractions - All the abstractions related to providing and managing storage for compute ex: Volume, VolumeClaim etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metadata Abstractions - All the abstractions related to discovering compute, network and storage abstractions ex : labels&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the following sections, we will be discussing about important compute abstractions. The other abstractions will be covered in future posts.&lt;/p&gt;

&lt;h2 id=&quot;a-brief-word-about-containers&quot;&gt;A Brief Word about Containers&lt;/h2&gt;

&lt;p&gt;Kubernetes is a container orchestration framework. But what is a container? In simple terms, container is a light weight virtual machine which runs one of the services
of an application. The major difference between VM and Containers is how they share operating system and underneath resources. In VM world, each VM has it’s own full copy of operating system. But in case of containers, all the containers share a common operating system kernel. So containers are much more light weight than the VM’s.&lt;/p&gt;

&lt;p&gt;Even though containers are around more than a decade, docker made containers popular. You can get basics of docker or container in general by going through this &lt;a href=&quot;https://www.youtube.com/watch?v=Q5POuMHxW-0&quot;&gt;video&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;compute-abstractions&quot;&gt;Compute Abstractions&lt;/h2&gt;

&lt;p&gt;Once we know what is a container, we can now start discussing about the different compute abstractions in kubernetes. Most of these abstractions discuss about how to create, manage and destroy the containers on scale.&lt;/p&gt;

&lt;h3 id=&quot;pod-abstraction&quot;&gt;Pod Abstraction&lt;/h3&gt;

&lt;p&gt;Pod is a collection of one or more containers. It’s smallest compute unit you can deploy on the kubernetes.&lt;/p&gt;

&lt;p&gt;One of the important aspects of pods are, they run all the containers in the single node. This gives the locality to the containers which need low latency connection between them. Also since they run on same machine, kubernetes creates a networking scheme which allows each containers to address them each other by “localhost”&lt;/p&gt;

&lt;h3 id=&quot;defining-the-pod&quot;&gt;Defining the Pod&lt;/h3&gt;

&lt;p&gt;Kubernetes uses yaml as it’s configuration language for defining various resources.&lt;/p&gt;

&lt;p&gt;In below configuration, we are defining a pod which runs a single container of nginx. Nginx is a popular web server.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-pod&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above yaml snippet defines the pod. The below are the different pieces.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;apiVersion&lt;/strong&gt; - parameter defines the  kubernetes API we are using. This versioning scheme allows kubernetes to support multiple versions of the API’s at same time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;kind&lt;/strong&gt; - This parameter defines for which abstraction of kubernetes we are defining this configuration. Here we are defining for a pod.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;metadata&lt;/strong&gt; - Metadata of the pod. This allows kubernetes to locate the pod uniquely across the cluster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;spec&lt;/strong&gt; - This defines the all the containers we want to run&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For each container we define&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;name&lt;/strong&gt; - Name of the container. This will be also used as the host name of the container. So this has to be unique within the pod&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;image&lt;/strong&gt; - Docker image that needs to be used to create the container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can read more about pod abstraction &lt;a href=&quot;https://kubernetes.io/docs/user-guide/pods/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can find the complete yaml file on &lt;a href=&quot;https://github.com/phatak-dev/blog/blob/master/code/KubernetesExamples/nginxpod.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;creating-pod-from-configuration&quot;&gt;Creating Pod from Configuration&lt;/h3&gt;

&lt;p&gt;Once we define the pod, then we can use &lt;em&gt;kubectl create&lt;/em&gt; command to create a pod&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f nginx.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will download the latest nginx image from dockerhub and starts the container inside the pod.&lt;/p&gt;

&lt;p&gt;If you run the below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should see the results as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;nginx-pod                          1/1       Running   0          37s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you have successfully ran a pod on your kubernetes instance.&lt;/p&gt;

&lt;h3 id=&quot;deployment-abstraction&quot;&gt;Deployment Abstraction&lt;/h3&gt;

&lt;p&gt;In earlier section, we discussed about pod abstraction. Pod abstraction works well when we need to create single copy of the container. But in clustered use cases like spark, we may need multiple instance of same containers. For example, we need multiple instances of spark workers. Expressing them individually is tedious and doesn’t scale well.So in those cases using pod abstraction is not good enough.Also pod abstraction doesn’t allow us to update the code inside the pod without changing the configuration file. This will be challenging in cluster environment where we may want to dynamically update configs/ version of software.&lt;/p&gt;

&lt;p&gt;So to overcome these challenges, kubernetes gives us another abstraction called deployments. As name suggest, this abstraction allows end to end deployment of a pod. This allows us to create, update and destroy pods with much cleaner abstractions than the bare bone pod abstraction. So kubernetes documentation prefers the deployment abstraction over simple pod abstraction.&lt;/p&gt;

&lt;p&gt;So let’s rewrite our nginx pod example using deployment abstraction. The below is the yaml configuration for deployment&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx-deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;
         &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;nginx&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The below are the major differences are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;replicas&lt;/strong&gt; - We can create multiple instances of the pod using this. As we need only instance here we are specifying as the 1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;template&lt;/strong&gt; - This holds the template for the pod. This information is same whatever we specified in the pod definition.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can access the complete file on &lt;a href=&quot;https://github.com/phatak-dev/blog/blob/master/code/KubernetesExamples/nginxdeployment.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;running-the-deployment&quot;&gt;Running the deployment&lt;/h3&gt;

&lt;p&gt;Use the below command to run the deployment&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f nginxdeployment.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can see all running deployments using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get deployments&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you have successfully ran the deployment. You can run multiple copies of the container just by increasing the replicas count.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Now we know the major compute abstractions of the kubernetes. Use deployment abstraction even when you need single pod. It makes things much cleaner.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h3&gt;

&lt;p&gt;Even though we have run the pod, we have not accessed  anything from it. So you may be asking how to access the front-page of nginx. To understand that, we need to understand the network/service abstractions provided by the kubernetes. We will be discussing about them in the next blog.&lt;/p&gt;

</description>
        <pubDate>Fri, 17 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-3</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-3</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 2 : Installing Kubernetes Locally using Minikube</title>
        <description>&lt;p&gt;In last blog, we discussed about what is kubernetes and what are it’s advantages. In this second post of the series, we are going to discuss
how to install the kubernetes locally on your machine.You can find all the posts in the series &lt;a href=&quot;/categories/kubernetes-series/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;installing-kubernetes-on-local-machine&quot;&gt;Installing Kubernetes on Local Machine&lt;/h2&gt;

&lt;p&gt;One of the cool features of kubernetes that it can be installed and tried out in local. It behaves exactly as it will be on a cluster. To try out 
kubernetes on local we need to install minikube and kubectl.&lt;/p&gt;

&lt;p&gt;The below are the steps&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;step-1-pre-requisites&quot;&gt;Step 1 :Pre-Requisites&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To install the kubernetes on local machine, we install minikube. But minikube normally uses some kind of virtualization layer to install the
needed software. So for our example, we will use virtualbox as our virtualization layer. For more pre-requisites refer &lt;a href=&quot;https://kubernetes.io/docs/getting-started-guides/minikube/#requirements&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Download and Install virtualbox from &lt;a href=&quot;http://www.virtualbox.org&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;step-2--install-minikube&quot;&gt;Step 2 : Install MiniKube&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Run the below commands to install minikube on linux. For other operating system, refer &lt;a href=&quot;https://github.com/kubernetes/minikube/releases&quot;&gt;here&lt;/a&gt;.
Latest version as of now is 0.16.0&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.16.0/minikube-linux-amd64 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; chmod +x minikube &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo mv minikube /usr/local/bin/&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;step-3--install-kubectl&quot;&gt;Step 3 : Install KubeCtl&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubectl is a command line utility which communicates to kubernetes over it’s REST API. We can install it using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;curl -LO https://storage.googleapis.com/kubernetes-release/release/&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;interacting-with-minikube&quot;&gt;Interacting With Minikube&lt;/h2&gt;

&lt;p&gt;Once we installed the minikube and kubectl , we can start playing with kubernetes.&lt;/p&gt;

&lt;p&gt;We can start minikube using below command. It downloads minikube iso and start a virtual machine in virtualbox.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;minikube start&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can open the kubernetes dashboard using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;minikube dashboard&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can check is anything running or not, using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po --all-namespaces&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This command should show some kubernetes container running.&lt;/p&gt;

&lt;p&gt;Now we have successfully installed and configured the kubernetes on our machine.&lt;/p&gt;

&lt;p&gt;In our next post, we will discuss the different abstractions of kubernetes and how to use them in our applications.&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-2</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 1 : Introduction to Kubernetes</title>
        <description>&lt;p&gt;As our workloads become more and more micro service oriented, building an infrastructure to deploy them easily 
becomes important. Most of the big data applications need multiple services likes HDFS, YARN, Spark  and their clusters.
Creating, deploying and monitoring them manually is tedious and error prone.&lt;/p&gt;

&lt;p&gt;So most of the users move to cloud to simplify it. Solutions like EMR, Databricks etc help in this regard. But then users will be locked into
those specific services. Also sometimes we want same deployment strategy to work on premise also. Most of the cloud providers don’t have that option today.&lt;/p&gt;

&lt;p&gt;So we need a framework which helps us to create and monitor complex big data clusters. Also it should helps us move between on premise and
other cloud providers seamlessly. Kubernetes is one those frameworks that can help us in that regard.&lt;/p&gt;

&lt;p&gt;In this set of posts, we are going to discuss how kubernetes, an open source container orchestration framework from Google, helps us
to achieve a deployment strategy for spark and other big data tools which works across the on premise and cloud. As part of the series, we will 
discuss how to install, configure and scale kubernetes on local and cloud. Also we are going to discuss how to build our own customised images for the services and applications.&lt;/p&gt;

&lt;p&gt;This is the first blog in the series where we discuss about what is kubernetes and it’s advantages. You can access
all other blogs in the series &lt;a href=&quot;/categories/kubernetes-series/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-is-kubernetes&quot;&gt;What is Kubernetes?&lt;/h2&gt;

&lt;p&gt;Kubernetes is an open source container orchestration framework. In simple words, it’s a framework which allows us
to create and manage multiple containers. These containers will be docker containers which will be running some services. 
These can be your typical webapp, database or even big data tools like spark, hbase etc.&lt;/p&gt;

&lt;h2 id=&quot;why-kubernetes&quot;&gt;Why Kubernetes?&lt;/h2&gt;

&lt;p&gt;Most of the readers may have tried docker before. It’s a framework which allows developers containerise their application. It has become a
popular way to develop, test and deploy applications on scale. When we already have docker, what is kubernetes bring into picture? Can’t we 
just build our clusters using normal docker itself?&lt;/p&gt;

&lt;p&gt;The below are the some of the advantages of using kubernetes over plain docker tools.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;orchestration&quot;&gt;Orchestration&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the import feature that sets kubernetes apart from docker is it’s not a container framework. But it’s more of a orchestration layer for multiple containers
that normally make an application. Docker itself has compose feature but it’s very limited. So as our application become complex, we will have
multiple containers which needs to be orchestrated. Doing them manually becomes tricky. So kubernetes helps in that regard.&lt;/p&gt;

&lt;p&gt;Also kubernetes has support for multiple container frameworks. Currently it supports docker and rkt. This makes users
to choose their own container frameworks rather than sticking with only docker.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;cloud-independent&quot;&gt;Cloud Independent&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the import design goal of kubernetes, is ability to run everywhere. We can run kubernetes in local machine, on-premise clusters or on cloud.
Kubernetes has support for AWS,GCE and Azure out of the box. Not only it normalises the deployment across the cloud, it will use best tool for given
problem given by specific cloud. So it tries to optimise for each cloud.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;support-for-easy-clustering&quot;&gt;Support for Easy Clustering&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the hard part of installing big data tools like spark on cloud is to build the cluster and maintain it. Creating clusters often need tinkering with networking to make sure all services are started in right places. Also once cluster is up and running, making sure each node has sufficient resources also is tricky.&lt;/p&gt;

&lt;p&gt;Often scaling cluster, adding node or removing it, is tricky. Kubernetes makes all this much easier compared to current solutions. It has excellent support to
virtual networking and ability to easily scale clusters on will.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;support-for-service-upgradation-and-rollback&quot;&gt;Support for Service Upgradation and Rollback&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the hard part of clustered applications, is to update the software. Sometime it may be you want to update the application code or want to update version of
spark itself. Having a well defined strategy to upgrade the clusters with check and balances is super critical. Also when things go south, ability to rollback 
in reasonably time frame is also important.&lt;/p&gt;

&lt;p&gt;Kubernetes provides well defined image ( container image) based upgradation policies which can unify the upgrading different services across cluster. This makes
life easier for all the ops people out there.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;effective-resource-isolation-and-management&quot;&gt;Effective Resource Isolation and Management&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the question, we often ponder should we run services like kafka next to spark or not? Most of the time people advise to have separate machines
so that each service gets sufficient resources. But defining machine size and segregating services based on machines becomes tricky as we want to scale our
services.&lt;/p&gt;

&lt;p&gt;Kubernetes frees you from the machine. Kubernetes asks you to define how much resources you want to dedicate for service. Once you do that, it will take care
of figuring out which machine to run those. It will make sure that it will effectively using all resources across machines and also give guarantees about resource
allocation. You no more need to worry about is one service is taking over all resources and depriving others or your machines are under utilized.&lt;/p&gt;

&lt;p&gt;Not only kubernetes allows you to define resources In terms of GB of RAM or number of cpu’s, it allows it to be defined in terms of percentage of machine resource or
in terms of no of requests. These options are there to dedicate the resources more granularly.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;well-defined-storage-management&quot;&gt;Well Defined Storage Management&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the challenges of micro service oriented architectures is to store the state across the restart/ upgradation of containers. It’s critical for applications like Databases not loose data when something goes wrong with container or machine.&lt;/p&gt;

&lt;p&gt;Kubernetes gives a clear abstraction of storage who’s life cycle is independent of the container itself. This makes users ability to use different storages like host based, network attached drives to make sure that there will be no data loss. These abstractions ties well with persistence options provided by cloud like EBS from aws. Kubernetes makes long running persistent services like databases a breeze.&lt;/p&gt;

&lt;p&gt;Now we know what kubernetes brings to the table. In our next post, we will be discussing how to install kubernetes on local machine.&lt;/p&gt;

</description>
        <pubDate>Mon, 13 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-1</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-1</guid>
      </item>
    
      <item>
        <title>Statistical Data Exploration using Spark 2.0 - Part 3 : Outlier Detection using Quantiles</title>
        <description>&lt;p&gt;In our &lt;a href=&quot;/statistical-data-exploration-spark-part-1&quot;&gt;first blog&lt;/a&gt; of the series, we discussed about generating summary data using spark.This summary data included mean, standard deviation and quantiles. Quantiles gives pretty good idea about spread of data and are one of the robust measurements compared to mean.&lt;/p&gt;

&lt;p&gt;In this third blog of the series, we will be discussing about how to use quantiles to identify the outliers in our data. You can find all other blogs in the series &lt;a href=&quot;/categories/statistical-data-exploration&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR All code examples available on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;outlier&quot;&gt;Outlier&lt;/h2&gt;
&lt;p&gt;For a given variable in data, outlier is a value distant from other values. Normally outlier is  introduced in data due to issue with measurements or some error. Outlier effects our inference of the data as they may skew the results.&lt;/p&gt;

&lt;p&gt;So in statistics its important to identify the outliers in the data, before we use it for analysis.&lt;/p&gt;

&lt;h2 id=&quot;outlier-detection-using-box-and-whisker-plot&quot;&gt;Outlier detection using Box-and-Whisker Plot&lt;/h2&gt;

&lt;p&gt;There are many methods to identify outlier in statistics. In this blog, we are going to discuss about one of the method which uses quantiles. The logic of the algorithm as follows&lt;/p&gt;

&lt;p&gt;Let’s say we have Q1 as first quantile(25%) and Q3 as third quantile(75%) , the inter quantile range or IQR will be given as&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;IQR = Q3 - Q1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;IQR gives the width of distribution of data between 25% and 75% of data. Using IQR we can identify the outliers. This method is known as Box and Whisker method.&lt;/p&gt;

&lt;p&gt;In this method, any value smaller than Q1- 1.5 * IQR or any value greater than Q3+1.5 * IQR will be categorised as the outlier.&lt;/p&gt;

&lt;p&gt;You can find more information on this method &lt;a href=&quot;http://www.purplemath.com/modules/boxwhisk3.htm&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;outlier-detection-in-spark&quot;&gt;Outlier detection in Spark&lt;/h2&gt;

&lt;p&gt;Once we understand the method, we can implement it in spark. The following are the steps for implementing the same.&lt;/p&gt;

&lt;h3 id=&quot;create-sample-data&quot;&gt;Create Sample Data&lt;/h3&gt;

&lt;p&gt;First we create a sample dataset to work with and then convert into a spark dataframe.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampleData&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;14.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;mf&quot;&gt;14.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;14.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;14.9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;15.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;15.9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;16.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rowRDD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkSession&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparkContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makeRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampleData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)))&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;StructType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;StructField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DoubleType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)))&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkSession&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rowRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above example, we have taken a list of values as sample data. If you observe the data, most of the values are around 14.1-14.7. From that we can assume mostly values 10.2, 16.4 are outliers. There is chance that 15.1 and 15.9 are also outliers but we are not fully sure.&lt;/p&gt;

&lt;h3 id=&quot;calculate-quantiles-and-iqr&quot;&gt;Calculate Quantiles and IQR&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;approxQuantile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q3&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantiles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;IQR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we did in earlier posts, we are using &lt;em&gt;approxQuantile&lt;/em&gt;  method to compute the quantiles needed. Once we have quantiles, we can calculate IQR.&lt;/p&gt;

&lt;h3 id=&quot;filter-outliers&quot;&gt;Filter Outliers&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lowerRange&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;IQR&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upperRange&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;IQR&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outliers&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;value &amp;lt; $lowerRange or value &amp;gt; $upperRange&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;outliers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above code, we first calculate the ranges. Then we filter the data using data frame filters.&lt;/p&gt;

&lt;p&gt;When we run this example, we get 10.2 and 16.4 as the outliers.&lt;/p&gt;

&lt;p&gt;You can access complete example on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0/blob/master/src/main/scala/com/madhukaraphatak/spark/dataexploration/OutliersWithIQR.scala&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog, we learned how to use quantiles to detect the outliers in data.&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Nov 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-3</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-3</guid>
      </item>
    
      <item>
        <title>Statistical Data Exploration using Spark 2.0 - Part 2 : Shape of Data with Histograms</title>
        <description>&lt;p&gt;In our last blog, we discussed about generating summary data using spark. The summary works great for understanding the range of data quantitatively. But sometimes, we want to understand how the data is distributed between different range of the values. Also rather than just know the numbers, it will help a lot if we are able visualize the same. This way of exploring data is known as understanding shape of the data.&lt;/p&gt;

&lt;p&gt;In this second blog of the series, we will be discussing how to understand the shape of the data using the histogram. You can find all other blogs in the series &lt;a href=&quot;/categories/statistical-data-exploration&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR All code examples available on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;histogram&quot;&gt;Histogram&lt;/h2&gt;

&lt;p&gt;Histograms are visual representation of the shape/distribution of the data. This visual representation is heavily used in statistical data exploration.&lt;/p&gt;

&lt;h2 id=&quot;histogram-in-r&quot;&gt;Histogram in R&lt;/h2&gt;

&lt;p&gt;In R, histogram is part of package named &lt;strong&gt;ggplot2&lt;/strong&gt;. Once you installed the package you can generate the histogram as below.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;hist&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;LifeExp&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We use hist method provided by the library to draw the histogram. The below picture shows the histogram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/histogram_in_r.png&quot; alt=&quot;Histogram in R&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;histogram-in-spark&quot;&gt;Histogram in Spark&lt;/h2&gt;

&lt;p&gt;In order to generate the histogram, we need two different things&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generate the values for histogram&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Display the visual representation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Calculating the histogram in spark is relatively easy. But unlike R, spark doesn’t come with built in visualization package. So I will be using &lt;a href=&quot;https://zeppelin.apache.org/&quot;&gt;Apache Zeppelin&lt;/a&gt; for generating charts.&lt;/p&gt;

&lt;h2 id=&quot;calculating-the-histogram&quot;&gt;Calculating the histogram&lt;/h2&gt;

&lt;p&gt;We will be using same dataset, life expectancy, dataset for generating our histograms. Refer to &lt;a href=&quot;/statistical-data-exploration-spark-part-1/&quot;&gt;last blog&lt;/a&gt; for loading data into spark dataframe.&lt;/p&gt;

&lt;p&gt;Dataframe API doesn’t have builtin function for histogram. But RDD API has. So using RDD API we can calculate histogram values as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;startValues&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lifeExpectancyDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;LifeExp&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;RDD histogram API takes number of bins.&lt;/p&gt;

&lt;p&gt;The result of the histogram are two arrays.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First array contains the starting values of each bin&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Second array contains the count for each bin&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The result of the above code on our data will be as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;startValues: Array[Double] = Array(47.794, 54.914, 62.034, 69.154, 76.274, 83.394)
counts: Array[Long] = Array(24, 18, 32, 69, 54)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So the values signify that there are 24 countries between life expectancy from 47.794 to 54.914. Most countries are between 76-83.&lt;/p&gt;

&lt;p&gt;If you don’t like using RDD API, we can add histogram function directly on Dataframe using implicits. Refer to the code on &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0/blob/master/src/main/scala/com/madhukaraphatak/spark/dataexploration/CustomStatFunctions.scala&quot;&gt;github&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&quot;visualizing-the-histogram&quot;&gt;Visualizing the histogram&lt;/h2&gt;

&lt;p&gt;Once we have calculated values for histogram, we want to visualize same. As we discussed earlier, we will be using zeppelin notebook for same.&lt;/p&gt;

&lt;p&gt;In zeppelin, in order to generate a graph easily we need dataframe. But in our case, we got data as arrays. So the below code will convert those arrays to dataframe which can be consumed by the zeppelin.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zippedValues&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;startValues&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HistRow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;startPoint&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rowRDD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zippedValues&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HistRow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histDf&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparkSession&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rowRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;histDf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createOrReplaceTempView&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;histogramTable&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above code, first we combining both arrays using zip method. It will give us a array of tuples. Then we convert that array into a dataframe using the case class.&lt;/p&gt;

&lt;p&gt;Once we have, dataframe ready we can run sql command and generate nice graphs as below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/histogram_lifexp.png&quot; alt=&quot;Histogram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can download the complete zeppelin notebook from &lt;a href=&quot;https://github.com/phatak-dev/Statistical-Data-Exploration-Using-Spark-2.0/blob/master/src/main/zeppelin/Shape%20of%20Data%20Histogram.json&quot;&gt;github&lt;/a&gt; and import into yours to test by yourself. Please make sure you are using Zeppelin 0.6.2 stable release.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Combining computing power of spark with visualization capabilities of zeppelin allows us to explore data in a way R or python does but for big data. This combination of tools make statistical data exploration on big data much easier and powerful.&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Oct 2016 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/statistical-data-exploration-spark-part-2</guid>
      </item>
    
  </channel>
</rss>
