---           
layout: post
title: "Gradient descent for Logistic regression in octave"
date: 2014-02-01 14:56:19 UTC
updated: 2014-02-01 14:56:19 UTC
comments: false
categories: 
---

<div dir="ltr" style="text-align: left;" trbidi="on">I am a huge fan of machine learning classes on <a href="http://ml-class.org/" target="_blank">coursera</a>. They do a great job of explaining machine learning techniques with lots of hands on . If you are following the course or watching videos you will find this post useful.<br /><br />In the discussion of Logistic Regression, &nbsp;exercise two, we use fminunc function rather than standard gradient descent for minimizing for theta. Exercise does not discuss how to use gradient descent for the same. If you use the code of gradient descent of linear &nbsp;regression exercise you don't get same values of theta . So you will be left wondering how to use gradient descent for logistic regression. &nbsp;Even I also got stuck at same place and was able to figure it out after lot of trial and error. To save other's pain I am sharing following code for the same.<br /><br /><br />Create a file called&nbsp;<b>gradientDescent.m</b> and paste the following code<br /><br /><pre class="brush: octave;">function [theta,cost] = gradientDescent(X, y, theta, alpha, num_iters)<br /><br />%GRADIENTDESCENT Performs gradient descent to learn theta<br /><br />% &nbsp; theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by<br /><br />% &nbsp; taking num_iters gradient steps with learning rate alpha<br /><br /><br /><br />% Initialize some useful values<br /><br />m = length(y); % number of training examples<br /><br />J_history = zeros(num_iters, 1);<br /><br />theta_history = theta;<br /><br /><br /><br />for iter = 1:num_iters<br /><br />&nbsp; &nbsp; h = sigmoid(X*theta);<br /><br />&nbsp; &nbsp; grad = (X'*(h - y))/m;<br /><br />&nbsp; &nbsp; theta = theta - alpha*grad; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br /><br />end<br /><br />[cost,gradient] = costFunction(theta,X,y); &nbsp; &nbsp; &nbsp; <br /><br /><br /><br /></pre>Other than the calculation of h, the other code is identical to the gradient descent of the logisitic regression.<br /><br /><b>To call gradient descent , add the following lines to&nbsp;ex2.m</b><br /><br /><pre class="brush: octave;">%%============= Part 4: Optimizing using Gradient Descent =============<br /><br />alpha = 0.0014;<br /><br />[theta,cost]= gradientDescent(X,y,initial_theta,alpha,2000000);<br /><br />% Print theta to screen<br /><br />fprintf('Cost at theta found by gradient Descent: %f\n', cost);<br /><br />fprintf('theta: \n');<br /><br />fprintf(' %f \n', theta);<br /><br /><br /><br />% Plot Boundary<br /><br />plotDecisionBoundary(theta, X, y);<br /><br /><br /><br />% Put some labels<br /><br />hold on;<br /><br />% Labels and Legend<br /><br />xlabel('Exam 1 score')<br /><br />ylabel('Exam 2 score')<br /><br /><br /><br />% Specified in plot order<br /><br />legend('Admitted', 'Not admitted')<br /><br />hold off;<br /><br /><br /><br />fprintf('\nProgram paused. Press enter to continue.\n');<br /><br />pause;<br /><br /></pre><div>Now you should have gradient descent working for logistic regression. </div><br /><br /><br /><br /></div><script type="text/javascript"> SyntaxHighlighter.highlight(); </script><img src="http://feeds.feedburner.com/~r/blogspot/jODW/~4/MrVy-5HKnIs" height="1" width="1"/>